{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308b967c-1101-4ba5-aa0e-501aec71f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n",
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Derivative + cosine annealing from MAML++\n",
    "Note, here, we replace FOMAML with Reptile\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns \n",
    "from math import pi as PI\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "from higher import innerloop_ctx\n",
    "import warnings\n",
    "import importlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Set random seeds for reproducibility of results \n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# set GPU or CPU depending on available hardware\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Available device: {device}\")\n",
    "\n",
    "if device == \"cuda:0\": \n",
    "  # set default so all tensors are on GPU, if available\n",
    "  # help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# import backbone model, dataset, and code utils\n",
    "from models import Neural_Network\n",
    "from constants import *\n",
    "from utils import *\n",
    "import analysis_utils\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba111fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dataset\n",
    "'''\n",
    "# specify the number of tasks to sample per meta-set\n",
    "# note: we end up sampling tasks at random, so sizes are not particularly relevant\n",
    "# artifact of the way we structured the dataset earlier \n",
    "meta_train_size=10000\n",
    "meta_val_size=1000\n",
    "meta_test_size=1000\n",
    "meta_train_eval_size = 20\n",
    "\n",
    "dataset = RegressionDomain(amp_min=amp_min, amp_max=amp_max, \n",
    "                           phase_min=phase_min, phase_max=phase_max, \n",
    "                           train_size=meta_train_size, val_size=meta_val_size, test_size=meta_test_size)\n",
    "\n",
    "meta_val_set = dataset.get_meta_val_batch()\n",
    "meta_test_set = dataset.get_meta_test_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0b0448-3fb2-4b40-b649-9939da4f78ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reptile Iter =  0  Current Loss 0.7727006673812866  Val Loss:  20.614574432373047\n",
      "Reptile Iter =  500  Current Loss 3.6266273478737863  Val Loss:  3.2069316460536377\n",
      "Reptile Iter =  1000  Current Loss 3.32192330433692  Val Loss:  2.953113012606704\n",
      "Reptile Iter =  1500  Current Loss 3.1559746260083816  Val Loss:  2.9366443950651475\n",
      "Reptile Iter =  2000  Current Loss 3.1004043967966473  Val Loss:  2.8908937046995566\n",
      "MAML Iter =  0  Current Loss 3.110223790733339  Val Loss:  2.798072363206933\n",
      "MAML Iter =  500  Current Loss 2.9627575211750905  Val Loss:  2.6978825843323087\n",
      "MAML Iter =  1000  Current Loss 2.7541517487975393  Val Loss:  2.4947458134855736\n",
      "MAML Iter =  1500  Current Loss 2.5454249950509036  Val Loss:  2.3156855054602534\n",
      "MAML Iter =  2000  Current Loss 2.355545800928612  Val Loss:  2.1452702863794215\n",
      "MAML Iter =  2500  Current Loss 2.192532329422806  Val Loss:  1.9971672263695832\n",
      "MAML Iter =  3000  Current Loss 2.0520456665227242  Val Loss:  1.8647355627423565\n",
      "MAML Iter =  3500  Current Loss 1.9314660947686133  Val Loss:  1.7585413638142868\n",
      "MAML Iter =  4000  Current Loss 1.8273821089879232  Val Loss:  1.6633466696776331\n",
      "MAML Iter =  4500  Current Loss 1.7362451114634239  Val Loss:  1.5851163814295766\n",
      "MAML Iter =  5000  Current Loss 1.656704008579457  Val Loss:  1.5112038197977538\n",
      "MAML Iter =  5500  Current Loss 1.5874116635148663  Val Loss:  1.4523692207178434\n",
      "MAML Iter =  6000  Current Loss 1.5254502379956347  Val Loss:  1.3971009617468104\n",
      "MAML Iter =  6500  Current Loss 1.4696247040359642  Val Loss:  1.3469162550505962\n",
      "MAML Iter =  7000  Current Loss 1.4198923429532495  Val Loss:  1.3006697987216926\n",
      "MAML Iter =  7500  Current Loss 1.3745198218171515  Val Loss:  1.2598579089762918\n",
      "MAML Iter =  8000  Current Loss 1.333315212005477  Val Loss:  1.2206964895587513\n",
      "MAML Iter =  8500  Current Loss 1.2959137633098858  Val Loss:  1.1859605211060864\n",
      "MAML Iter =  9000  Current Loss 1.2612194027676888  Val Loss:  1.1543660717493989\n",
      "MAML Iter =  9500  Current Loss 1.2292094967044398  Val Loss:  1.1245549856609796\n",
      "MAML Iter =  10000  Current Loss 1.1993168535035699  Val Loss:  1.0966590638531695\n",
      "MAML Iter =  10500  Current Loss 1.1717026496808531  Val Loss:  1.0743532575926267\n",
      "MAML Iter =  11000  Current Loss 1.145726301556991  Val Loss:  1.0518239406814913\n",
      "MAML Iter =  11500  Current Loss 1.121817526738843  Val Loss:  1.0287761886614215\n",
      "MAML Iter =  12000  Current Loss 1.09964048175623  Val Loss:  1.0076446802991854\n",
      "MAML Iter =  12500  Current Loss 1.0783928062418395  Val Loss:  0.9885338795859372\n",
      "MAML Iter =  13000  Current Loss 1.0586536283480872  Val Loss:  0.9706086589456154\n",
      "MAML Iter =  13500  Current Loss 1.039502967717884  Val Loss:  0.9532408786388242\n",
      "MAML Iter =  14000  Current Loss 1.0216590912785413  Val Loss:  0.9368881700646443\n",
      "MAML Iter =  14500  Current Loss 1.004672019482695  Val Loss:  0.9219929023005577\n",
      "MAML Iter =  15000  Current Loss 0.9890636675902319  Val Loss:  0.9073206518818362\n",
      "MAML Iter =  15500  Current Loss 0.9738837706294218  Val Loss:  0.893263076753284\n",
      "MAML Iter =  16000  Current Loss 0.9594508777410864  Val Loss:  0.8802943793717921\n",
      "MAML Iter =  16500  Current Loss 0.9455444737834404  Val Loss:  0.8678483244252434\n",
      "MAML Iter =  17000  Current Loss 0.9322137982662636  Val Loss:  0.8545280376149267\n",
      "MAML Iter =  17500  Current Loss 0.919709069916017  Val Loss:  0.8429916789048602\n",
      "MAML Iter =  18000  Current Loss 0.9078906564200168  Val Loss:  0.8327241977429375\n",
      "MAML Iter =  18500  Current Loss 0.896467094123223  Val Loss:  0.8221820024103607\n",
      "MAML Iter =  19000  Current Loss 0.8855395803367251  Val Loss:  0.8120286612323814\n",
      "MAML Iter =  19500  Current Loss 0.8751742954561196  Val Loss:  0.8020265458037726\n",
      "MAML Iter =  20000  Current Loss 0.8652602802320529  Val Loss:  0.793043951995399\n",
      "MAML Iter =  20500  Current Loss 0.8557193685273615  Val Loss:  0.7847565868299706\n",
      "MAML Iter =  21000  Current Loss 0.8466225981479653  Val Loss:  0.776546957749284\n",
      "MAML Iter =  21500  Current Loss 0.8378237117333805  Val Loss:  0.7688266526286268\n",
      "MAML Iter =  22000  Current Loss 0.8293287173698124  Val Loss:  0.7615961601307969\n",
      "MAML Iter =  22500  Current Loss 0.8212875434655206  Val Loss:  0.7541465456261858\n",
      "MAML Iter =  23000  Current Loss 0.8135329020667413  Val Loss:  0.7477755170388678\n",
      "MAML Iter =  23500  Current Loss 0.8059518540293777  Val Loss:  0.7405676297333885\n",
      "MAML Iter =  24000  Current Loss 0.7988440671019091  Val Loss:  0.7335624900231948\n",
      "MAML Iter =  24500  Current Loss 0.7919552287935984  Val Loss:  0.7273695512096923\n",
      "MAML Iter =  25000  Current Loss 0.785424927662421  Val Loss:  0.7215038502745752\n",
      "MAML Iter =  25500  Current Loss 0.7789375090980847  Val Loss:  0.715549077252245\n",
      "MAML Iter =  26000  Current Loss 0.7726832831793375  Val Loss:  0.7103116224832416\n",
      "MAML Iter =  26500  Current Loss 0.766651095412417  Val Loss:  0.7051272136320599\n",
      "MAML Iter =  27000  Current Loss 0.7606717247901198  Val Loss:  0.6999739407070364\n",
      "MAML Iter =  27500  Current Loss 0.7550211999600513  Val Loss:  0.6950167354027827\n",
      "MAML Iter =  28000  Current Loss 0.7496432179057906  Val Loss:  0.6896528606233101\n",
      "MAML Iter =  28500  Current Loss 0.7444082818987637  Val Loss:  0.6855459849401875\n",
      "MAML Iter =  29000  Current Loss 0.7394362315661384  Val Loss:  0.6814305639851708\n",
      "MAML Iter =  29500  Current Loss 0.7343748190646266  Val Loss:  0.6774836049251544\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Main training loop\n",
    "\n",
    "Handling computation graphs and second-order backprop help and partial inspiration from: \n",
    "- https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2 \n",
    "- https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853/3 \n",
    "- https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "- https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "- https://discuss.pytorch.org/t/how-to-manually-update-network-parameters-while-keeping-track-of-its-computational-graph/131642/2\n",
    "- https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "- https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "- https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "\n",
    "Neural network configuration and helper class functions copied directly from \n",
    "-https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "Note, different ways to refer to the task-specific vs. meta/aggregate updates to the parameters\n",
    "Sometimes called \"inner\" and \"outer\" loop, respectively\n",
    "Here, refered to as \"task_specific\" and \"agg\"/meta\" (the latter, for consistency w/ ocariz code)\n",
    "'''\n",
    "\n",
    "#Instantiate the model network\n",
    "model = Neural_Network()\n",
    "# move to the current device (GPU or CPU)\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "model.to(device)\n",
    "\n",
    "N = 1 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "K = 10 # number of samples to draw from the task\n",
    "min_lr_meta = .0001 #minimum learning rate for the meta optimizer \n",
    "\n",
    "#Used to store the validation losses\n",
    "metaLosses = []\n",
    "metaValLosses = []\n",
    "\n",
    "#Meta-optimizer for the outer loop\n",
    "meta_optimizer = torch.optim.Adam(model.parameters(), lr = lr_meta)\n",
    "    \n",
    "cosScheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=meta_optimizer, T_max=num_epochs,\n",
    "                   eta_min=0, verbose = False)\n",
    "\n",
    "#Inner optimizer, we were doing this by hand previously\n",
    "inner_loop_optimizer = torch.optim.SGD(model.parameters(), lr = lr_task_specific)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Sample of how to use the classes\n",
    "'''\n",
    "\n",
    "k = 5 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "lr_meta_reptile = 0.001 # meta-update learning rate\n",
    "lr_k_reptile = .01\n",
    "num_epochs_reptile = 2500 #70001 #Number of iterations for outer loop\n",
    "printing_step_reptile = 500 # show log of loss every x epochs\n",
    "meta_optimizer_reptile = torch.optim.Adam(model.parameters(), lr=lr_meta_reptile)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs_reptile):\n",
    "    \n",
    "    waves = dataset.get_meta_train_batch(task_batch_size=1)\n",
    "    \n",
    "    new_model, meta_loss = training_reptile(model, waves[0], criterion, lr_k_reptile, k)\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    metaupdate(model,new_model,meta_optimizer_reptile)\n",
    "    \n",
    "    val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "    \n",
    "    val_loss = test_set_validation(model,new_model,val_wave,criterion,lr_k_reptile,k)\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step_reptile == 0:\n",
    "        print(\"Reptile Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), \"backup_data/reptile_da_model.pt\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cosScheduler.step(epoch=epoch)\n",
    "    # store loss over all tasks to then do a large meta-level update of initial params\n",
    "    # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "    meta_loss = None\n",
    "    \n",
    "    #Sample a new wave each time\n",
    "    waves = dataset.get_meta_train_batch(task_batch_size=T)\n",
    "    \n",
    "    #Loop through all of the tasks\n",
    "    for i, T_i in enumerate(waves): \n",
    "        train_eval_info = task_specific_train_and_eval(model, T_i, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "        held_out_task_specific_loss = train_eval_info[0]\n",
    "        if meta_loss is None: \n",
    "            meta_loss = held_out_task_specific_loss\n",
    "        else:\n",
    "            meta_loss += held_out_task_specific_loss\n",
    "            \n",
    "    meta_optimizer.zero_grad()\n",
    "    meta_loss /= T\n",
    "    meta_loss.backward()\n",
    "    meta_optimizer.step()\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    # validation \n",
    "    val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "    val_eval_info = task_specific_train_and_eval(model, val_wave, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "    val_loss=val_eval_info[0]\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step == 0:\n",
    "        print(\"MAML Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), \"backup_data/maml_da_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b804e161-9383-419e-8948-11fd829167fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Error: 3.0178774673910813, Var: 5.7114881349904945\n",
      "Step: 1, Error: 0.319833796847146, Var: 0.0707601953782532\n",
      "Step: 5, Error: 0.04498379230464343, Var: 0.01419622615001988\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(analysis_utils)\n",
    "\n",
    "num_k_shots = 10\n",
    "K = 10\n",
    "num_eval=1000\n",
    "file_tag = \"derivative_annealing_unidim\"\n",
    "\n",
    "res = analysis_utils.k_shot_evaluation(model, dataset, criterion, num_k_shots=num_k_shots, K=K, num_eval=num_eval,\n",
    "                        file_tag=file_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reptile Iter =  0  Current Loss 8.404191017150879  Val Loss:  2.138186454772949\n",
      "Reptile Iter =  500  Current Loss 4.0521035872870925  Val Loss:  3.157861019040074\n",
      "Reptile Iter =  1000  Current Loss 3.4833531957206953  Val Loss:  3.0154320136376405\n",
      "Reptile Iter =  1500  Current Loss 3.3722086444143424  Val Loss:  2.9675254756609433\n",
      "Reptile Iter =  2000  Current Loss 3.2985631785313156  Val Loss:  2.9523129018854513\n",
      "MAML Iter =  0  Current Loss 3.221447347610118  Val Loss:  2.8881680524783526\n",
      "MAML Iter =  500  Current Loss 2.998992838741353  Val Loss:  2.6925569842835713\n",
      "MAML Iter =  1000  Current Loss 2.735289250869483  Val Loss:  2.4600737181513357\n",
      "MAML Iter =  1500  Current Loss 2.503446376287776  Val Loss:  2.2599910488911465\n",
      "MAML Iter =  2000  Current Loss 2.3140240203853364  Val Loss:  2.082990241638248\n",
      "MAML Iter =  2500  Current Loss 2.1548156720127944  Val Loss:  1.9468436097380162\n",
      "MAML Iter =  3000  Current Loss 2.0191686387419265  Val Loss:  1.825914414494025\n",
      "MAML Iter =  3500  Current Loss 1.903600197429445  Val Loss:  1.7273789063969283\n",
      "MAML Iter =  4000  Current Loss 1.8026425823084609  Val Loss:  1.6397045316560954\n",
      "MAML Iter =  4500  Current Loss 1.7132403301010009  Val Loss:  1.5600072242972576\n",
      "MAML Iter =  5000  Current Loss 1.6350716754636274  Val Loss:  1.487607376236992\n",
      "MAML Iter =  5500  Current Loss 1.564709900518583  Val Loss:  1.4239204407444916\n",
      "MAML Iter =  6000  Current Loss 1.5023206907350095  Val Loss:  1.367417521175426\n",
      "MAML Iter =  6500  Current Loss 1.4459707011574214  Val Loss:  1.3168424124062634\n",
      "MAML Iter =  7000  Current Loss 1.3949572682383096  Val Loss:  1.2702881898456548\n",
      "MAML Iter =  7500  Current Loss 1.3489134931632785  Val Loss:  1.22809340830266\n",
      "MAML Iter =  8000  Current Loss 1.3069965609004104  Val Loss:  1.1904799683143614\n",
      "MAML Iter =  8500  Current Loss 1.267938174462107  Val Loss:  1.1550360902377408\n",
      "MAML Iter =  9000  Current Loss 1.232501577258144  Val Loss:  1.1220776153847096\n",
      "MAML Iter =  9500  Current Loss 1.1988129466309614  Val Loss:  1.0934530801642506\n",
      "MAML Iter =  10000  Current Loss 1.1680657833802155  Val Loss:  1.0647084941962313\n",
      "MAML Iter =  10500  Current Loss 1.1392642395517483  Val Loss:  1.0387521050952098\n",
      "MAML Iter =  11000  Current Loss 1.1125345939254268  Val Loss:  1.0145782499456388\n",
      "MAML Iter =  11500  Current Loss 1.0874580621746546  Val Loss:  0.991807202755379\n",
      "MAML Iter =  12000  Current Loss 1.0637877169657695  Val Loss:  0.9697494441729111\n",
      "MAML Iter =  12500  Current Loss 1.0413617775995  Val Loss:  0.9495546327117959\n",
      "MAML Iter =  13000  Current Loss 1.0204184464072135  Val Loss:  0.9297311120934479\n",
      "MAML Iter =  13500  Current Loss 1.0006436341760643  Val Loss:  0.9118770022213222\n",
      "MAML Iter =  14000  Current Loss 0.9824764546164764  Val Loss:  0.8963190709901889\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Main training loop --- run w/ just DA!!\n",
    "\n",
    "Handling computation graphs and second-order backprop help and partial inspiration from: \n",
    "- https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2 \n",
    "- https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853/3 \n",
    "- https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "- https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "- https://discuss.pytorch.org/t/how-to-manually-update-network-parameters-while-keeping-track-of-its-computational-graph/131642/2\n",
    "- https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "- https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "- https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "\n",
    "Neural network configuration and helper class functions copied directly from \n",
    "-https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "Note, different ways to refer to the task-specific vs. meta/aggregate updates to the parameters\n",
    "Sometimes called \"inner\" and \"outer\" loop, respectively\n",
    "Here, refered to as \"task_specific\" and \"agg\"/meta\" (the latter, for consistency w/ ocariz code)\n",
    "'''\n",
    "\n",
    "#Instantiate the model network\n",
    "model = Neural_Network()\n",
    "# move to the current device (GPU or CPU)\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "model.to(device)\n",
    "\n",
    "N = 1 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "K = 10 # number of samples to draw from the task\n",
    "min_lr_meta = .0001 #minimum learning rate for the meta optimizer \n",
    "\n",
    "#Used to store the validation losses\n",
    "metaLosses = []\n",
    "metaValLosses = []\n",
    "\n",
    "#Meta-optimizer for the outer loop\n",
    "meta_optimizer = torch.optim.Adam(model.parameters(), lr = lr_meta)\n",
    "\n",
    "#Inner optimizer, we were doing this by hand previously\n",
    "inner_loop_optimizer = torch.optim.SGD(model.parameters(), lr = lr_task_specific)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Sample of how to use the classes\n",
    "'''\n",
    "\n",
    "k = 5 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "lr_meta_reptile = 0.001 # meta-update learning rate\n",
    "lr_k_reptile = .01\n",
    "num_epochs_reptile = 2500 #70001 #Number of iterations for outer loop\n",
    "printing_step_reptile = 500 # show log of loss every x epochs\n",
    "meta_optimizer_reptile = torch.optim.Adam(model.parameters(), lr=lr_meta_reptile)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs_reptile):\n",
    "    \n",
    "    waves = dataset.get_meta_train_batch(task_batch_size=1)\n",
    "    \n",
    "    new_model, meta_loss = training_reptile(model, waves[0], criterion, lr_k_reptile, k)\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    metaupdate(model,new_model,meta_optimizer_reptile)\n",
    "    \n",
    "    val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "    \n",
    "    val_loss = test_set_validation(model,new_model,val_wave,criterion,lr_k_reptile,k)\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step_reptile == 0:\n",
    "        print(\"Reptile Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), \"backup_data/reptile_da_only_model.pt\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # store loss over all tasks to then do a large meta-level update of initial params\n",
    "    # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "    meta_loss = None\n",
    "    \n",
    "    #Sample a new wave each time\n",
    "    waves = dataset.get_meta_train_batch(task_batch_size=T)\n",
    "    \n",
    "    #Loop through all of the tasks\n",
    "    for i, T_i in enumerate(waves): \n",
    "        train_eval_info = task_specific_train_and_eval(model, T_i, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "        held_out_task_specific_loss = train_eval_info[0]\n",
    "        if meta_loss is None: \n",
    "            meta_loss = held_out_task_specific_loss\n",
    "        else:\n",
    "            meta_loss += held_out_task_specific_loss\n",
    "            \n",
    "    meta_optimizer.zero_grad()\n",
    "    meta_loss /= T\n",
    "    meta_loss.backward()\n",
    "    meta_optimizer.step()\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    # validation \n",
    "    val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "    val_eval_info = task_specific_train_and_eval(model, val_wave, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "    val_loss=val_eval_info[0]\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step == 0:\n",
    "        print(\"MAML Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), \"backup_data/maml_da_only_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(analysis_utils)\n",
    "\n",
    "num_k_shots = 10\n",
    "K = 10\n",
    "num_eval=1000\n",
    "file_tag = \"derivative_annealing_only_unidim\"\n",
    "\n",
    "res = analysis_utils.k_shot_evaluation(model, dataset, criterion, num_k_shots=num_k_shots, K=K, num_eval=num_eval,\n",
    "                        file_tag=file_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main training loop --- run w/ just CA!!\n",
    "\n",
    "Handling computation graphs and second-order backprop help and partial inspiration from: \n",
    "- https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2 \n",
    "- https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853/3 \n",
    "- https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "- https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "- https://discuss.pytorch.org/t/how-to-manually-update-network-parameters-while-keeping-track-of-its-computational-graph/131642/2\n",
    "- https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "- https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "- https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "\n",
    "Neural network configuration and helper class functions copied directly from \n",
    "-https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "Note, different ways to refer to the task-specific vs. meta/aggregate updates to the parameters\n",
    "Sometimes called \"inner\" and \"outer\" loop, respectively\n",
    "Here, refered to as \"task_specific\" and \"agg\"/meta\" (the latter, for consistency w/ ocariz code)\n",
    "'''\n",
    "\n",
    "#Instantiate the model network\n",
    "model = Neural_Network()\n",
    "# move to the current device (GPU or CPU)\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "model.to(device)\n",
    "\n",
    "N = 1 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "K = 10 # number of samples to draw from the task\n",
    "min_lr_meta = .0001 #minimum learning rate for the meta optimizer \n",
    "\n",
    "#Used to store the validation losses\n",
    "metaLosses = []\n",
    "metaValLosses = []\n",
    "\n",
    "#Meta-optimizer for the outer loop\n",
    "meta_optimizer = torch.optim.Adam(model.parameters(), lr = lr_meta)\n",
    "\n",
    "#Inner optimizer, we were doing this by hand previously\n",
    "inner_loop_optimizer = torch.optim.SGD(model.parameters(), lr = lr_task_specific)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # store loss over all tasks to then do a large meta-level update of initial params\n",
    "    # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "    meta_loss = None\n",
    "    \n",
    "    #Sample a new wave each time\n",
    "    waves = dataset.get_meta_train_batch(task_batch_size=T)\n",
    "    \n",
    "    #Loop through all of the tasks\n",
    "    for i, T_i in enumerate(waves): \n",
    "        train_eval_info = task_specific_train_and_eval(model, T_i, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "        held_out_task_specific_loss = train_eval_info[0]\n",
    "        if meta_loss is None: \n",
    "            meta_loss = held_out_task_specific_loss\n",
    "        else:\n",
    "            meta_loss += held_out_task_specific_loss\n",
    "            \n",
    "    meta_optimizer.zero_grad()\n",
    "    meta_loss /= T\n",
    "    meta_loss.backward()\n",
    "    meta_optimizer.step()\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    # validation \n",
    "    val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "    val_eval_info = task_specific_train_and_eval(model, val_wave, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "    val_loss=val_eval_info[0]\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step == 0:\n",
    "        print(\"MAML Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), \"backup_data/maml_ca_only_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ec6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(analysis_utils)\n",
    "\n",
    "num_k_shots = 10\n",
    "K = 10\n",
    "num_eval=1000\n",
    "file_tag = \"cosine_annealing_only_unidim\"\n",
    "\n",
    "res = analysis_utils.k_shot_evaluation(model, dataset, criterion, num_k_shots=num_k_shots, K=K, num_eval=num_eval,\n",
    "                        file_tag=file_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6169e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
