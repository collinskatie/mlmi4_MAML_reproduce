{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UrjQGgr5nUHC",
   "metadata": {
    "id": "UrjQGgr5nUHC"
   },
   "source": [
    "<h1> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eGl9mcc0nOMP",
   "metadata": {
    "id": "eGl9mcc0nOMP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Code to run Reptile, following Ocariz's notebooks in the Reptile folder\n",
    "Note, utils kept here b/c of use of globals (issues when run from files)\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns \n",
    "from math import pi as PI\n",
    "import random\n",
    "# !pip3 install higher\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "from higher import innerloop_ctx\n",
    "import warnings\n",
    "\n",
    "# import backbone model, dataset, and code utils\n",
    "from models import Neural_Network\n",
    "from constants import *\n",
    "import analysis_utils\n",
    "from data import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G-ExWACxQ3mt",
   "metadata": {
    "id": "G-ExWACxQ3mt"
   },
   "source": [
    "<h1> Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1zyNHFXdOnug",
   "metadata": {
    "id": "1zyNHFXdOnug"
   },
   "outputs": [],
   "source": [
    "# The Minimum Square Error is used to evaluate the difference between prediction and ground truth\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def copy_existing_model(model):\n",
    "    # Function to copy an existing model\n",
    "    # We initialize a new model\n",
    "    new_model = Neural_Network()\n",
    "    # Copy the previous model's parameters into the new model\n",
    "    new_model.load_state_dict(model.state_dict())\n",
    "    return new_model\n",
    "\n",
    "def get_samples_in_good_format(wave, num_samples=10):\n",
    "    #This function is used to sample data from a wave\n",
    "    sample_data = wave.get_samples(num_samples=num_samples)\n",
    "    x = sample_data[\"input\"]\n",
    "    y_true = sample_data[\"output\"]\n",
    "    # We add [:,None] to get the right dimensions to pass to the model: we want K x 1 (we have scalars inputs hence the x 1)\n",
    "    # Note that we convert everything torch tensors\n",
    "    x = torch.tensor(x[:,None])\n",
    "    y_true = torch.tensor(y_true[:,None])\n",
    "    return x,y_true\n",
    "#     # set to whatever the base device is (GPU or CPU)\n",
    "#     # help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "#     return x.to(device),y_true.to(device) \n",
    "\n",
    "def initialization_to_store_meta_losses():\n",
    "  # This function creates lists to store the meta losses\n",
    "  global store_train_loss_meta; store_train_loss_meta = []\n",
    "  global store_test_loss_meta; store_test_loss_meta = []\n",
    "\n",
    "def test_set_validation(model,new_model,wave,lr_inner,k,store_test_loss_meta):\n",
    "    # This functions does not actually affect the main algorithm, it is just used to evaluate the new model\n",
    "    new_model,losses = training(model, wave, lr_inner, k)\n",
    "    # Obtain the loss\n",
    "    loss = evaluation(new_model, wave)\n",
    "    # Store loss\n",
    "    store_test_loss_meta.append(loss)\n",
    "    return losses\n",
    "\n",
    "def train_set_evaluation(new_model,wave,store_train_loss_meta):\n",
    "    loss = evaluation(new_model, wave)\n",
    "    store_train_loss_meta.append(loss) \n",
    "\n",
    "def print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step=1000):\n",
    "  if epoch % printing_step == 0:\n",
    "    print(f'Epochh : {epoch}, Average Train Meta Loss : {np.mean(store_train_loss_meta)}, Average Test Meta Loss : {np.mean(store_test_loss_meta)}')\n",
    "\n",
    "#This is based on the paper update rule, we calculate the difference between parameters and then this is used by the optimizer, rather than doing the update by hand\n",
    "def reptile_parameter_update(model,new_model):\n",
    "  # Zip models for the loop\n",
    "  zip_models = zip(model.parameters(), new_model.parameters())\n",
    "  for parameter, new_parameter in zip_models:\n",
    "    if parameter.grad is None:\n",
    "      parameter.grad = torch.tensor(torch.zeros_like(parameter))\n",
    "    # Here we are adding the gradient that will later be used by the optimizer\n",
    "    parameter.grad.data.add_(parameter.data - new_parameter.data)\n",
    "\n",
    "# Define commands in order needed for the metaupdate\n",
    "# Note that if we change the order it doesn't behave the same\n",
    "def metaoptimizer_update(metaoptimizer):\n",
    "  # Take step\n",
    "  metaoptimizer.step()\n",
    "  # Reset gradients\n",
    "  metaoptimizer.zero_grad()\n",
    "\n",
    "def metaupdate(model,new_model,metaoptimizer):\n",
    "  # Combine the two previous functions into a single metaupdate function\n",
    "  # First we calculate the gradients\n",
    "  reptile_parameter_update(model,new_model)\n",
    "  # Use those gradients in the optimizer\n",
    "  metaoptimizer_update(metaoptimizer)\n",
    "\n",
    "def evaluation(new_model, wave, item = True):\n",
    "    # Get data\n",
    "    x, label = get_samples_in_good_format(wave)\n",
    "    # Make model prediction\n",
    "    prediction = new_model(x)\n",
    "    # Get loss\n",
    "    if item == True: #Depending on whether we need to return the loss value for storing or for backprop\n",
    "      loss = criterion(prediction,label).item()\n",
    "    else:\n",
    "      loss = criterion(prediction,label)\n",
    "    return loss\n",
    "\n",
    "def training(model, wave, lr_k, k):\n",
    "    # Create new model which we will train on\n",
    "    new_model = copy_existing_model(model)\n",
    "    # Define new optimizer\n",
    "    koptimizer = torch.optim.SGD(new_model.parameters(), lr=lr_k)\n",
    "    # Update the model multiple times, note that k>1 (do not confuse k with K)\n",
    "    losses = []\n",
    "    for i in range(k):\n",
    "        # Reset optimizer\n",
    "        koptimizer.zero_grad()\n",
    "        # Evaluate the model\n",
    "        loss = evaluation(new_model, wave, item = False)\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        koptimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return new_model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-4Ps8P2IRCmF",
   "metadata": {
    "id": "-4Ps8P2IRCmF"
   },
   "source": [
    "<h1> Reptile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ogpg_DHizlC",
   "metadata": {
    "id": "8ogpg_DHizlC"
   },
   "outputs": [],
   "source": [
    "#Define important variables\n",
    "epochs = num_epochs * T # number of epochs\n",
    "lr_meta=0.001 # Learning rate for meta model (outer loop)\n",
    "printing_step=1000 # how many epochs should we wait to print the loss\n",
    "lr_k=0.01 # Internal learning rate\n",
    "k=5 # Number of internal updates for each task\n",
    "\n",
    "# Initializations\n",
    "initialization_to_store_meta_losses()\n",
    "model = Neural_Network()\n",
    "metaoptimizer = torch.optim.Adam(model.parameters(), lr=lr_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec34fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dataset\n",
    "'''\n",
    "# specify the number of tasks to sample per meta-set\n",
    "# note: we end up sampling tasks at random, so sizes are not particularly relevant\n",
    "# artifact of the way we structured the dataset earlier \n",
    "meta_train_size=10000\n",
    "meta_val_size=1000\n",
    "meta_test_size=1000\n",
    "meta_train_eval_size = 20\n",
    "\n",
    "dataset = RegressionDomain(amp_min=amp_min, amp_max=amp_max, \n",
    "                           phase_min=phase_min, phase_max=phase_max, \n",
    "                           train_size=meta_train_size, val_size=meta_val_size, test_size=meta_test_size)\n",
    "\n",
    "meta_val_set = dataset.get_meta_val_batch()\n",
    "meta_test_set = dataset.get_meta_test_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "-4-zQWWKFt3s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "-4-zQWWKFt3s",
    "outputId": "a90fc4c6-006c-43c8-d882-4c3a983a5923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 0, Average Train Meta Loss : 0.31318166851997375, Average Test Meta Loss : 7.635172367095947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-94f2234b00e2>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x[:,None])\n",
      "<ipython-input-2-94f2234b00e2>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true = torch.tensor(y_true[:,None])\n",
      "<ipython-input-2-94f2234b00e2>:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  parameter.grad = torch.tensor(torch.zeros_like(parameter))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 1000, Average Train Meta Loss : 2.2108276298922767, Average Test Meta Loss : 2.0172824171105668\n",
      "Epochh : 2000, Average Train Meta Loss : 1.7053038490859829, Average Test Meta Loss : 1.5909115785510857\n",
      "Epochh : 3000, Average Train Meta Loss : 1.4590862448426862, Average Test Meta Loss : 1.327020225114655\n",
      "Epochh : 4000, Average Train Meta Loss : 1.275214175650337, Average Test Meta Loss : 1.134882091877957\n",
      "Epochh : 5000, Average Train Meta Loss : 1.1239877696370266, Average Test Meta Loss : 1.0000415980665323\n",
      "Epochh : 6000, Average Train Meta Loss : 1.0036163470235209, Average Test Meta Loss : 0.8940683421557013\n",
      "Epochh : 7000, Average Train Meta Loss : 0.9025949065264243, Average Test Meta Loss : 0.804106604433534\n",
      "Epochh : 8000, Average Train Meta Loss : 0.8202770355784678, Average Test Meta Loss : 0.7320274474338948\n",
      "Epochh : 9000, Average Train Meta Loss : 0.7512476268261629, Average Test Meta Loss : 0.670789918907111\n",
      "Epochh : 10000, Average Train Meta Loss : 0.6934954195888292, Average Test Meta Loss : 0.6190205646044336\n",
      "Epochh : 11000, Average Train Meta Loss : 0.6456089968428144, Average Test Meta Loss : 0.5756553955256336\n",
      "Epochh : 12000, Average Train Meta Loss : 0.6028726090556729, Average Test Meta Loss : 0.5373456488939672\n",
      "Epochh : 13000, Average Train Meta Loss : 0.5654401465952357, Average Test Meta Loss : 0.5040846656507815\n",
      "Epochh : 14000, Average Train Meta Loss : 0.5333136670098163, Average Test Meta Loss : 0.47451214676958325\n",
      "Epochh : 15000, Average Train Meta Loss : 0.5040215896426119, Average Test Meta Loss : 0.448808495334204\n",
      "Epochh : 16000, Average Train Meta Loss : 0.47860094951444265, Average Test Meta Loss : 0.4260756451449776\n",
      "Epochh : 17000, Average Train Meta Loss : 0.45588562014082706, Average Test Meta Loss : 0.40574200046366377\n",
      "Epochh : 18000, Average Train Meta Loss : 0.4351712032071252, Average Test Meta Loss : 0.38696314468742876\n",
      "Epochh : 19000, Average Train Meta Loss : 0.4160312065864453, Average Test Meta Loss : 0.37077854022220325\n",
      "Epochh : 20000, Average Train Meta Loss : 0.39922894756048755, Average Test Meta Loss : 0.35538525724915243\n",
      "Epochh : 21000, Average Train Meta Loss : 0.383690749611793, Average Test Meta Loss : 0.34181031313889054\n",
      "Epochh : 22000, Average Train Meta Loss : 0.36982497605041, Average Test Meta Loss : 0.32912061968720946\n",
      "Epochh : 23000, Average Train Meta Loss : 0.3571126204348452, Average Test Meta Loss : 0.3176028179126462\n",
      "Epochh : 24000, Average Train Meta Loss : 0.3450080400880612, Average Test Meta Loss : 0.3068639840710345\n",
      "Epochh : 25000, Average Train Meta Loss : 0.3337068130838694, Average Test Meta Loss : 0.2969070925392033\n",
      "Epochh : 26000, Average Train Meta Loss : 0.32337996461043694, Average Test Meta Loss : 0.2877782169423096\n",
      "Epochh : 27000, Average Train Meta Loss : 0.31393423221935324, Average Test Meta Loss : 0.279061906554411\n",
      "Epochh : 28000, Average Train Meta Loss : 0.3051177301534652, Average Test Meta Loss : 0.27100893319332897\n",
      "Epochh : 29000, Average Train Meta Loss : 0.2966971154127867, Average Test Meta Loss : 0.26375549512082397\n",
      "Epochh : 30000, Average Train Meta Loss : 0.28886244769524283, Average Test Meta Loss : 0.2566804445237232\n",
      "Epochh : 31000, Average Train Meta Loss : 0.28147659360889743, Average Test Meta Loss : 0.25003666109728445\n",
      "Epochh : 32000, Average Train Meta Loss : 0.27439937083902227, Average Test Meta Loss : 0.2439426994002732\n",
      "Epochh : 33000, Average Train Meta Loss : 0.26784459499462576, Average Test Meta Loss : 0.23798981841251168\n",
      "Epochh : 34000, Average Train Meta Loss : 0.26172285994370453, Average Test Meta Loss : 0.2323128988124537\n",
      "Epochh : 35000, Average Train Meta Loss : 0.2557298477872966, Average Test Meta Loss : 0.22707629019545467\n",
      "Epochh : 36000, Average Train Meta Loss : 0.2501219233860436, Average Test Meta Loss : 0.22203537898778897\n",
      "Epochh : 37000, Average Train Meta Loss : 0.24472809451350383, Average Test Meta Loss : 0.21717211920488255\n",
      "Epochh : 38000, Average Train Meta Loss : 0.2395849273024704, Average Test Meta Loss : 0.21265626426491283\n",
      "Epochh : 39000, Average Train Meta Loss : 0.234645826825855, Average Test Meta Loss : 0.20838386363472047\n",
      "Epochh : 40000, Average Train Meta Loss : 0.22992655536230733, Average Test Meta Loss : 0.20427918399983008\n",
      "Epochh : 41000, Average Train Meta Loss : 0.22553319297669736, Average Test Meta Loss : 0.2003519525141456\n",
      "Epochh : 42000, Average Train Meta Loss : 0.22136541897484116, Average Test Meta Loss : 0.19655948154992642\n",
      "Epochh : 43000, Average Train Meta Loss : 0.21739157556016211, Average Test Meta Loss : 0.1930307017618409\n",
      "Epochh : 44000, Average Train Meta Loss : 0.21343094044631197, Average Test Meta Loss : 0.189555367496963\n",
      "Epochh : 45000, Average Train Meta Loss : 0.20960682426081315, Average Test Meta Loss : 0.18620681005767165\n",
      "Epochh : 46000, Average Train Meta Loss : 0.20598364316757542, Average Test Meta Loss : 0.18300456274944754\n",
      "Epochh : 47000, Average Train Meta Loss : 0.20258341607385552, Average Test Meta Loss : 0.1798909790220527\n",
      "Epochh : 48000, Average Train Meta Loss : 0.19928488078032006, Average Test Meta Loss : 0.1768913621893987\n",
      "Epochh : 49000, Average Train Meta Loss : 0.1960668020878859, Average Test Meta Loss : 0.17403930713509141\n",
      "Epochh : 50000, Average Train Meta Loss : 0.19293199235624275, Average Test Meta Loss : 0.17132645786530584\n",
      "Epochh : 51000, Average Train Meta Loss : 0.18991863780085558, Average Test Meta Loss : 0.16866445735104774\n",
      "Epochh : 52000, Average Train Meta Loss : 0.18706108956567002, Average Test Meta Loss : 0.16613106457900065\n",
      "Epochh : 53000, Average Train Meta Loss : 0.18430181176620336, Average Test Meta Loss : 0.1636868741537228\n",
      "Epochh : 54000, Average Train Meta Loss : 0.18167516197487193, Average Test Meta Loss : 0.16142335273480424\n",
      "Epochh : 55000, Average Train Meta Loss : 0.17910921049731182, Average Test Meta Loss : 0.15917286541302092\n",
      "Epochh : 56000, Average Train Meta Loss : 0.176691541378676, Average Test Meta Loss : 0.15691677910484392\n",
      "Epochh : 57000, Average Train Meta Loss : 0.17429871107088316, Average Test Meta Loss : 0.15475607680799766\n",
      "Epochh : 58000, Average Train Meta Loss : 0.17189371927977404, Average Test Meta Loss : 0.15268751224048166\n",
      "Epochh : 59000, Average Train Meta Loss : 0.16962781437262126, Average Test Meta Loss : 0.15062102456329557\n",
      "Epochh : 60000, Average Train Meta Loss : 0.16737952605748216, Average Test Meta Loss : 0.14870064559838117\n",
      "Epochh : 61000, Average Train Meta Loss : 0.165204690515388, Average Test Meta Loss : 0.14685216341779833\n",
      "Epochh : 62000, Average Train Meta Loss : 0.16305476668654592, Average Test Meta Loss : 0.14500886635421054\n",
      "Epochh : 63000, Average Train Meta Loss : 0.16109472251144222, Average Test Meta Loss : 0.14324367033770427\n",
      "Epochh : 64000, Average Train Meta Loss : 0.15910632403937158, Average Test Meta Loss : 0.14153978285636293\n",
      "Epochh : 65000, Average Train Meta Loss : 0.15723523234546227, Average Test Meta Loss : 0.13984163176760978\n",
      "Epochh : 66000, Average Train Meta Loss : 0.1553691841771873, Average Test Meta Loss : 0.13816225135433194\n",
      "Epochh : 67000, Average Train Meta Loss : 0.15354790894411285, Average Test Meta Loss : 0.13661219615485476\n",
      "Epochh : 68000, Average Train Meta Loss : 0.15180595493996676, Average Test Meta Loss : 0.13502100840199535\n",
      "Epochh : 69000, Average Train Meta Loss : 0.15005975578432557, Average Test Meta Loss : 0.13354872586747052\n",
      "Epochh : 70000, Average Train Meta Loss : 0.14839352900166122, Average Test Meta Loss : 0.1320885936210592\n",
      "Epochh : 71000, Average Train Meta Loss : 0.14674194861412856, Average Test Meta Loss : 0.13063162264448908\n",
      "Epochh : 72000, Average Train Meta Loss : 0.1451506362805891, Average Test Meta Loss : 0.1292573229108237\n",
      "Epochh : 73000, Average Train Meta Loss : 0.14364104230239993, Average Test Meta Loss : 0.1278658510181671\n",
      "Epochh : 74000, Average Train Meta Loss : 0.1421198807957671, Average Test Meta Loss : 0.12651948492104728\n",
      "Epochh : 75000, Average Train Meta Loss : 0.1406679492714102, Average Test Meta Loss : 0.1252432224328047\n",
      "Epochh : 76000, Average Train Meta Loss : 0.13924740366229785, Average Test Meta Loss : 0.12398582830215088\n",
      "Epochh : 77000, Average Train Meta Loss : 0.1378638195412222, Average Test Meta Loss : 0.12273443679917029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 78000, Average Train Meta Loss : 0.13647021599380407, Average Test Meta Loss : 0.12151050236459272\n",
      "Epochh : 79000, Average Train Meta Loss : 0.13513841312081923, Average Test Meta Loss : 0.12033158335328908\n",
      "Epochh : 80000, Average Train Meta Loss : 0.13385006273114122, Average Test Meta Loss : 0.1191705951057149\n",
      "Epochh : 81000, Average Train Meta Loss : 0.13258177667491056, Average Test Meta Loss : 0.11803374010155981\n",
      "Epochh : 82000, Average Train Meta Loss : 0.13132654719934397, Average Test Meta Loss : 0.11693738447811426\n",
      "Epochh : 83000, Average Train Meta Loss : 0.1301244331973532, Average Test Meta Loss : 0.11587891387978996\n",
      "Epochh : 84000, Average Train Meta Loss : 0.1289391383994853, Average Test Meta Loss : 0.11483675143773232\n",
      "Epochh : 85000, Average Train Meta Loss : 0.12780384313212145, Average Test Meta Loss : 0.11383102629810532\n",
      "Epochh : 86000, Average Train Meta Loss : 0.12665083098377017, Average Test Meta Loss : 0.11281114057494528\n",
      "Epochh : 87000, Average Train Meta Loss : 0.12555359179749853, Average Test Meta Loss : 0.11181801483533346\n",
      "Epochh : 88000, Average Train Meta Loss : 0.12445361366357498, Average Test Meta Loss : 0.11084461603871763\n",
      "Epochh : 89000, Average Train Meta Loss : 0.12341770789051638, Average Test Meta Loss : 0.1099144782290397\n",
      "Epochh : 90000, Average Train Meta Loss : 0.12240677593943101, Average Test Meta Loss : 0.10903455048596539\n",
      "Epochh : 91000, Average Train Meta Loss : 0.12139323667459297, Average Test Meta Loss : 0.10813211844799205\n",
      "Epochh : 92000, Average Train Meta Loss : 0.12039471747430694, Average Test Meta Loss : 0.10724423034815711\n",
      "Epochh : 93000, Average Train Meta Loss : 0.11943909875427429, Average Test Meta Loss : 0.10637134554200961\n",
      "Epochh : 94000, Average Train Meta Loss : 0.1184908650572506, Average Test Meta Loss : 0.10553996432366816\n",
      "Epochh : 95000, Average Train Meta Loss : 0.1175610693095749, Average Test Meta Loss : 0.10473062531262163\n",
      "Epochh : 96000, Average Train Meta Loss : 0.11665327628722874, Average Test Meta Loss : 0.10391559805109367\n",
      "Epochh : 97000, Average Train Meta Loss : 0.11574813780311952, Average Test Meta Loss : 0.10311737721578956\n",
      "Epochh : 98000, Average Train Meta Loss : 0.11487662114354105, Average Test Meta Loss : 0.10233561480804586\n",
      "Epochh : 99000, Average Train Meta Loss : 0.11400765586042717, Average Test Meta Loss : 0.10157406981563055\n",
      "Epochh : 100000, Average Train Meta Loss : 0.11315624050225412, Average Test Meta Loss : 0.10080838812394002\n",
      "Epochh : 101000, Average Train Meta Loss : 0.11230087985781281, Average Test Meta Loss : 0.10005528897889825\n",
      "Epochh : 102000, Average Train Meta Loss : 0.11147865024522258, Average Test Meta Loss : 0.09931768807883105\n",
      "Epochh : 103000, Average Train Meta Loss : 0.11065940837024721, Average Test Meta Loss : 0.09860904081713276\n",
      "Epochh : 104000, Average Train Meta Loss : 0.10986229947643382, Average Test Meta Loss : 0.09790181182317582\n",
      "Epochh : 105000, Average Train Meta Loss : 0.10908643060421563, Average Test Meta Loss : 0.09720617268775482\n",
      "Epochh : 106000, Average Train Meta Loss : 0.10831541294394334, Average Test Meta Loss : 0.09650857504907139\n",
      "Epochh : 107000, Average Train Meta Loss : 0.10758095492674379, Average Test Meta Loss : 0.09585128279715374\n",
      "Epochh : 108000, Average Train Meta Loss : 0.10685560593214312, Average Test Meta Loss : 0.09521101833386834\n",
      "Epochh : 109000, Average Train Meta Loss : 0.10613208688256752, Average Test Meta Loss : 0.09455312481555014\n",
      "Epochh : 110000, Average Train Meta Loss : 0.1054238453487199, Average Test Meta Loss : 0.09391473904322736\n",
      "Epochh : 111000, Average Train Meta Loss : 0.10472397686879574, Average Test Meta Loss : 0.09328692749698295\n",
      "Epochh : 112000, Average Train Meta Loss : 0.1040384360382707, Average Test Meta Loss : 0.09266940676811873\n",
      "Epochh : 113000, Average Train Meta Loss : 0.10334823738449475, Average Test Meta Loss : 0.09205855030274722\n",
      "Epochh : 114000, Average Train Meta Loss : 0.10268399204789606, Average Test Meta Loss : 0.09148302588746227\n",
      "Epochh : 115000, Average Train Meta Loss : 0.10206041226959159, Average Test Meta Loss : 0.09090008706910434\n",
      "Epochh : 116000, Average Train Meta Loss : 0.10142564401807895, Average Test Meta Loss : 0.09033632371133783\n",
      "Epochh : 117000, Average Train Meta Loss : 0.1007762652413495, Average Test Meta Loss : 0.08978455215613142\n",
      "Epochh : 118000, Average Train Meta Loss : 0.10014584659830852, Average Test Meta Loss : 0.0892412964489177\n",
      "Epochh : 119000, Average Train Meta Loss : 0.0995173955714782, Average Test Meta Loss : 0.08869672178821202\n",
      "Epochh : 120000, Average Train Meta Loss : 0.09890404167231781, Average Test Meta Loss : 0.08814747400334744\n",
      "Epochh : 121000, Average Train Meta Loss : 0.09831490107069721, Average Test Meta Loss : 0.0876182077252777\n",
      "Epochh : 122000, Average Train Meta Loss : 0.09772896275148744, Average Test Meta Loss : 0.0871030030975342\n",
      "Epochh : 123000, Average Train Meta Loss : 0.09717859071468128, Average Test Meta Loss : 0.08660829862415947\n",
      "Epochh : 124000, Average Train Meta Loss : 0.09661970874481862, Average Test Meta Loss : 0.08611582284730687\n",
      "Epochh : 125000, Average Train Meta Loss : 0.09605403575788268, Average Test Meta Loss : 0.08562106323087147\n",
      "Epochh : 126000, Average Train Meta Loss : 0.09549561594480482, Average Test Meta Loss : 0.08512693621477739\n",
      "Epochh : 127000, Average Train Meta Loss : 0.09494250832916741, Average Test Meta Loss : 0.08465123088049309\n",
      "Epochh : 128000, Average Train Meta Loss : 0.09442249351345489, Average Test Meta Loss : 0.0842045947854295\n",
      "Epochh : 129000, Average Train Meta Loss : 0.09389907592042507, Average Test Meta Loss : 0.08373297390384273\n",
      "Epochh : 130000, Average Train Meta Loss : 0.0933844582180205, Average Test Meta Loss : 0.08328928739770886\n",
      "Epochh : 131000, Average Train Meta Loss : 0.09295820918879956, Average Test Meta Loss : 0.08285139151751689\n",
      "Epochh : 132000, Average Train Meta Loss : 0.09243910461738632, Average Test Meta Loss : 0.08241775915536446\n",
      "Epochh : 133000, Average Train Meta Loss : 0.091934952381928, Average Test Meta Loss : 0.08197546242261364\n",
      "Epochh : 134000, Average Train Meta Loss : 0.09146166492746784, Average Test Meta Loss : 0.08154521222196379\n",
      "Epochh : 135000, Average Train Meta Loss : 0.09098690539578744, Average Test Meta Loss : 0.08111218374540817\n",
      "Epochh : 136000, Average Train Meta Loss : 0.09052703373836245, Average Test Meta Loss : 0.08069178515833397\n",
      "Epochh : 137000, Average Train Meta Loss : 0.09004929642282289, Average Test Meta Loss : 0.08027926723083294\n",
      "Epochh : 138000, Average Train Meta Loss : 0.08956834144679524, Average Test Meta Loss : 0.07986480411401521\n",
      "Epochh : 139000, Average Train Meta Loss : 0.08910314132725766, Average Test Meta Loss : 0.07947446043176219\n",
      "Epochh : 140000, Average Train Meta Loss : 0.08865600050889984, Average Test Meta Loss : 0.07908860225705286\n",
      "Epochh : 141000, Average Train Meta Loss : 0.08821951982547604, Average Test Meta Loss : 0.07869328644774344\n",
      "Epochh : 142000, Average Train Meta Loss : 0.08780580097867759, Average Test Meta Loss : 0.07830279047492222\n",
      "Epochh : 143000, Average Train Meta Loss : 0.08737613252799094, Average Test Meta Loss : 0.07790910751648344\n",
      "Epochh : 144000, Average Train Meta Loss : 0.08696471438483766, Average Test Meta Loss : 0.0775419664760225\n",
      "Epochh : 145000, Average Train Meta Loss : 0.08655229953573762, Average Test Meta Loss : 0.07716676725380038\n",
      "Epochh : 146000, Average Train Meta Loss : 0.08612571883384287, Average Test Meta Loss : 0.07679481984946403\n",
      "Epochh : 147000, Average Train Meta Loss : 0.08572714035976756, Average Test Meta Loss : 0.07643492981022404\n",
      "Epochh : 148000, Average Train Meta Loss : 0.08532079766399524, Average Test Meta Loss : 0.0760872242125871\n",
      "Epochh : 149000, Average Train Meta Loss : 0.0849277419977091, Average Test Meta Loss : 0.07572970352103113\n",
      "Epochh : 150000, Average Train Meta Loss : 0.08452374467853446, Average Test Meta Loss : 0.075381055275821\n",
      "Epochh : 151000, Average Train Meta Loss : 0.08414724346855319, Average Test Meta Loss : 0.07504631022016965\n",
      "Epochh : 152000, Average Train Meta Loss : 0.08375211404003692, Average Test Meta Loss : 0.07471311859663678\n",
      "Epochh : 153000, Average Train Meta Loss : 0.08338265011584794, Average Test Meta Loss : 0.07437012006960564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 154000, Average Train Meta Loss : 0.08300565915494151, Average Test Meta Loss : 0.07403369712905802\n",
      "Epochh : 155000, Average Train Meta Loss : 0.08264646797817075, Average Test Meta Loss : 0.0737025670536819\n",
      "Epochh : 156000, Average Train Meta Loss : 0.08228143914156272, Average Test Meta Loss : 0.07337777876443358\n",
      "Epochh : 157000, Average Train Meta Loss : 0.08192518433976925, Average Test Meta Loss : 0.07306877589000133\n",
      "Epochh : 158000, Average Train Meta Loss : 0.08158457466346382, Average Test Meta Loss : 0.07276989737223707\n",
      "Epochh : 159000, Average Train Meta Loss : 0.08122746619147996, Average Test Meta Loss : 0.07245455230700187\n",
      "Epochh : 160000, Average Train Meta Loss : 0.08088773003709379, Average Test Meta Loss : 0.07214362643746211\n",
      "Epochh : 161000, Average Train Meta Loss : 0.08053997045704725, Average Test Meta Loss : 0.07184217545394704\n",
      "Epochh : 162000, Average Train Meta Loss : 0.08022693925124824, Average Test Meta Loss : 0.07154445573849288\n",
      "Epochh : 163000, Average Train Meta Loss : 0.07990387766871429, Average Test Meta Loss : 0.07125403701263822\n",
      "Epochh : 164000, Average Train Meta Loss : 0.0795727377988574, Average Test Meta Loss : 0.07095964492226388\n",
      "Epochh : 165000, Average Train Meta Loss : 0.0792434108241324, Average Test Meta Loss : 0.070679435249211\n",
      "Epochh : 166000, Average Train Meta Loss : 0.07891331777038828, Average Test Meta Loss : 0.07039631497606662\n",
      "Epochh : 167000, Average Train Meta Loss : 0.07859482217573519, Average Test Meta Loss : 0.07011520974580837\n",
      "Epochh : 168000, Average Train Meta Loss : 0.0782738834316459, Average Test Meta Loss : 0.06983545171107522\n",
      "Epochh : 169000, Average Train Meta Loss : 0.07796929785330288, Average Test Meta Loss : 0.06955375096077886\n",
      "Epochh : 170000, Average Train Meta Loss : 0.07764926653783799, Average Test Meta Loss : 0.0692760540204712\n",
      "Epochh : 171000, Average Train Meta Loss : 0.07732944357477534, Average Test Meta Loss : 0.0690133751961365\n",
      "Epochh : 172000, Average Train Meta Loss : 0.07704284772210983, Average Test Meta Loss : 0.06874098073191295\n",
      "Epochh : 173000, Average Train Meta Loss : 0.0767356821141135, Average Test Meta Loss : 0.06846832763329659\n",
      "Epochh : 174000, Average Train Meta Loss : 0.07643758202234734, Average Test Meta Loss : 0.06821502768978079\n",
      "Epochh : 175000, Average Train Meta Loss : 0.07614589277732267, Average Test Meta Loss : 0.06796272236841035\n",
      "Epochh : 176000, Average Train Meta Loss : 0.07585584425436838, Average Test Meta Loss : 0.06771067826213993\n",
      "Epochh : 177000, Average Train Meta Loss : 0.07556655304946668, Average Test Meta Loss : 0.06745239555674154\n",
      "Epochh : 178000, Average Train Meta Loss : 0.07529454325000509, Average Test Meta Loss : 0.0671938795275256\n",
      "Epochh : 179000, Average Train Meta Loss : 0.07500417825487896, Average Test Meta Loss : 0.06694242546461925\n",
      "Epochh : 180000, Average Train Meta Loss : 0.07473295367500764, Average Test Meta Loss : 0.06669141167817011\n",
      "Epochh : 181000, Average Train Meta Loss : 0.07446315029352775, Average Test Meta Loss : 0.0664472486487046\n",
      "Epochh : 182000, Average Train Meta Loss : 0.0741990982211742, Average Test Meta Loss : 0.06620177422901179\n",
      "Epochh : 183000, Average Train Meta Loss : 0.07393402319355685, Average Test Meta Loss : 0.06597254000713974\n",
      "Epochh : 184000, Average Train Meta Loss : 0.07368324294637264, Average Test Meta Loss : 0.06573344488141596\n",
      "Epochh : 185000, Average Train Meta Loss : 0.07341438854135746, Average Test Meta Loss : 0.06552062916913522\n",
      "Epochh : 186000, Average Train Meta Loss : 0.07315606636621663, Average Test Meta Loss : 0.065290398987366\n",
      "Epochh : 187000, Average Train Meta Loss : 0.07289497599095238, Average Test Meta Loss : 0.0650629542375446\n",
      "Epochh : 188000, Average Train Meta Loss : 0.07264132701848056, Average Test Meta Loss : 0.06484158135826637\n",
      "Epochh : 189000, Average Train Meta Loss : 0.07239398096436199, Average Test Meta Loss : 0.06462531837362279\n",
      "Epochh : 190000, Average Train Meta Loss : 0.07215626502997988, Average Test Meta Loss : 0.06440951207014921\n",
      "Epochh : 191000, Average Train Meta Loss : 0.07191868269886685, Average Test Meta Loss : 0.0641969590948531\n",
      "Epochh : 192000, Average Train Meta Loss : 0.07167672445466347, Average Test Meta Loss : 0.0639929949805062\n",
      "Epochh : 193000, Average Train Meta Loss : 0.071447109747562, Average Test Meta Loss : 0.06377668582511757\n",
      "Epochh : 194000, Average Train Meta Loss : 0.07119227064946866, Average Test Meta Loss : 0.06356201074541051\n",
      "Epochh : 195000, Average Train Meta Loss : 0.07096110598754207, Average Test Meta Loss : 0.06335823229432398\n",
      "Epochh : 196000, Average Train Meta Loss : 0.07072682501001921, Average Test Meta Loss : 0.06315698170566744\n",
      "Epochh : 197000, Average Train Meta Loss : 0.07050417519308172, Average Test Meta Loss : 0.06295813195843465\n",
      "Epochh : 198000, Average Train Meta Loss : 0.07027813861990354, Average Test Meta Loss : 0.06275556348341814\n",
      "Epochh : 199000, Average Train Meta Loss : 0.07004113392332911, Average Test Meta Loss : 0.06255971616329405\n",
      "Epochh : 200000, Average Train Meta Loss : 0.06981621945904248, Average Test Meta Loss : 0.06235571810529446\n",
      "Epochh : 201000, Average Train Meta Loss : 0.06959383647380164, Average Test Meta Loss : 0.06216492486091016\n",
      "Epochh : 202000, Average Train Meta Loss : 0.06937048169056409, Average Test Meta Loss : 0.06197532804619724\n",
      "Epochh : 203000, Average Train Meta Loss : 0.06916523760283001, Average Test Meta Loss : 0.061787122661749974\n",
      "Epochh : 204000, Average Train Meta Loss : 0.06894556156606503, Average Test Meta Loss : 0.06158989471105324\n",
      "Epochh : 205000, Average Train Meta Loss : 0.06873694266219652, Average Test Meta Loss : 0.06140949696985038\n",
      "Epochh : 206000, Average Train Meta Loss : 0.06851587662708405, Average Test Meta Loss : 0.06122784115728399\n",
      "Epochh : 207000, Average Train Meta Loss : 0.06830785416500855, Average Test Meta Loss : 0.06104423913886377\n",
      "Epochh : 208000, Average Train Meta Loss : 0.06811374576301986, Average Test Meta Loss : 0.06086505121503796\n",
      "Epochh : 209000, Average Train Meta Loss : 0.0679045011507514, Average Test Meta Loss : 0.060676788876924154\n",
      "Epochh : 210000, Average Train Meta Loss : 0.06769835328715737, Average Test Meta Loss : 0.060486367895953616\n",
      "Epochh : 211000, Average Train Meta Loss : 0.06749382697150884, Average Test Meta Loss : 0.06031203599316426\n",
      "Epochh : 212000, Average Train Meta Loss : 0.06730815561625739, Average Test Meta Loss : 0.06013952777153075\n",
      "Epochh : 213000, Average Train Meta Loss : 0.06710329145882087, Average Test Meta Loss : 0.059971057178585314\n",
      "Epochh : 214000, Average Train Meta Loss : 0.06691772159764456, Average Test Meta Loss : 0.059805593922625376\n",
      "Epochh : 215000, Average Train Meta Loss : 0.0667169839494089, Average Test Meta Loss : 0.05964336416397296\n",
      "Epochh : 216000, Average Train Meta Loss : 0.06652707104412696, Average Test Meta Loss : 0.05947691166986564\n",
      "Epochh : 217000, Average Train Meta Loss : 0.06633498004111854, Average Test Meta Loss : 0.05929973039333734\n",
      "Epochh : 218000, Average Train Meta Loss : 0.0661382843050566, Average Test Meta Loss : 0.05913863401398494\n",
      "Epochh : 219000, Average Train Meta Loss : 0.06594896999600534, Average Test Meta Loss : 0.058969452174227735\n",
      "Epochh : 220000, Average Train Meta Loss : 0.06576126846850955, Average Test Meta Loss : 0.058809071927759\n",
      "Epochh : 221000, Average Train Meta Loss : 0.0655701254131576, Average Test Meta Loss : 0.05864241518463332\n",
      "Epochh : 222000, Average Train Meta Loss : 0.06537689804676337, Average Test Meta Loss : 0.058480027636492946\n",
      "Epochh : 223000, Average Train Meta Loss : 0.06519019111950647, Average Test Meta Loss : 0.05831148575081314\n",
      "Epochh : 224000, Average Train Meta Loss : 0.06501122047402273, Average Test Meta Loss : 0.05814798273762433\n",
      "Epochh : 225000, Average Train Meta Loss : 0.06483336279471759, Average Test Meta Loss : 0.05799335017018913\n",
      "Epochh : 226000, Average Train Meta Loss : 0.06465442132429404, Average Test Meta Loss : 0.057838283531447904\n",
      "Epochh : 227000, Average Train Meta Loss : 0.06449464732462785, Average Test Meta Loss : 0.057688491277106006\n",
      "Epochh : 228000, Average Train Meta Loss : 0.06431834804172328, Average Test Meta Loss : 0.05753201248347081\n",
      "Epochh : 229000, Average Train Meta Loss : 0.06414713852288792, Average Test Meta Loss : 0.05737405734407945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 230000, Average Train Meta Loss : 0.06397732837902667, Average Test Meta Loss : 0.05722362447704256\n",
      "Epochh : 231000, Average Train Meta Loss : 0.06380023064384613, Average Test Meta Loss : 0.057067350858525275\n",
      "Epochh : 232000, Average Train Meta Loss : 0.06362787818830914, Average Test Meta Loss : 0.056907956656445435\n",
      "Epochh : 233000, Average Train Meta Loss : 0.06346021702933063, Average Test Meta Loss : 0.05676694525215705\n",
      "Epochh : 234000, Average Train Meta Loss : 0.06329714882415431, Average Test Meta Loss : 0.05661837549018454\n",
      "Epochh : 235000, Average Train Meta Loss : 0.06313286343332761, Average Test Meta Loss : 0.05646856590930669\n",
      "Epochh : 236000, Average Train Meta Loss : 0.06296872126462298, Average Test Meta Loss : 0.05632540441861287\n",
      "Epochh : 237000, Average Train Meta Loss : 0.06280583598960952, Average Test Meta Loss : 0.05617951262211654\n",
      "Epochh : 238000, Average Train Meta Loss : 0.06265223542435959, Average Test Meta Loss : 0.056038279222131324\n",
      "Epochh : 239000, Average Train Meta Loss : 0.06249724395415376, Average Test Meta Loss : 0.05590142884695356\n",
      "Epochh : 240000, Average Train Meta Loss : 0.062332735694283145, Average Test Meta Loss : 0.055751990144705894\n",
      "Epochh : 241000, Average Train Meta Loss : 0.06217592399326979, Average Test Meta Loss : 0.05560577332238022\n",
      "Epochh : 242000, Average Train Meta Loss : 0.06201731999560255, Average Test Meta Loss : 0.05546114300148359\n",
      "Epochh : 243000, Average Train Meta Loss : 0.06185389437522748, Average Test Meta Loss : 0.05531829539615888\n",
      "Epochh : 244000, Average Train Meta Loss : 0.06169759478130065, Average Test Meta Loss : 0.05518068691880375\n",
      "Epochh : 245000, Average Train Meta Loss : 0.06154182654710171, Average Test Meta Loss : 0.05504500843031112\n",
      "Epochh : 246000, Average Train Meta Loss : 0.06139071350433904, Average Test Meta Loss : 0.05490816811065214\n",
      "Epochh : 247000, Average Train Meta Loss : 0.06124127624056792, Average Test Meta Loss : 0.054779743015133886\n",
      "Epochh : 248000, Average Train Meta Loss : 0.06109554744019172, Average Test Meta Loss : 0.05465546578745913\n",
      "Epochh : 249000, Average Train Meta Loss : 0.06094240748063231, Average Test Meta Loss : 0.05452131684911297\n",
      "Epochh : 250000, Average Train Meta Loss : 0.06079144328873518, Average Test Meta Loss : 0.054388251371757894\n",
      "Epochh : 251000, Average Train Meta Loss : 0.06064143511738335, Average Test Meta Loss : 0.054261411015526395\n",
      "Epochh : 252000, Average Train Meta Loss : 0.060499335568833114, Average Test Meta Loss : 0.05413443597273395\n",
      "Epochh : 253000, Average Train Meta Loss : 0.060359009850635, Average Test Meta Loss : 0.05400706540217601\n",
      "Epochh : 254000, Average Train Meta Loss : 0.06023082060338033, Average Test Meta Loss : 0.05387756777945581\n",
      "Epochh : 255000, Average Train Meta Loss : 0.06008767343280895, Average Test Meta Loss : 0.05374828890056971\n",
      "Epochh : 256000, Average Train Meta Loss : 0.059943476125130755, Average Test Meta Loss : 0.053620833733739784\n",
      "Epochh : 257000, Average Train Meta Loss : 0.05979793318744736, Average Test Meta Loss : 0.053493288175903646\n",
      "Epochh : 258000, Average Train Meta Loss : 0.0596546499042782, Average Test Meta Loss : 0.05336955250758809\n",
      "Epochh : 259000, Average Train Meta Loss : 0.05951545079733981, Average Test Meta Loss : 0.05324825456803821\n",
      "Epochh : 260000, Average Train Meta Loss : 0.059383337872621386, Average Test Meta Loss : 0.05312707483397137\n",
      "Epochh : 261000, Average Train Meta Loss : 0.05924591172126848, Average Test Meta Loss : 0.053007891838372796\n",
      "Epochh : 262000, Average Train Meta Loss : 0.0591074086845694, Average Test Meta Loss : 0.05288840100593384\n",
      "Epochh : 263000, Average Train Meta Loss : 0.05898262667644659, Average Test Meta Loss : 0.05277027267526361\n",
      "Epochh : 264000, Average Train Meta Loss : 0.05885325063700414, Average Test Meta Loss : 0.052656324131990465\n",
      "Epochh : 265000, Average Train Meta Loss : 0.05872018275080841, Average Test Meta Loss : 0.05254169740555377\n",
      "Epochh : 266000, Average Train Meta Loss : 0.05858476171227869, Average Test Meta Loss : 0.05242432109192733\n",
      "Epochh : 267000, Average Train Meta Loss : 0.05845928715041802, Average Test Meta Loss : 0.05230634676553156\n",
      "Epochh : 268000, Average Train Meta Loss : 0.05833249120593254, Average Test Meta Loss : 0.05218951002146766\n",
      "Epochh : 269000, Average Train Meta Loss : 0.058202310590120064, Average Test Meta Loss : 0.05207270256187179\n",
      "Epochh : 270000, Average Train Meta Loss : 0.05807409778806323, Average Test Meta Loss : 0.05195950017710363\n",
      "Epochh : 271000, Average Train Meta Loss : 0.05794464003005985, Average Test Meta Loss : 0.05185662135479736\n",
      "Epochh : 272000, Average Train Meta Loss : 0.057815815538787646, Average Test Meta Loss : 0.05174773313891426\n",
      "Epochh : 273000, Average Train Meta Loss : 0.05768930621601495, Average Test Meta Loss : 0.05163912138375668\n",
      "Epochh : 274000, Average Train Meta Loss : 0.057573070636564104, Average Test Meta Loss : 0.051531424429576624\n",
      "Epochh : 275000, Average Train Meta Loss : 0.05745690366194856, Average Test Meta Loss : 0.051425493893251496\n",
      "Epochh : 276000, Average Train Meta Loss : 0.05733403228540346, Average Test Meta Loss : 0.051318468950906845\n",
      "Epochh : 277000, Average Train Meta Loss : 0.057210613417723696, Average Test Meta Loss : 0.05121224061294917\n",
      "Epochh : 278000, Average Train Meta Loss : 0.05709032505690613, Average Test Meta Loss : 0.051106820595665264\n",
      "Epochh : 279000, Average Train Meta Loss : 0.05697603238912498, Average Test Meta Loss : 0.05100136189878651\n",
      "Epochh : 280000, Average Train Meta Loss : 0.05685500736768756, Average Test Meta Loss : 0.050899046389198635\n",
      "Epochh : 281000, Average Train Meta Loss : 0.05673073372226957, Average Test Meta Loss : 0.050796066053380544\n",
      "Epochh : 282000, Average Train Meta Loss : 0.056614975545717025, Average Test Meta Loss : 0.050689185391377324\n",
      "Epochh : 283000, Average Train Meta Loss : 0.05649738033257015, Average Test Meta Loss : 0.050589134268961666\n",
      "Epochh : 284000, Average Train Meta Loss : 0.05638337795579987, Average Test Meta Loss : 0.05048673054357175\n",
      "Epochh : 285000, Average Train Meta Loss : 0.05626906359139981, Average Test Meta Loss : 0.050380393795502146\n",
      "Epochh : 286000, Average Train Meta Loss : 0.05614617109550513, Average Test Meta Loss : 0.050279932139730525\n",
      "Epochh : 287000, Average Train Meta Loss : 0.05603462113146305, Average Test Meta Loss : 0.050179934293372454\n",
      "Epochh : 288000, Average Train Meta Loss : 0.05591544354606807, Average Test Meta Loss : 0.05007855069055337\n",
      "Epochh : 289000, Average Train Meta Loss : 0.055799458079664355, Average Test Meta Loss : 0.049980334794314955\n",
      "Epochh : 290000, Average Train Meta Loss : 0.05568504481687711, Average Test Meta Loss : 0.049877497418770886\n",
      "Epochh : 291000, Average Train Meta Loss : 0.05557984711580513, Average Test Meta Loss : 0.04978229952567333\n",
      "Epochh : 292000, Average Train Meta Loss : 0.05547069685278516, Average Test Meta Loss : 0.04968394499747256\n",
      "Epochh : 293000, Average Train Meta Loss : 0.05536174280003328, Average Test Meta Loss : 0.04959100868338203\n",
      "Epochh : 294000, Average Train Meta Loss : 0.05525447211144193, Average Test Meta Loss : 0.04949923508751149\n",
      "Epochh : 295000, Average Train Meta Loss : 0.05514435549455809, Average Test Meta Loss : 0.04941241925544921\n",
      "Epochh : 296000, Average Train Meta Loss : 0.05504977065942246, Average Test Meta Loss : 0.04931709697010199\n",
      "Epochh : 297000, Average Train Meta Loss : 0.054949387740704986, Average Test Meta Loss : 0.04922965035460667\n",
      "Epochh : 298000, Average Train Meta Loss : 0.05484736885797106, Average Test Meta Loss : 0.04913744591337033\n",
      "Epochh : 299000, Average Train Meta Loss : 0.054741169886276905, Average Test Meta Loss : 0.0490460793338648\n",
      "Epochh : 300000, Average Train Meta Loss : 0.05463672514098792, Average Test Meta Loss : 0.04895307582760601\n",
      "Epochh : 301000, Average Train Meta Loss : 0.05453805577339607, Average Test Meta Loss : 0.048858118930267695\n",
      "Epochh : 302000, Average Train Meta Loss : 0.05443199072020764, Average Test Meta Loss : 0.04876719040838011\n",
      "Epochh : 303000, Average Train Meta Loss : 0.05433104730677131, Average Test Meta Loss : 0.0486772567547347\n",
      "Epochh : 304000, Average Train Meta Loss : 0.05422600946714556, Average Test Meta Loss : 0.04858656446073588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 305000, Average Train Meta Loss : 0.05412264438155721, Average Test Meta Loss : 0.048496374390706566\n",
      "Epochh : 306000, Average Train Meta Loss : 0.054029701975482694, Average Test Meta Loss : 0.04840668264465334\n",
      "Epochh : 307000, Average Train Meta Loss : 0.05393092346770655, Average Test Meta Loss : 0.04831980752239132\n",
      "Epochh : 308000, Average Train Meta Loss : 0.05383240950653249, Average Test Meta Loss : 0.04822814473761157\n",
      "Epochh : 309000, Average Train Meta Loss : 0.05373914209353107, Average Test Meta Loss : 0.04814472296162063\n",
      "Epochh : 310000, Average Train Meta Loss : 0.053640063541738246, Average Test Meta Loss : 0.04805937542121311\n",
      "Epochh : 311000, Average Train Meta Loss : 0.053541716075332195, Average Test Meta Loss : 0.047972629943937355\n",
      "Epochh : 312000, Average Train Meta Loss : 0.05344799638577648, Average Test Meta Loss : 0.0478940322709344\n",
      "Epochh : 313000, Average Train Meta Loss : 0.053351886940385396, Average Test Meta Loss : 0.04780645995634705\n",
      "Epochh : 314000, Average Train Meta Loss : 0.05325308411207188, Average Test Meta Loss : 0.04771649495846055\n",
      "Epochh : 315000, Average Train Meta Loss : 0.05316324706001242, Average Test Meta Loss : 0.047631901787268886\n",
      "Epochh : 316000, Average Train Meta Loss : 0.05306929671647953, Average Test Meta Loss : 0.04754785624673447\n",
      "Epochh : 317000, Average Train Meta Loss : 0.052976155830770304, Average Test Meta Loss : 0.04746248660217455\n",
      "Epochh : 318000, Average Train Meta Loss : 0.05287908124945956, Average Test Meta Loss : 0.04738127255234755\n",
      "Epochh : 319000, Average Train Meta Loss : 0.05278464640595372, Average Test Meta Loss : 0.04730245354815981\n",
      "Epochh : 320000, Average Train Meta Loss : 0.052696071658332296, Average Test Meta Loss : 0.047223723642700485\n",
      "Epochh : 321000, Average Train Meta Loss : 0.05260183763624081, Average Test Meta Loss : 0.04714326259663232\n",
      "Epochh : 322000, Average Train Meta Loss : 0.05251181611373397, Average Test Meta Loss : 0.04706222160778497\n",
      "Epochh : 323000, Average Train Meta Loss : 0.05242331722377143, Average Test Meta Loss : 0.04698527752508496\n",
      "Epochh : 324000, Average Train Meta Loss : 0.052336513228795165, Average Test Meta Loss : 0.046906932902312085\n",
      "Epochh : 325000, Average Train Meta Loss : 0.05224785364491887, Average Test Meta Loss : 0.046831685097215386\n",
      "Epochh : 326000, Average Train Meta Loss : 0.052156687732984874, Average Test Meta Loss : 0.04675067035164898\n",
      "Epochh : 327000, Average Train Meta Loss : 0.05206665279713131, Average Test Meta Loss : 0.04667207539731089\n",
      "Epochh : 328000, Average Train Meta Loss : 0.0519783321897318, Average Test Meta Loss : 0.04659377934582562\n",
      "Epochh : 329000, Average Train Meta Loss : 0.05189029711471501, Average Test Meta Loss : 0.04651853728810435\n",
      "Epochh : 330000, Average Train Meta Loss : 0.05180797147502641, Average Test Meta Loss : 0.046445788229063915\n",
      "Epochh : 331000, Average Train Meta Loss : 0.0517273364298372, Average Test Meta Loss : 0.04637252943262291\n",
      "Epochh : 332000, Average Train Meta Loss : 0.051647325573665526, Average Test Meta Loss : 0.046299578104131864\n",
      "Epochh : 333000, Average Train Meta Loss : 0.05156425218850306, Average Test Meta Loss : 0.04622498382790348\n",
      "Epochh : 334000, Average Train Meta Loss : 0.05147801819735195, Average Test Meta Loss : 0.04615154135814261\n",
      "Epochh : 335000, Average Train Meta Loss : 0.0513909915722408, Average Test Meta Loss : 0.04607702365313581\n",
      "Epochh : 336000, Average Train Meta Loss : 0.05130820670107301, Average Test Meta Loss : 0.04600870577083914\n",
      "Epochh : 337000, Average Train Meta Loss : 0.051228707407269855, Average Test Meta Loss : 0.04593781029931162\n",
      "Epochh : 338000, Average Train Meta Loss : 0.0511454172354046, Average Test Meta Loss : 0.04587063217069203\n",
      "Epochh : 339000, Average Train Meta Loss : 0.05107088631834602, Average Test Meta Loss : 0.04579598705336011\n",
      "Epochh : 340000, Average Train Meta Loss : 0.05099323666734743, Average Test Meta Loss : 0.045727400579083415\n",
      "Epochh : 341000, Average Train Meta Loss : 0.05091025876109026, Average Test Meta Loss : 0.04565235775669522\n",
      "Epochh : 342000, Average Train Meta Loss : 0.050827388633060966, Average Test Meta Loss : 0.04558191004841807\n",
      "Epochh : 343000, Average Train Meta Loss : 0.050745248137452015, Average Test Meta Loss : 0.04551233013886476\n",
      "Epochh : 344000, Average Train Meta Loss : 0.050669111745795276, Average Test Meta Loss : 0.0454389881021872\n",
      "Epochh : 345000, Average Train Meta Loss : 0.050593379243469905, Average Test Meta Loss : 0.04537168688311597\n",
      "Epochh : 346000, Average Train Meta Loss : 0.0505111393181482, Average Test Meta Loss : 0.04530514537916732\n",
      "Epochh : 347000, Average Train Meta Loss : 0.05044122553411494, Average Test Meta Loss : 0.04523833678535843\n",
      "Epochh : 348000, Average Train Meta Loss : 0.05036272588279129, Average Test Meta Loss : 0.04516873184462048\n",
      "Epochh : 349000, Average Train Meta Loss : 0.05028676955446139, Average Test Meta Loss : 0.04509873137656355\n",
      "Epochh : 350000, Average Train Meta Loss : 0.05020854505506199, Average Test Meta Loss : 0.045030433068685496\n",
      "Epochh : 351000, Average Train Meta Loss : 0.050128156525181176, Average Test Meta Loss : 0.04496040799863359\n",
      "Epochh : 352000, Average Train Meta Loss : 0.05005158301821187, Average Test Meta Loss : 0.044893973789292455\n",
      "Epochh : 353000, Average Train Meta Loss : 0.04997619501343594, Average Test Meta Loss : 0.04482251241218739\n",
      "Epochh : 354000, Average Train Meta Loss : 0.049905196637144815, Average Test Meta Loss : 0.04475550929427871\n",
      "Epochh : 355000, Average Train Meta Loss : 0.04983471260752172, Average Test Meta Loss : 0.044690386797845434\n",
      "Epochh : 356000, Average Train Meta Loss : 0.049760011713425575, Average Test Meta Loss : 0.04462686753038973\n",
      "Epochh : 357000, Average Train Meta Loss : 0.04968471316241863, Average Test Meta Loss : 0.044559594570414675\n",
      "Epochh : 358000, Average Train Meta Loss : 0.04961538855991085, Average Test Meta Loss : 0.04449314074891073\n",
      "Epochh : 359000, Average Train Meta Loss : 0.04954314089737374, Average Test Meta Loss : 0.04442787767143081\n",
      "Epochh : 360000, Average Train Meta Loss : 0.04946693777615022, Average Test Meta Loss : 0.044364203607923355\n",
      "Epochh : 361000, Average Train Meta Loss : 0.049394071598327934, Average Test Meta Loss : 0.04430584323378642\n",
      "Epochh : 362000, Average Train Meta Loss : 0.04932209779243998, Average Test Meta Loss : 0.044241372411786335\n",
      "Epochh : 363000, Average Train Meta Loss : 0.049247401526981566, Average Test Meta Loss : 0.04417463012292003\n",
      "Epochh : 364000, Average Train Meta Loss : 0.04917581846051751, Average Test Meta Loss : 0.04411343937199307\n",
      "Epochh : 365000, Average Train Meta Loss : 0.049107772340471, Average Test Meta Loss : 0.044054943650690084\n",
      "Epochh : 366000, Average Train Meta Loss : 0.04903136438345392, Average Test Meta Loss : 0.04399531891795118\n",
      "Epochh : 367000, Average Train Meta Loss : 0.048962921383028664, Average Test Meta Loss : 0.04393754949181341\n",
      "Epochh : 368000, Average Train Meta Loss : 0.04889442411591527, Average Test Meta Loss : 0.04388036318725485\n",
      "Epochh : 369000, Average Train Meta Loss : 0.04882627016701507, Average Test Meta Loss : 0.04382210689268932\n",
      "Epochh : 370000, Average Train Meta Loss : 0.04876125557845645, Average Test Meta Loss : 0.04376490915361334\n",
      "Epochh : 371000, Average Train Meta Loss : 0.04869286660904996, Average Test Meta Loss : 0.04370335321695145\n",
      "Epochh : 372000, Average Train Meta Loss : 0.048626885454913935, Average Test Meta Loss : 0.04364690839707616\n",
      "Epochh : 373000, Average Train Meta Loss : 0.04856536936218496, Average Test Meta Loss : 0.04359128013249572\n",
      "Epochh : 374000, Average Train Meta Loss : 0.04849932478464799, Average Test Meta Loss : 0.04353191212673065\n",
      "Epochh : 375000, Average Train Meta Loss : 0.04843442535043868, Average Test Meta Loss : 0.04347852408093788\n",
      "Epochh : 376000, Average Train Meta Loss : 0.048369812273537496, Average Test Meta Loss : 0.04342052475407629\n",
      "Epochh : 377000, Average Train Meta Loss : 0.04830180827833669, Average Test Meta Loss : 0.043363444239687264\n",
      "Epochh : 378000, Average Train Meta Loss : 0.04823487953039626, Average Test Meta Loss : 0.04330740377383339\n",
      "Epochh : 379000, Average Train Meta Loss : 0.048171479619943704, Average Test Meta Loss : 0.04325130180619554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 380000, Average Train Meta Loss : 0.04810591933844561, Average Test Meta Loss : 0.043192564756018965\n",
      "Epochh : 381000, Average Train Meta Loss : 0.048039442971445294, Average Test Meta Loss : 0.04313005742471731\n",
      "Epochh : 382000, Average Train Meta Loss : 0.047972797699498894, Average Test Meta Loss : 0.043074854153662134\n",
      "Epochh : 383000, Average Train Meta Loss : 0.047915268243983414, Average Test Meta Loss : 0.043017305554388936\n",
      "Epochh : 384000, Average Train Meta Loss : 0.04785134965182722, Average Test Meta Loss : 0.042965032153448664\n",
      "Epochh : 385000, Average Train Meta Loss : 0.0477906035753327, Average Test Meta Loss : 0.04291652419682223\n",
      "Epochh : 386000, Average Train Meta Loss : 0.04772430830495238, Average Test Meta Loss : 0.04285980231800682\n",
      "Epochh : 387000, Average Train Meta Loss : 0.04765947992631881, Average Test Meta Loss : 0.04280692676834974\n",
      "Epochh : 388000, Average Train Meta Loss : 0.04759816676371115, Average Test Meta Loss : 0.04275042858197876\n",
      "Epochh : 389000, Average Train Meta Loss : 0.047534841097007445, Average Test Meta Loss : 0.042694468333230665\n",
      "Epochh : 390000, Average Train Meta Loss : 0.04747709665300328, Average Test Meta Loss : 0.04263846700146532\n",
      "Epochh : 391000, Average Train Meta Loss : 0.04741487956785884, Average Test Meta Loss : 0.04258639134495948\n",
      "Epochh : 392000, Average Train Meta Loss : 0.047351456636626876, Average Test Meta Loss : 0.04253468461419991\n",
      "Epochh : 393000, Average Train Meta Loss : 0.04728937560557716, Average Test Meta Loss : 0.04248427969366159\n",
      "Epochh : 394000, Average Train Meta Loss : 0.04723079001569495, Average Test Meta Loss : 0.04242635216122979\n",
      "Epochh : 395000, Average Train Meta Loss : 0.04716812747729422, Average Test Meta Loss : 0.042372080548860803\n",
      "Epochh : 396000, Average Train Meta Loss : 0.047112502758210245, Average Test Meta Loss : 0.04232160179583951\n",
      "Epochh : 397000, Average Train Meta Loss : 0.047050594211381934, Average Test Meta Loss : 0.04227060570801771\n",
      "Epochh : 398000, Average Train Meta Loss : 0.04699491810187098, Average Test Meta Loss : 0.042217795414227606\n",
      "Epochh : 399000, Average Train Meta Loss : 0.046933412152494816, Average Test Meta Loss : 0.04216490855083081\n",
      "Epochh : 400000, Average Train Meta Loss : 0.046880640011984005, Average Test Meta Loss : 0.042117299238596126\n",
      "Epochh : 401000, Average Train Meta Loss : 0.04681969380063843, Average Test Meta Loss : 0.042063548217773056\n",
      "Epochh : 402000, Average Train Meta Loss : 0.04676228169889005, Average Test Meta Loss : 0.04201372499404695\n",
      "Epochh : 403000, Average Train Meta Loss : 0.04670637082975064, Average Test Meta Loss : 0.04196322860967548\n",
      "Epochh : 404000, Average Train Meta Loss : 0.04665156974899233, Average Test Meta Loss : 0.04191055681766334\n",
      "Epochh : 405000, Average Train Meta Loss : 0.04660055530718703, Average Test Meta Loss : 0.04186582032300808\n",
      "Epochh : 406000, Average Train Meta Loss : 0.0465414901523146, Average Test Meta Loss : 0.041822461182563817\n",
      "Epochh : 407000, Average Train Meta Loss : 0.046490516282875365, Average Test Meta Loss : 0.04177195503922123\n",
      "Epochh : 408000, Average Train Meta Loss : 0.04643347865677819, Average Test Meta Loss : 0.04172389001122225\n",
      "Epochh : 409000, Average Train Meta Loss : 0.04637958645743341, Average Test Meta Loss : 0.041674083189664216\n",
      "Epochh : 410000, Average Train Meta Loss : 0.046328934157394704, Average Test Meta Loss : 0.04162101967863047\n",
      "Epochh : 411000, Average Train Meta Loss : 0.04627153928065141, Average Test Meta Loss : 0.041571930995652345\n",
      "Epochh : 412000, Average Train Meta Loss : 0.04621690500622733, Average Test Meta Loss : 0.04152164412252492\n",
      "Epochh : 413000, Average Train Meta Loss : 0.04616175858040949, Average Test Meta Loss : 0.0414724414008869\n",
      "Epochh : 414000, Average Train Meta Loss : 0.04610357943215799, Average Test Meta Loss : 0.04142604072884263\n",
      "Epochh : 415000, Average Train Meta Loss : 0.04604822930599619, Average Test Meta Loss : 0.041380845636259087\n",
      "Epochh : 416000, Average Train Meta Loss : 0.045994460404516595, Average Test Meta Loss : 0.04133328092715898\n",
      "Epochh : 417000, Average Train Meta Loss : 0.045941148765453244, Average Test Meta Loss : 0.041285587430020025\n",
      "Epochh : 418000, Average Train Meta Loss : 0.04588513566534286, Average Test Meta Loss : 0.04123830572928941\n",
      "Epochh : 419000, Average Train Meta Loss : 0.045836363020310814, Average Test Meta Loss : 0.04119431313144044\n",
      "Epochh : 420000, Average Train Meta Loss : 0.045783662156204136, Average Test Meta Loss : 0.04114953066510382\n",
      "Epochh : 421000, Average Train Meta Loss : 0.04573180028920869, Average Test Meta Loss : 0.04110281761209111\n",
      "Epochh : 422000, Average Train Meta Loss : 0.04567998029334968, Average Test Meta Loss : 0.041054644372560116\n",
      "Epochh : 423000, Average Train Meta Loss : 0.0456267724716775, Average Test Meta Loss : 0.04100935850365377\n",
      "Epochh : 424000, Average Train Meta Loss : 0.045578787572654815, Average Test Meta Loss : 0.040963775250372175\n",
      "Epochh : 425000, Average Train Meta Loss : 0.04552549056960465, Average Test Meta Loss : 0.0409172600916355\n",
      "Epochh : 426000, Average Train Meta Loss : 0.04547517619107407, Average Test Meta Loss : 0.040867737296312004\n",
      "Epochh : 427000, Average Train Meta Loss : 0.045422489985837367, Average Test Meta Loss : 0.040824572720513326\n",
      "Epochh : 428000, Average Train Meta Loss : 0.04537130819639522, Average Test Meta Loss : 0.04077307443210939\n",
      "Epochh : 429000, Average Train Meta Loss : 0.04531609747138289, Average Test Meta Loss : 0.04072635609714093\n",
      "Epochh : 430000, Average Train Meta Loss : 0.045261891651386145, Average Test Meta Loss : 0.040688583924952906\n",
      "Epochh : 431000, Average Train Meta Loss : 0.04521254460597972, Average Test Meta Loss : 0.04064389588449799\n",
      "Epochh : 432000, Average Train Meta Loss : 0.04516369289680894, Average Test Meta Loss : 0.04060294745641385\n",
      "Epochh : 433000, Average Train Meta Loss : 0.045111359797270746, Average Test Meta Loss : 0.04055838455205915\n",
      "Epochh : 434000, Average Train Meta Loss : 0.04506490980973124, Average Test Meta Loss : 0.04051633164798545\n",
      "Epochh : 435000, Average Train Meta Loss : 0.04501848283043824, Average Test Meta Loss : 0.04047498767242852\n",
      "Epochh : 436000, Average Train Meta Loss : 0.044967569519319156, Average Test Meta Loss : 0.0404292515823832\n",
      "Epochh : 437000, Average Train Meta Loss : 0.04491434218944449, Average Test Meta Loss : 0.04038428185607629\n",
      "Epochh : 438000, Average Train Meta Loss : 0.044866354078100995, Average Test Meta Loss : 0.04033823684667029\n",
      "Epochh : 439000, Average Train Meta Loss : 0.0448219801127164, Average Test Meta Loss : 0.04029306929624473\n",
      "Epochh : 440000, Average Train Meta Loss : 0.044768375653138945, Average Test Meta Loss : 0.04025043058039585\n",
      "Epochh : 441000, Average Train Meta Loss : 0.04471759523302535, Average Test Meta Loss : 0.040207092770491254\n",
      "Epochh : 442000, Average Train Meta Loss : 0.044666785121614344, Average Test Meta Loss : 0.04016390750123415\n",
      "Epochh : 443000, Average Train Meta Loss : 0.044620480042041236, Average Test Meta Loss : 0.040119078579300645\n",
      "Epochh : 444000, Average Train Meta Loss : 0.04457286159290542, Average Test Meta Loss : 0.04007894191398581\n",
      "Epochh : 445000, Average Train Meta Loss : 0.044523894964615326, Average Test Meta Loss : 0.04003568918763437\n",
      "Epochh : 446000, Average Train Meta Loss : 0.04447234410509345, Average Test Meta Loss : 0.03999501802016823\n",
      "Epochh : 447000, Average Train Meta Loss : 0.044424348144341146, Average Test Meta Loss : 0.03995503031476865\n",
      "Epochh : 448000, Average Train Meta Loss : 0.04437472710212031, Average Test Meta Loss : 0.039912484095022197\n",
      "Epochh : 449000, Average Train Meta Loss : 0.04432885753086512, Average Test Meta Loss : 0.03987035688927954\n",
      "Epochh : 450000, Average Train Meta Loss : 0.0442812879756765, Average Test Meta Loss : 0.039828740666353335\n",
      "Epochh : 451000, Average Train Meta Loss : 0.04423444473909533, Average Test Meta Loss : 0.0397869557388859\n",
      "Epochh : 452000, Average Train Meta Loss : 0.04419134612226033, Average Test Meta Loss : 0.039743120422171575\n",
      "Epochh : 453000, Average Train Meta Loss : 0.044145710216339375, Average Test Meta Loss : 0.03970446872504625\n",
      "Epochh : 454000, Average Train Meta Loss : 0.04409997474781643, Average Test Meta Loss : 0.039665902271816204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 455000, Average Train Meta Loss : 0.044054230433908353, Average Test Meta Loss : 0.03962195810771266\n",
      "Epochh : 456000, Average Train Meta Loss : 0.04400971737666909, Average Test Meta Loss : 0.0395841423166413\n",
      "Epochh : 457000, Average Train Meta Loss : 0.04396217453796567, Average Test Meta Loss : 0.03954484024551099\n",
      "Epochh : 458000, Average Train Meta Loss : 0.043917464117967785, Average Test Meta Loss : 0.03950219153518061\n",
      "Epochh : 459000, Average Train Meta Loss : 0.04387303270465637, Average Test Meta Loss : 0.039462588906613735\n",
      "Epochh : 460000, Average Train Meta Loss : 0.043826661549356986, Average Test Meta Loss : 0.0394216816627921\n",
      "Epochh : 461000, Average Train Meta Loss : 0.04378272174833429, Average Test Meta Loss : 0.03938564106685614\n",
      "Epochh : 462000, Average Train Meta Loss : 0.043738017332014816, Average Test Meta Loss : 0.03935040193719919\n",
      "Epochh : 463000, Average Train Meta Loss : 0.043694958918543886, Average Test Meta Loss : 0.03931190575116163\n",
      "Epochh : 464000, Average Train Meta Loss : 0.043654537967763696, Average Test Meta Loss : 0.039274255271781446\n",
      "Epochh : 465000, Average Train Meta Loss : 0.04361189750994968, Average Test Meta Loss : 0.03923574330282829\n",
      "Epochh : 466000, Average Train Meta Loss : 0.04356902095123693, Average Test Meta Loss : 0.03920063765179147\n",
      "Epochh : 467000, Average Train Meta Loss : 0.04352501916987279, Average Test Meta Loss : 0.03916472252584071\n",
      "Epochh : 468000, Average Train Meta Loss : 0.04348142347239846, Average Test Meta Loss : 0.03912989117077223\n",
      "Epochh : 469000, Average Train Meta Loss : 0.043441097105732344, Average Test Meta Loss : 0.03909276849672808\n",
      "Epochh : 470000, Average Train Meta Loss : 0.04340117635759797, Average Test Meta Loss : 0.039060019416924135\n",
      "Epochh : 471000, Average Train Meta Loss : 0.04335903601321219, Average Test Meta Loss : 0.03902698930092174\n",
      "Epochh : 472000, Average Train Meta Loss : 0.04331841693968836, Average Test Meta Loss : 0.038990296680059607\n",
      "Epochh : 473000, Average Train Meta Loss : 0.043274546093193526, Average Test Meta Loss : 0.03895117671679715\n",
      "Epochh : 474000, Average Train Meta Loss : 0.0432328663096869, Average Test Meta Loss : 0.03891355734423242\n",
      "Epochh : 475000, Average Train Meta Loss : 0.043189895921706585, Average Test Meta Loss : 0.03887812217625978\n",
      "Epochh : 476000, Average Train Meta Loss : 0.04314372836573796, Average Test Meta Loss : 0.03884251198755841\n",
      "Epochh : 477000, Average Train Meta Loss : 0.04310452323239845, Average Test Meta Loss : 0.03880435473477559\n",
      "Epochh : 478000, Average Train Meta Loss : 0.0430650238655919, Average Test Meta Loss : 0.03876682874652039\n",
      "Epochh : 479000, Average Train Meta Loss : 0.043024755777947454, Average Test Meta Loss : 0.03873174390297624\n",
      "Epochh : 480000, Average Train Meta Loss : 0.042988078313344565, Average Test Meta Loss : 0.03869768314697156\n",
      "Epochh : 481000, Average Train Meta Loss : 0.04295236606884694, Average Test Meta Loss : 0.03866499008838632\n",
      "Epochh : 482000, Average Train Meta Loss : 0.042913149648970715, Average Test Meta Loss : 0.038626674469673494\n",
      "Epochh : 483000, Average Train Meta Loss : 0.04287493024936926, Average Test Meta Loss : 0.03859483756905412\n",
      "Epochh : 484000, Average Train Meta Loss : 0.04283353070300968, Average Test Meta Loss : 0.03856132092018566\n",
      "Epochh : 485000, Average Train Meta Loss : 0.042793742186137546, Average Test Meta Loss : 0.038526396277521475\n",
      "Epochh : 486000, Average Train Meta Loss : 0.042752716183696304, Average Test Meta Loss : 0.0384920266335119\n",
      "Epochh : 487000, Average Train Meta Loss : 0.04271674276405302, Average Test Meta Loss : 0.03845504057341259\n",
      "Epochh : 488000, Average Train Meta Loss : 0.04267795579306447, Average Test Meta Loss : 0.03842131472518425\n",
      "Epochh : 489000, Average Train Meta Loss : 0.0426452019000284, Average Test Meta Loss : 0.03839171099110974\n",
      "Epochh : 490000, Average Train Meta Loss : 0.042609221691483574, Average Test Meta Loss : 0.03836822673451832\n",
      "Epochh : 491000, Average Train Meta Loss : 0.042568632646074266, Average Test Meta Loss : 0.03833655402049783\n",
      "Epochh : 492000, Average Train Meta Loss : 0.042528451629560204, Average Test Meta Loss : 0.03830065837600651\n",
      "Epochh : 493000, Average Train Meta Loss : 0.042485959056979786, Average Test Meta Loss : 0.03826305273854556\n",
      "Epochh : 494000, Average Train Meta Loss : 0.04244682345812518, Average Test Meta Loss : 0.03823064905174205\n",
      "Epochh : 495000, Average Train Meta Loss : 0.042412968350298653, Average Test Meta Loss : 0.038199315674784816\n",
      "Epochh : 496000, Average Train Meta Loss : 0.042369368238100745, Average Test Meta Loss : 0.03816530144475125\n",
      "Epochh : 497000, Average Train Meta Loss : 0.042336900199517545, Average Test Meta Loss : 0.038132979118453494\n",
      "Epochh : 498000, Average Train Meta Loss : 0.042301455142710574, Average Test Meta Loss : 0.03809854463530988\n",
      "Epochh : 499000, Average Train Meta Loss : 0.04226573081699363, Average Test Meta Loss : 0.03806456415366331\n",
      "Epochh : 500000, Average Train Meta Loss : 0.04222461792144913, Average Test Meta Loss : 0.03803139786747718\n",
      "Epochh : 501000, Average Train Meta Loss : 0.042186042653133346, Average Test Meta Loss : 0.03800096602033317\n",
      "Epochh : 502000, Average Train Meta Loss : 0.04215077829443834, Average Test Meta Loss : 0.03796709954655769\n",
      "Epochh : 503000, Average Train Meta Loss : 0.042115955898409475, Average Test Meta Loss : 0.03793659462054718\n",
      "Epochh : 504000, Average Train Meta Loss : 0.042081097708591794, Average Test Meta Loss : 0.037906397111885606\n",
      "Epochh : 505000, Average Train Meta Loss : 0.04204449646880454, Average Test Meta Loss : 0.03787857444256052\n",
      "Epochh : 506000, Average Train Meta Loss : 0.04200700026949095, Average Test Meta Loss : 0.03784698626115244\n",
      "Epochh : 507000, Average Train Meta Loss : 0.0419719034065467, Average Test Meta Loss : 0.037811901547178235\n",
      "Epochh : 508000, Average Train Meta Loss : 0.041935681461337, Average Test Meta Loss : 0.03778072944021093\n",
      "Epochh : 509000, Average Train Meta Loss : 0.0418999694862416, Average Test Meta Loss : 0.03775146695602593\n",
      "Epochh : 510000, Average Train Meta Loss : 0.04186649622495163, Average Test Meta Loss : 0.03772012079861752\n",
      "Epochh : 511000, Average Train Meta Loss : 0.04182999266576283, Average Test Meta Loss : 0.03769099246803278\n",
      "Epochh : 512000, Average Train Meta Loss : 0.04179887911960559, Average Test Meta Loss : 0.03766554300204825\n",
      "Epochh : 513000, Average Train Meta Loss : 0.0417612658787939, Average Test Meta Loss : 0.037635471814368666\n",
      "Epochh : 514000, Average Train Meta Loss : 0.04172752970969219, Average Test Meta Loss : 0.03760876980802962\n",
      "Epochh : 515000, Average Train Meta Loss : 0.04169301759118591, Average Test Meta Loss : 0.037576748514461834\n",
      "Epochh : 516000, Average Train Meta Loss : 0.04165588476940677, Average Test Meta Loss : 0.037546403641502975\n",
      "Epochh : 517000, Average Train Meta Loss : 0.04162447159624304, Average Test Meta Loss : 0.03751606744022053\n",
      "Epochh : 518000, Average Train Meta Loss : 0.041589548762561684, Average Test Meta Loss : 0.03748542749570067\n",
      "Epochh : 519000, Average Train Meta Loss : 0.041557468742584515, Average Test Meta Loss : 0.03745334964495274\n",
      "Epochh : 520000, Average Train Meta Loss : 0.04152518356736159, Average Test Meta Loss : 0.03742408679554854\n",
      "Epochh : 521000, Average Train Meta Loss : 0.04148843248229289, Average Test Meta Loss : 0.03739501254798434\n",
      "Epochh : 522000, Average Train Meta Loss : 0.04145617298577534, Average Test Meta Loss : 0.03736451202328677\n",
      "Epochh : 523000, Average Train Meta Loss : 0.0414232626557985, Average Test Meta Loss : 0.03733572498650498\n",
      "Epochh : 524000, Average Train Meta Loss : 0.041391790106135515, Average Test Meta Loss : 0.03730736470814272\n",
      "Epochh : 525000, Average Train Meta Loss : 0.041363849254899665, Average Test Meta Loss : 0.0372798268657734\n",
      "Epochh : 526000, Average Train Meta Loss : 0.04133473386733517, Average Test Meta Loss : 0.037249543848082434\n",
      "Epochh : 527000, Average Train Meta Loss : 0.041307231468627664, Average Test Meta Loss : 0.03722087844752891\n",
      "Epochh : 528000, Average Train Meta Loss : 0.04127444929774383, Average Test Meta Loss : 0.037189355636859614\n",
      "Epochh : 529000, Average Train Meta Loss : 0.04124705165967826, Average Test Meta Loss : 0.0371625222049557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 530000, Average Train Meta Loss : 0.04121592218136808, Average Test Meta Loss : 0.03713589890454504\n",
      "Epochh : 531000, Average Train Meta Loss : 0.04118095241503536, Average Test Meta Loss : 0.03710380301637553\n",
      "Epochh : 532000, Average Train Meta Loss : 0.04115031442487513, Average Test Meta Loss : 0.03707540659317501\n",
      "Epochh : 533000, Average Train Meta Loss : 0.04111679072207422, Average Test Meta Loss : 0.03704682479958092\n",
      "Epochh : 534000, Average Train Meta Loss : 0.04108440609909071, Average Test Meta Loss : 0.037018115616744895\n",
      "Epochh : 535000, Average Train Meta Loss : 0.0410512400528446, Average Test Meta Loss : 0.036990524151406926\n",
      "Epochh : 536000, Average Train Meta Loss : 0.04101648271258239, Average Test Meta Loss : 0.03696173909299034\n",
      "Epochh : 537000, Average Train Meta Loss : 0.040983239393110486, Average Test Meta Loss : 0.036931225569089475\n",
      "Epochh : 538000, Average Train Meta Loss : 0.04094972995203495, Average Test Meta Loss : 0.036903803354506624\n",
      "Epochh : 539000, Average Train Meta Loss : 0.04091449268984267, Average Test Meta Loss : 0.0368753246142287\n",
      "Epochh : 540000, Average Train Meta Loss : 0.04088061299945435, Average Test Meta Loss : 0.036848532143407046\n",
      "Epochh : 541000, Average Train Meta Loss : 0.04084839663451888, Average Test Meta Loss : 0.03682041729270376\n",
      "Epochh : 542000, Average Train Meta Loss : 0.04081530029558737, Average Test Meta Loss : 0.036788963902728435\n",
      "Epochh : 543000, Average Train Meta Loss : 0.04078128699703446, Average Test Meta Loss : 0.03675979749588189\n",
      "Epochh : 544000, Average Train Meta Loss : 0.0407477923734097, Average Test Meta Loss : 0.03673282211769094\n",
      "Epochh : 545000, Average Train Meta Loss : 0.04072178662643572, Average Test Meta Loss : 0.03670404445950372\n",
      "Epochh : 546000, Average Train Meta Loss : 0.04069305111950099, Average Test Meta Loss : 0.036677376330392385\n",
      "Epochh : 547000, Average Train Meta Loss : 0.04066292843970685, Average Test Meta Loss : 0.036651025581846346\n",
      "Epochh : 548000, Average Train Meta Loss : 0.040631688812894015, Average Test Meta Loss : 0.03662637810302734\n",
      "Epochh : 549000, Average Train Meta Loss : 0.04060466529439932, Average Test Meta Loss : 0.036606592159185915\n",
      "Epochh : 550000, Average Train Meta Loss : 0.04057815549390252, Average Test Meta Loss : 0.03658038758304818\n",
      "Epochh : 551000, Average Train Meta Loss : 0.040550706331977826, Average Test Meta Loss : 0.036555253289521104\n",
      "Epochh : 552000, Average Train Meta Loss : 0.04052127011591329, Average Test Meta Loss : 0.03653258536141442\n",
      "Epochh : 553000, Average Train Meta Loss : 0.040489782095830804, Average Test Meta Loss : 0.03650529799178503\n",
      "Epochh : 554000, Average Train Meta Loss : 0.040460322636458684, Average Test Meta Loss : 0.036478123642360874\n",
      "Epochh : 555000, Average Train Meta Loss : 0.04042745946525912, Average Test Meta Loss : 0.03645005414126954\n",
      "Epochh : 556000, Average Train Meta Loss : 0.040394094662198364, Average Test Meta Loss : 0.03642317779340774\n",
      "Epochh : 557000, Average Train Meta Loss : 0.04036406746521767, Average Test Meta Loss : 0.03639659175239532\n",
      "Epochh : 558000, Average Train Meta Loss : 0.040335513549907984, Average Test Meta Loss : 0.03637349358573988\n",
      "Epochh : 559000, Average Train Meta Loss : 0.04030820656413991, Average Test Meta Loss : 0.03634835059713302\n",
      "Epochh : 560000, Average Train Meta Loss : 0.04028247917295759, Average Test Meta Loss : 0.036322558349859396\n",
      "Epochh : 561000, Average Train Meta Loss : 0.04025377269963233, Average Test Meta Loss : 0.0362950676525895\n",
      "Epochh : 562000, Average Train Meta Loss : 0.040225926768591894, Average Test Meta Loss : 0.036271730437299865\n",
      "Epochh : 563000, Average Train Meta Loss : 0.040199123960546194, Average Test Meta Loss : 0.036246050405375665\n",
      "Epochh : 564000, Average Train Meta Loss : 0.0401663408916562, Average Test Meta Loss : 0.03622087990423657\n",
      "Epochh : 565000, Average Train Meta Loss : 0.04013533196451749, Average Test Meta Loss : 0.0361984250498677\n",
      "Epochh : 566000, Average Train Meta Loss : 0.0401064036705806, Average Test Meta Loss : 0.03617371925868743\n",
      "Epochh : 567000, Average Train Meta Loss : 0.04007480461919258, Average Test Meta Loss : 0.03614840732798155\n",
      "Epochh : 568000, Average Train Meta Loss : 0.040047769936009545, Average Test Meta Loss : 0.03612155882823424\n",
      "Epochh : 569000, Average Train Meta Loss : 0.040022207471056115, Average Test Meta Loss : 0.03609434271672985\n",
      "Epochh : 570000, Average Train Meta Loss : 0.03999326626135836, Average Test Meta Loss : 0.036067790970713116\n",
      "Epochh : 571000, Average Train Meta Loss : 0.0399651681688452, Average Test Meta Loss : 0.03604094288484014\n",
      "Epochh : 572000, Average Train Meta Loss : 0.039939422607847115, Average Test Meta Loss : 0.0360137604957344\n",
      "Epochh : 573000, Average Train Meta Loss : 0.03990991956162191, Average Test Meta Loss : 0.035987944741634546\n",
      "Epochh : 574000, Average Train Meta Loss : 0.039882966522677823, Average Test Meta Loss : 0.035962744997007856\n",
      "Epochh : 575000, Average Train Meta Loss : 0.039853053513021805, Average Test Meta Loss : 0.035934215622730796\n",
      "Epochh : 576000, Average Train Meta Loss : 0.039824920006984336, Average Test Meta Loss : 0.03590870062936878\n",
      "Epochh : 577000, Average Train Meta Loss : 0.03979852463698337, Average Test Meta Loss : 0.03588405101956657\n",
      "Epochh : 578000, Average Train Meta Loss : 0.0397689344511135, Average Test Meta Loss : 0.035863049491205376\n",
      "Epochh : 579000, Average Train Meta Loss : 0.039738518938010885, Average Test Meta Loss : 0.03583948966032066\n",
      "Epochh : 580000, Average Train Meta Loss : 0.03971330205522479, Average Test Meta Loss : 0.03581423277047946\n",
      "Epochh : 581000, Average Train Meta Loss : 0.039685932174188164, Average Test Meta Loss : 0.03578982154501589\n",
      "Epochh : 582000, Average Train Meta Loss : 0.03965605799571172, Average Test Meta Loss : 0.03576781875320915\n",
      "Epochh : 583000, Average Train Meta Loss : 0.039626404305881, Average Test Meta Loss : 0.03574473947592783\n",
      "Epochh : 584000, Average Train Meta Loss : 0.03959716097690436, Average Test Meta Loss : 0.035718330256462524\n",
      "Epochh : 585000, Average Train Meta Loss : 0.039569168665218396, Average Test Meta Loss : 0.03569457432077681\n",
      "Epochh : 586000, Average Train Meta Loss : 0.03954328725373631, Average Test Meta Loss : 0.035670738097076764\n",
      "Epochh : 587000, Average Train Meta Loss : 0.03951263307436911, Average Test Meta Loss : 0.035648847819683305\n",
      "Epochh : 588000, Average Train Meta Loss : 0.03948558811380516, Average Test Meta Loss : 0.03562737255798194\n",
      "Epochh : 589000, Average Train Meta Loss : 0.03946559829357335, Average Test Meta Loss : 0.035606398316838095\n",
      "Epochh : 590000, Average Train Meta Loss : 0.039441100858425664, Average Test Meta Loss : 0.03558327521756549\n",
      "Epochh : 591000, Average Train Meta Loss : 0.039413469680332026, Average Test Meta Loss : 0.03556074873230863\n",
      "Epochh : 592000, Average Train Meta Loss : 0.039387280334068145, Average Test Meta Loss : 0.03553787852833619\n",
      "Epochh : 593000, Average Train Meta Loss : 0.039359431016808825, Average Test Meta Loss : 0.03551356429438567\n",
      "Epochh : 594000, Average Train Meta Loss : 0.03933391668400237, Average Test Meta Loss : 0.035488295236585254\n",
      "Epochh : 595000, Average Train Meta Loss : 0.03930789670285286, Average Test Meta Loss : 0.035464347547029054\n",
      "Epochh : 596000, Average Train Meta Loss : 0.039283518295603774, Average Test Meta Loss : 0.03544224558199956\n",
      "Epochh : 597000, Average Train Meta Loss : 0.03925969032337641, Average Test Meta Loss : 0.03542313647221051\n",
      "Epochh : 598000, Average Train Meta Loss : 0.03923208126807154, Average Test Meta Loss : 0.03540215682883729\n",
      "Epochh : 599000, Average Train Meta Loss : 0.03920828184305084, Average Test Meta Loss : 0.035381528243255196\n",
      "Epochh : 600000, Average Train Meta Loss : 0.03918754191746797, Average Test Meta Loss : 0.03536034312629447\n",
      "Epochh : 601000, Average Train Meta Loss : 0.0391618796389013, Average Test Meta Loss : 0.03533781277565592\n",
      "Epochh : 602000, Average Train Meta Loss : 0.03913517742876426, Average Test Meta Loss : 0.03531621093719979\n",
      "Epochh : 603000, Average Train Meta Loss : 0.03911007833374965, Average Test Meta Loss : 0.03529337536301336\n",
      "Epochh : 604000, Average Train Meta Loss : 0.03908390999538189, Average Test Meta Loss : 0.03527024653603619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 605000, Average Train Meta Loss : 0.03905486502751441, Average Test Meta Loss : 0.035246175477932976\n",
      "Epochh : 606000, Average Train Meta Loss : 0.039027789293695915, Average Test Meta Loss : 0.03522174581492361\n",
      "Epochh : 607000, Average Train Meta Loss : 0.039003789144819466, Average Test Meta Loss : 0.03520085972842414\n",
      "Epochh : 608000, Average Train Meta Loss : 0.03897953033306045, Average Test Meta Loss : 0.03518182346100036\n",
      "Epochh : 609000, Average Train Meta Loss : 0.03895381960552826, Average Test Meta Loss : 0.035159063632105046\n",
      "Epochh : 610000, Average Train Meta Loss : 0.03892625897501673, Average Test Meta Loss : 0.035138342921723006\n",
      "Epochh : 611000, Average Train Meta Loss : 0.03889953290562624, Average Test Meta Loss : 0.03511665578311813\n",
      "Epochh : 612000, Average Train Meta Loss : 0.03887224218673327, Average Test Meta Loss : 0.03509571238976991\n",
      "Epochh : 613000, Average Train Meta Loss : 0.03884710749113276, Average Test Meta Loss : 0.03507096488846951\n",
      "Epochh : 614000, Average Train Meta Loss : 0.03882162987153221, Average Test Meta Loss : 0.03504939376699322\n",
      "Epochh : 615000, Average Train Meta Loss : 0.03879753391604708, Average Test Meta Loss : 0.035024853160530896\n",
      "Epochh : 616000, Average Train Meta Loss : 0.038772331691878076, Average Test Meta Loss : 0.035002594363077504\n",
      "Epochh : 617000, Average Train Meta Loss : 0.03874482469796657, Average Test Meta Loss : 0.03497838420783039\n",
      "Epochh : 618000, Average Train Meta Loss : 0.038720767353823564, Average Test Meta Loss : 0.03495697000044361\n",
      "Epochh : 619000, Average Train Meta Loss : 0.03869789587867608, Average Test Meta Loss : 0.034933406616039164\n",
      "Epochh : 620000, Average Train Meta Loss : 0.038673801081441396, Average Test Meta Loss : 0.03491105844832557\n",
      "Epochh : 621000, Average Train Meta Loss : 0.038647046293089814, Average Test Meta Loss : 0.03488725573473658\n",
      "Epochh : 622000, Average Train Meta Loss : 0.03862462208967534, Average Test Meta Loss : 0.03486705610575071\n",
      "Epochh : 623000, Average Train Meta Loss : 0.038603352097569654, Average Test Meta Loss : 0.03484621378108308\n",
      "Epochh : 624000, Average Train Meta Loss : 0.03857931206736528, Average Test Meta Loss : 0.03482815573686852\n",
      "Epochh : 625000, Average Train Meta Loss : 0.03855280115392767, Average Test Meta Loss : 0.03480691723250458\n",
      "Epochh : 626000, Average Train Meta Loss : 0.03853008804377933, Average Test Meta Loss : 0.03478383723126256\n",
      "Epochh : 627000, Average Train Meta Loss : 0.03850477485253083, Average Test Meta Loss : 0.03476334693644027\n",
      "Epochh : 628000, Average Train Meta Loss : 0.03848214247258749, Average Test Meta Loss : 0.03474152108884719\n",
      "Epochh : 629000, Average Train Meta Loss : 0.038454449628296535, Average Test Meta Loss : 0.034717403042457955\n",
      "Epochh : 630000, Average Train Meta Loss : 0.03842854834063989, Average Test Meta Loss : 0.03469458496883814\n",
      "Epochh : 631000, Average Train Meta Loss : 0.03840325930291093, Average Test Meta Loss : 0.03467247991993588\n",
      "Epochh : 632000, Average Train Meta Loss : 0.03837867102122783, Average Test Meta Loss : 0.03464951526924729\n",
      "Epochh : 633000, Average Train Meta Loss : 0.03835212682958805, Average Test Meta Loss : 0.034628811122079335\n",
      "Epochh : 634000, Average Train Meta Loss : 0.03832951145547609, Average Test Meta Loss : 0.03460751537624921\n",
      "Epochh : 635000, Average Train Meta Loss : 0.03830153826603265, Average Test Meta Loss : 0.0345847857725641\n",
      "Epochh : 636000, Average Train Meta Loss : 0.038274656583326135, Average Test Meta Loss : 0.03456445171985561\n",
      "Epochh : 637000, Average Train Meta Loss : 0.03825465479945972, Average Test Meta Loss : 0.03454119319081745\n",
      "Epochh : 638000, Average Train Meta Loss : 0.0382278373686349, Average Test Meta Loss : 0.03451943381553412\n",
      "Epochh : 639000, Average Train Meta Loss : 0.03820457100070215, Average Test Meta Loss : 0.034500218544203896\n",
      "Epochh : 640000, Average Train Meta Loss : 0.038181398741143346, Average Test Meta Loss : 0.034479039530163666\n",
      "Epochh : 641000, Average Train Meta Loss : 0.038158890776072873, Average Test Meta Loss : 0.03446003294672442\n",
      "Epochh : 642000, Average Train Meta Loss : 0.03813520225891406, Average Test Meta Loss : 0.034441697599323744\n",
      "Epochh : 643000, Average Train Meta Loss : 0.038112532562190264, Average Test Meta Loss : 0.03442305072054107\n",
      "Epochh : 644000, Average Train Meta Loss : 0.038090580681373866, Average Test Meta Loss : 0.0344032556614599\n",
      "Epochh : 645000, Average Train Meta Loss : 0.03806972634750197, Average Test Meta Loss : 0.03438384238276735\n",
      "Epochh : 646000, Average Train Meta Loss : 0.03805011965421861, Average Test Meta Loss : 0.03436169204587345\n",
      "Epochh : 647000, Average Train Meta Loss : 0.03802539373819985, Average Test Meta Loss : 0.03434070831125009\n",
      "Epochh : 648000, Average Train Meta Loss : 0.03800633314551987, Average Test Meta Loss : 0.03432192194647832\n",
      "Epochh : 649000, Average Train Meta Loss : 0.037981200903497866, Average Test Meta Loss : 0.03430060072158731\n",
      "Epochh : 650000, Average Train Meta Loss : 0.03796009527531596, Average Test Meta Loss : 0.034283538506437956\n",
      "Epochh : 651000, Average Train Meta Loss : 0.0379337445471846, Average Test Meta Loss : 0.034261756414152855\n",
      "Epochh : 652000, Average Train Meta Loss : 0.037912968181215013, Average Test Meta Loss : 0.03424275214865359\n",
      "Epochh : 653000, Average Train Meta Loss : 0.037891793371309754, Average Test Meta Loss : 0.034225387058236696\n",
      "Epochh : 654000, Average Train Meta Loss : 0.03787191454187486, Average Test Meta Loss : 0.03420798845217487\n",
      "Epochh : 655000, Average Train Meta Loss : 0.03784796245219774, Average Test Meta Loss : 0.03418875654577089\n",
      "Epochh : 656000, Average Train Meta Loss : 0.037824862281843515, Average Test Meta Loss : 0.034166929325018365\n",
      "Epochh : 657000, Average Train Meta Loss : 0.03780126536650595, Average Test Meta Loss : 0.034147035993436124\n",
      "Epochh : 658000, Average Train Meta Loss : 0.03778012529179078, Average Test Meta Loss : 0.03412678207775257\n",
      "Epochh : 659000, Average Train Meta Loss : 0.03775600544753172, Average Test Meta Loss : 0.03410787521734999\n",
      "Epochh : 660000, Average Train Meta Loss : 0.037736048460078456, Average Test Meta Loss : 0.03408882886350327\n",
      "Epochh : 661000, Average Train Meta Loss : 0.037712684224469016, Average Test Meta Loss : 0.03406564662525958\n",
      "Epochh : 662000, Average Train Meta Loss : 0.03769104760242042, Average Test Meta Loss : 0.03404664254466204\n",
      "Epochh : 663000, Average Train Meta Loss : 0.0376714325407908, Average Test Meta Loss : 0.034025304979589735\n",
      "Epochh : 664000, Average Train Meta Loss : 0.03764812463185061, Average Test Meta Loss : 0.034006617888129004\n",
      "Epochh : 665000, Average Train Meta Loss : 0.03762330840893619, Average Test Meta Loss : 0.03398730070793563\n",
      "Epochh : 666000, Average Train Meta Loss : 0.03760045533315241, Average Test Meta Loss : 0.03396866093662619\n",
      "Epochh : 667000, Average Train Meta Loss : 0.03757960233505442, Average Test Meta Loss : 0.033950753249327684\n",
      "Epochh : 668000, Average Train Meta Loss : 0.03755669777618566, Average Test Meta Loss : 0.03393366807795013\n",
      "Epochh : 669000, Average Train Meta Loss : 0.037534267066453124, Average Test Meta Loss : 0.03391509418979368\n",
      "Epochh : 670000, Average Train Meta Loss : 0.03751152741320041, Average Test Meta Loss : 0.0338978863046809\n",
      "Epochh : 671000, Average Train Meta Loss : 0.03749231706662748, Average Test Meta Loss : 0.033881463333502324\n",
      "Epochh : 672000, Average Train Meta Loss : 0.037472955592707226, Average Test Meta Loss : 0.03386365069977725\n",
      "Epochh : 673000, Average Train Meta Loss : 0.03745455951619893, Average Test Meta Loss : 0.03384503808694394\n",
      "Epochh : 674000, Average Train Meta Loss : 0.03743456323456642, Average Test Meta Loss : 0.033827159080908185\n",
      "Epochh : 675000, Average Train Meta Loss : 0.037415392664433805, Average Test Meta Loss : 0.033807422201333\n",
      "Epochh : 676000, Average Train Meta Loss : 0.037393283307845346, Average Test Meta Loss : 0.03378917736984813\n",
      "Epochh : 677000, Average Train Meta Loss : 0.03737052812690902, Average Test Meta Loss : 0.03376756193777037\n",
      "Epochh : 678000, Average Train Meta Loss : 0.0373494741843866, Average Test Meta Loss : 0.033747886852191655\n",
      "Epochh : 679000, Average Train Meta Loss : 0.03733051030217014, Average Test Meta Loss : 0.033730567446397595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 680000, Average Train Meta Loss : 0.03731304446270171, Average Test Meta Loss : 0.03371514630936809\n",
      "Epochh : 681000, Average Train Meta Loss : 0.03729099003013676, Average Test Meta Loss : 0.03369622534459346\n",
      "Epochh : 682000, Average Train Meta Loss : 0.03726983370131356, Average Test Meta Loss : 0.03367699711311206\n",
      "Epochh : 683000, Average Train Meta Loss : 0.037248631599108774, Average Test Meta Loss : 0.03365757678273598\n",
      "Epochh : 684000, Average Train Meta Loss : 0.03722954373204831, Average Test Meta Loss : 0.033639996077327775\n",
      "Epochh : 685000, Average Train Meta Loss : 0.0372114253802866, Average Test Meta Loss : 0.03362107263441295\n",
      "Epochh : 686000, Average Train Meta Loss : 0.03719181144370583, Average Test Meta Loss : 0.03360478921048319\n",
      "Epochh : 687000, Average Train Meta Loss : 0.03716769545077383, Average Test Meta Loss : 0.03358719888072263\n",
      "Epochh : 688000, Average Train Meta Loss : 0.037146642486697384, Average Test Meta Loss : 0.03356688464412216\n",
      "Epochh : 689000, Average Train Meta Loss : 0.037126629197189266, Average Test Meta Loss : 0.033547517768156315\n",
      "Epochh : 690000, Average Train Meta Loss : 0.03710488661023686, Average Test Meta Loss : 0.03352953781633319\n",
      "Epochh : 691000, Average Train Meta Loss : 0.03708640766104885, Average Test Meta Loss : 0.033513652717925925\n",
      "Epochh : 692000, Average Train Meta Loss : 0.03706815169711298, Average Test Meta Loss : 0.03349666067241447\n",
      "Epochh : 693000, Average Train Meta Loss : 0.037046032251366386, Average Test Meta Loss : 0.0334805407849545\n",
      "Epochh : 694000, Average Train Meta Loss : 0.037029733111920156, Average Test Meta Loss : 0.03346267756137217\n",
      "Epochh : 695000, Average Train Meta Loss : 0.037009059811320695, Average Test Meta Loss : 0.033444938646138664\n",
      "Epochh : 696000, Average Train Meta Loss : 0.03698963906825254, Average Test Meta Loss : 0.03342950108638316\n",
      "Epochh : 697000, Average Train Meta Loss : 0.03697177129808461, Average Test Meta Loss : 0.03341292328733994\n",
      "Epochh : 698000, Average Train Meta Loss : 0.03695247203808922, Average Test Meta Loss : 0.033394921342587196\n",
      "Epochh : 699000, Average Train Meta Loss : 0.03692851180165656, Average Test Meta Loss : 0.03337533018575738\n",
      "Epochh : 700000, Average Train Meta Loss : 0.03690990095676492, Average Test Meta Loss : 0.033355347194958335\n",
      "Epochh : 701000, Average Train Meta Loss : 0.03689234613700731, Average Test Meta Loss : 0.033339615447689354\n",
      "Epochh : 702000, Average Train Meta Loss : 0.036870979522249235, Average Test Meta Loss : 0.03332273011473384\n",
      "Epochh : 703000, Average Train Meta Loss : 0.036851850241688286, Average Test Meta Loss : 0.03330579899239865\n",
      "Epochh : 704000, Average Train Meta Loss : 0.03683254215111915, Average Test Meta Loss : 0.0332896126389972\n",
      "Epochh : 705000, Average Train Meta Loss : 0.03681175342663061, Average Test Meta Loss : 0.0332765017271459\n",
      "Epochh : 706000, Average Train Meta Loss : 0.036791417449183496, Average Test Meta Loss : 0.03325650739321116\n",
      "Epochh : 707000, Average Train Meta Loss : 0.03677454730620644, Average Test Meta Loss : 0.033240589855436424\n",
      "Epochh : 708000, Average Train Meta Loss : 0.03675591232840935, Average Test Meta Loss : 0.033222766142151354\n",
      "Epochh : 709000, Average Train Meta Loss : 0.03673481069951847, Average Test Meta Loss : 0.03320500151034333\n",
      "Epochh : 710000, Average Train Meta Loss : 0.03671722097130025, Average Test Meta Loss : 0.033187280292133865\n",
      "Epochh : 711000, Average Train Meta Loss : 0.03669663797526973, Average Test Meta Loss : 0.03317060855329322\n",
      "Epochh : 712000, Average Train Meta Loss : 0.03667645517883757, Average Test Meta Loss : 0.033152617241850806\n",
      "Epochh : 713000, Average Train Meta Loss : 0.03665852097965784, Average Test Meta Loss : 0.03313531455660629\n",
      "Epochh : 714000, Average Train Meta Loss : 0.03664076971831391, Average Test Meta Loss : 0.033117567110471\n",
      "Epochh : 715000, Average Train Meta Loss : 0.03662126418088821, Average Test Meta Loss : 0.03310042161894778\n",
      "Epochh : 716000, Average Train Meta Loss : 0.03660533574295896, Average Test Meta Loss : 0.033083183944354604\n",
      "Epochh : 717000, Average Train Meta Loss : 0.03658531225724142, Average Test Meta Loss : 0.03306514572635567\n",
      "Epochh : 718000, Average Train Meta Loss : 0.03656468274560953, Average Test Meta Loss : 0.03304865007051281\n",
      "Epochh : 719000, Average Train Meta Loss : 0.036548945625917435, Average Test Meta Loss : 0.03303145840040538\n",
      "Epochh : 720000, Average Train Meta Loss : 0.036530746671312266, Average Test Meta Loss : 0.0330137679529857\n",
      "Epochh : 721000, Average Train Meta Loss : 0.03651576527515718, Average Test Meta Loss : 0.03299695090525881\n",
      "Epochh : 722000, Average Train Meta Loss : 0.03649581101119982, Average Test Meta Loss : 0.032978513139761974\n",
      "Epochh : 723000, Average Train Meta Loss : 0.03647833015028136, Average Test Meta Loss : 0.03296301280252275\n",
      "Epochh : 724000, Average Train Meta Loss : 0.03645917829122766, Average Test Meta Loss : 0.032948498636162386\n",
      "Epochh : 725000, Average Train Meta Loss : 0.03644052404850547, Average Test Meta Loss : 0.03293237264918422\n",
      "Epochh : 726000, Average Train Meta Loss : 0.03642201531638526, Average Test Meta Loss : 0.03291608440649896\n",
      "Epochh : 727000, Average Train Meta Loss : 0.0364012496073834, Average Test Meta Loss : 0.032898009110293794\n",
      "Epochh : 728000, Average Train Meta Loss : 0.036382089232377214, Average Test Meta Loss : 0.032883431823749976\n",
      "Epochh : 729000, Average Train Meta Loss : 0.03636200716469394, Average Test Meta Loss : 0.03286605159262554\n",
      "Epochh : 730000, Average Train Meta Loss : 0.03634410933435473, Average Test Meta Loss : 0.03284976780869407\n",
      "Epochh : 731000, Average Train Meta Loss : 0.036324244304918776, Average Test Meta Loss : 0.03283602362996936\n",
      "Epochh : 732000, Average Train Meta Loss : 0.036306961723957615, Average Test Meta Loss : 0.032822308770337996\n",
      "Epochh : 733000, Average Train Meta Loss : 0.03629040628324468, Average Test Meta Loss : 0.03280693484687913\n",
      "Epochh : 734000, Average Train Meta Loss : 0.03627224389713677, Average Test Meta Loss : 0.03279232936628855\n",
      "Epochh : 735000, Average Train Meta Loss : 0.03625548398609591, Average Test Meta Loss : 0.03277596839557307\n",
      "Epochh : 736000, Average Train Meta Loss : 0.03623652176054755, Average Test Meta Loss : 0.03276116470805211\n",
      "Epochh : 737000, Average Train Meta Loss : 0.03621926042854551, Average Test Meta Loss : 0.032744535733319785\n",
      "Epochh : 738000, Average Train Meta Loss : 0.03620066280159369, Average Test Meta Loss : 0.03272943776827251\n",
      "Epochh : 739000, Average Train Meta Loss : 0.03618665866268727, Average Test Meta Loss : 0.032714278688687023\n",
      "Epochh : 740000, Average Train Meta Loss : 0.03616693892386318, Average Test Meta Loss : 0.032698039550553414\n",
      "Epochh : 741000, Average Train Meta Loss : 0.036146160495745185, Average Test Meta Loss : 0.03268061394729498\n",
      "Epochh : 742000, Average Train Meta Loss : 0.036129161415359635, Average Test Meta Loss : 0.032665657531259\n",
      "Epochh : 743000, Average Train Meta Loss : 0.036110608688618456, Average Test Meta Loss : 0.032650352132753156\n",
      "Epochh : 744000, Average Train Meta Loss : 0.036094011045390145, Average Test Meta Loss : 0.03263567962155832\n",
      "Epochh : 745000, Average Train Meta Loss : 0.0360756958520415, Average Test Meta Loss : 0.032621006793813495\n",
      "Epochh : 746000, Average Train Meta Loss : 0.03605903864543857, Average Test Meta Loss : 0.032605948230665364\n",
      "Epochh : 747000, Average Train Meta Loss : 0.03604031091145723, Average Test Meta Loss : 0.032590271864621864\n",
      "Epochh : 748000, Average Train Meta Loss : 0.03602513985829272, Average Test Meta Loss : 0.032575358993444554\n",
      "Epochh : 749000, Average Train Meta Loss : 0.036008705756556465, Average Test Meta Loss : 0.032562543012811994\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # Sample a sine wave (Task from training data)\n",
    "    wave = dataset.get_meta_train_batch(task_batch_size=1)\n",
    "\n",
    "    # Update model predefined number of times based on k\n",
    "    new_model,_ = training(model, wave[0], lr_k, k)\n",
    "\n",
    "    # Evalaute the loss for the training data\n",
    "    train_set_evaluation(new_model,wave[0],store_train_loss_meta)     \n",
    "    \n",
    "    #Meta-update --> Get gradient for meta loop and update\n",
    "    metaupdate(model,new_model,metaoptimizer)\n",
    "    \n",
    "    # Evalaute the loss for the test data\n",
    "    # Note that we need to sample the wave from the test data\n",
    "    wave = dataset.get_meta_val_batch(task_batch_size=1)\n",
    "    test_set_validation(model,new_model,wave[0],lr_k,k,store_test_loss_meta)\n",
    "\n",
    "    # Print losses every 'printing_step' epochs\n",
    "    print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb0ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"baseline_reptile.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQjoz6FYctJM",
   "metadata": {
    "id": "bQjoz6FYctJM"
   },
   "source": [
    "<h1> Few Shot learning with new meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9271d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-94f2234b00e2>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x[:,None])\n",
      "<ipython-input-2-94f2234b00e2>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true = torch.tensor(y_true[:,None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Error: 2.995114146474749, Var: 4.887627204671044\n",
      "Step: 1, Error: 0.3946176098957658, Var: 0.1404156915653422\n",
      "Step: 5, Error: 0.021816651885485042, Var: 0.0004434216557384568\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wave = dataset.get_meta_val_batch(task_batch_size=1)#wave = random.sample(SINE_TEST, 1)\n",
    "#     test_set_validation(model,new_model,wave[0],lr_k,k,store_test_loss_meta)\n",
    "\n",
    "all_losses = []\n",
    "num_eval = 1000\n",
    "num_k_shots = 11\n",
    "\n",
    "test_waves = dataset.get_meta_test_batch(task_batch_size=num_eval)\n",
    "for test_eval in range(num_eval): \n",
    "    test_wave = test_waves[test_eval]\n",
    "    metaTrainLosses = test_set_validation(model,new_model,test_wave,lr_k,num_k_shots,store_test_loss_meta)\n",
    "    all_losses.append(np.array(metaTrainLosses))\n",
    "    \n",
    "\n",
    "all_losses = np.array(all_losses)\n",
    "np.save(f\"reptile_sine_{num_k_shots}.npy\", all_losses)\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "mean_loss = np.mean(all_losses, axis=0)\n",
    "\n",
    "# confidence interval plotting help from: https://stackoverflow.com/questions/59747313/how-to-plot-confidence-interval-in-python\n",
    "y = mean_loss\n",
    "x = list(range(num_k_shots))\n",
    "ci = 1.96 * np.std(all_losses, axis=0)**2/np.sqrt(len(y))\n",
    "\n",
    "ax_size=16\n",
    "title_size=18\n",
    "                                                  \n",
    "ax.plot(x, y, linewidth=3, label=f\"Mean Loss\")\n",
    "# to avoid having MSE < 0\n",
    "truncated_error = np.clip(y-ci, a_min=0, a_max=None)\n",
    "ax.fill_between(x, truncated_error, (y+ci), alpha=.5,label=f\"95% CI\")\n",
    "\n",
    "ax.set_xlabel(\"Gradient Steps\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"Mean Squared Error (MSE)\",fontsize=ax_size)\n",
    "ax.set_title(\"Sine Wave Regression: k-Shot Evaluation\",fontsize=title_size)\n",
    "ax.legend()#loc=\"upper right\")\n",
    "plt.savefig(\"reptile_sine_wave_reg_kshot.png\")\n",
    "\n",
    "analysis_steps=[0,1,5]\n",
    "for analysis_step in analysis_steps: \n",
    "    print(f\"Step: {analysis_step}, Error: {mean_loss[analysis_step]}, Var: {ci[analysis_step]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc2ea8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fee9af70e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 20\n",
    "avgTrain = []\n",
    "avgVal = []\n",
    "metaLosses = store_train_loss_meta\n",
    "metaValLosses = store_test_loss_meta\n",
    "for r in range(int(num/2),int(len(metaLosses)-num/2)):\n",
    "    currSum1 = 0\n",
    "    for t in range(int(-num/2),int(num/2)):\n",
    "        currSum1 += metaLosses[r+t]\n",
    "    currSum1 /= num\n",
    "    avgTrain.append(currSum1)\n",
    "    \n",
    "    currSum2 = 0\n",
    "    for s in range(int(-num/2),int(num/2)):\n",
    "        currSum2 += metaValLosses[r+s]\n",
    "    currSum2 /= num\n",
    "    avgVal.append(currSum2)\n",
    "    \n",
    "    \n",
    "plt.plot(avgVal) \n",
    "plt.plot(avgTrain) \n",
    "plt.legend(['Validation Loss','Train Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F4_Y4VF_dpNb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "F4_Y4VF_dpNb",
    "outputId": "4e200e1a-efb2-40a8-e404-dbdd3386a355"
   },
   "outputs": [],
   "source": [
    "plt.plot(store_train_loss_meta,label = 'Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('k shots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "-4nGnU0foEYo",
   "metadata": {
    "id": "-4nGnU0foEYo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-94f2234b00e2>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x[:,None])\n",
      "<ipython-input-2-94f2234b00e2>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true = torch.tensor(y_true[:,None])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Error: 3.0472987083988263, Var: 5.246738964570363\n",
      "Step: 1, Error: 0.39712528279959225, Var: 0.15220474552885474\n",
      "Step: 5, Error: 0.022106033206742724, Var: 0.00046797550965395053\n"
     ]
    }
   ],
   "source": [
    "def eval_adjust(model,wave,lr_inner,k, num_samples=10, with_noise=False, noise_dev=1, input_range=[-5.0, 5.0], extract_task_info=False):\n",
    "    \n",
    "    # get samples \n",
    "    x, label = get_samples_in_good_format(wave, num_samples=num_samples)\n",
    "    \n",
    "    # Create new model which we will train on\n",
    "    new_model = copy_existing_model(model)\n",
    "    \n",
    "    # Define new optimizer\n",
    "    koptimizer = torch.optim.SGD(new_model.parameters(), lr=lr_k)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for grad_step in range(k):\n",
    "        # Reset optimizer\n",
    "        koptimizer.zero_grad()\n",
    "        # Evaluate the model\n",
    "        loss = evaluation(new_model, wave, item = False)\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        koptimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "all_losses = []\n",
    "num_eval = 1000\n",
    "num_k_shots = 11\n",
    "\n",
    "test_waves = dataset.get_meta_test_batch(task_batch_size=num_eval)\n",
    "for test_eval in range(num_eval): \n",
    "    test_wave = test_waves[test_eval]\n",
    "    metaTrainLosses = eval_adjust(model,test_wave,lr_k,num_k_shots)\n",
    "    all_losses.append(np.array(metaTrainLosses))\n",
    "analysis_steps=[0,1,5]\n",
    "for analysis_step in analysis_steps: \n",
    "    print(f\"Step: {analysis_step}, Error: {mean_loss[analysis_step]}, Var: {ci[analysis_step]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0c696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Reptile.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
