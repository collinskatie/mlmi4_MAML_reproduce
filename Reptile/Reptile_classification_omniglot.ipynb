{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Imports"
      ],
      "metadata": {
        "id": "UrjQGgr5nUHC"
      },
      "id": "UrjQGgr5nUHC"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Required imports for neural network\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "import torchvision\n"
      ],
      "metadata": {
        "id": "eGl9mcc0nOMP"
      },
      "id": "eGl9mcc0nOMP",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Data Loading and Generation"
      ],
      "metadata": {
        "id": "T3KVOwFXFOY0"
      },
      "id": "T3KVOwFXFOY0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omniglot dataset\n",
        "\n",
        "Note this omniglot dataset has just 1000 images in the paper they report using 1200 for training"
      ],
      "metadata": {
        "id": "9nDo1jnW4wo4"
      },
      "id": "9nDo1jnW4wo4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import omniglot dataset\n",
        "dataset = torchvision.datasets.Omniglot(\n",
        "    root=\"./data\", download=True, transform=torchvision.transforms.ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqnEGwSRnOgX",
        "outputId": "91e4aa46-b7a6-4184-a22c-2446bda6d420"
      },
      "id": "AqnEGwSRnOgX",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary for all classes there is 50\n",
        "dataset_classes={}\n",
        "for i in range(len(dataset)):\n",
        "  image, label = dataset[i]\n",
        "  try:\n",
        "    dataset_classes[f'{label}']= torch.cat((dataset_classes[f'{label}'],image[None,:,:,:]))\n",
        "  except:\n",
        "    dataset_classes[f'{label}']=image[None,:,:,:]\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "HEkrsJBool3Q"
      },
      "id": "HEkrsJBool3Q",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Omni:\n",
        "  x = []\n",
        "  y = []"
      ],
      "metadata": {
        "id": "bfYGR92aDqui"
      },
      "id": "bfYGR92aDqui",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number_of_classes = 964 #Define how money classes to train on, max is 50\n",
        "\n",
        "# # Create empty lists\n",
        "# dataset_list_train = []\n",
        "# dataset_list_train_label = []\n",
        "# dataset_list_test = []\n",
        "# dataset_list_test_label = []\n",
        "\n",
        "# # Extract traning data\n",
        "# for label in range(number_of_classes):\n",
        "#   for j in range(len(dataset_classes[f'{label}'])-1):\n",
        "#     dataset_list_train.append(dataset_classes[f'{label}'][j])\n",
        "#     dataset_list_train_label.append(label)\n",
        "\n",
        "# # Shuffle training data and labels\n",
        "# zip_train_for_shuffling = list(zip(dataset_list_train,dataset_list_train_label))\n",
        "# random.shuffle(zip_train_for_shuffling)\n",
        "# dataset_list_train,dataset_list_train_label = zip(*zip_train_for_shuffling)\n",
        "\n",
        "# # Extr<ct test data\n",
        "# for label in range(number_of_classes):\n",
        "#   dataset_list_test.append(dataset_classes[f'{label}'][-1])\n",
        "#   dataset_list_test_label.append(label)\n",
        "\n",
        "# # Shuffle test data and labels\n",
        "# zip_test_for_shuffling = list(zip(dataset_list_test,dataset_list_test_label))\n",
        "# random.shuffle(zip_test_for_shuffling)\n",
        "# dataset_list_test,dataset_list_test_label = zip(*zip_test_for_shuffling)\n",
        "\n",
        "# OMNI_TRAIN=[]\n",
        "# for _ in range(len(dataset_list_train_label)):\n",
        "#   OMNI_TRAIN.append(Omni())\n",
        "\n",
        "# OMNI_TEST=[]\n",
        "# for _ in range(len(dataset_list_test_label)):\n",
        "#   OMNI_TEST.append(Omni())\n",
        "\n",
        "# for i in range(len(dataset_list_train_label)):\n",
        "#   OMNI_TRAIN[i].x = dataset_list_train[i][None,:,:,:]\n",
        "#   OMNI_TRAIN[i].y = dataset_list_train_label[i]\n",
        "\n",
        "# for i in range(len(dataset_list_test_label)):\n",
        "#   OMNI_TEST[i].x = dataset_list_test[i][None,:,:,:]\n",
        "#   OMNI_TEST[i].y = dataset_list_test_label[i]\n",
        "\n"
      ],
      "metadata": {
        "id": "vzxOBfUN3Q24"
      },
      "id": "vzxOBfUN3Q24",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Neural Network Model"
      ],
      "metadata": {
        "id": "cu4urLF7Q88A"
      },
      "id": "cu4urLF7Q88A"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network\n",
        "class Neural_Network(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, output_size=964):\n",
        "        super(Neural_Network, self).__init__()\n",
        "        # network layers\n",
        "        self.hidden1 = nn.Conv2d(input_size,hidden_size,kernel_size=3)\n",
        "        self.hidden2 = nn.Conv2d(hidden_size,hidden_size,kernel_size=3)\n",
        "        self.hidden3 = nn.Conv2d(hidden_size,hidden_size,kernel_size=3)\n",
        "        self.hidden4 = nn.Conv2d(hidden_size,hidden_size,kernel_size=3)\n",
        "        self.batchnorm = nn.BatchNorm2d(hidden_size)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(1024,output_size)\n",
        "\n",
        "        #Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        # Convolutional part\n",
        "        x = self.hidden1(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden3(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden4(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Fully-connected part\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "R1B0YTz6ytyN"
      },
      "id": "R1B0YTz6ytyN",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Helper functions"
      ],
      "metadata": {
        "id": "G-ExWACxQ3mt"
      },
      "id": "G-ExWACxQ3mt"
    },
    {
      "cell_type": "code",
      "source": [
        "# The Minimum Square Error is used to evaluate the difference between prediction and ground truth\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "def transform_label(label,K=5):\n",
        "    label_tensor=torch.zeros((1,number_of_tasks))\n",
        "    label_tensor[:,label]=1\n",
        "\n",
        "    return label_tensor.repeat(K, 1).float()\n",
        "\n",
        "def copy_existing_model(model):\n",
        "    # Function to copy an existing model\n",
        "    # We initialize a new model\n",
        "    new_model = Neural_Network(output_size=number_of_tasks)\n",
        "    # Copy the previous model's parameters into the new model\n",
        "    new_model.load_state_dict(model.state_dict())\n",
        "    return new_model\n",
        "\n",
        "def initialization_to_store_meta_losses():\n",
        "  # This function creates lists to store the meta losses\n",
        "  global store_train_loss_meta; store_train_loss_meta = []\n",
        "  global store_test_loss_meta; store_test_loss_meta = []\n",
        "\n",
        "def test_set_validation(model,new_model,omni,lr_inner,k,store_test_loss_meta,K=1):\n",
        "    # This functions does not actually affect the main algorithm, it is just used to evaluate the new model\n",
        "    new_model = training(model, omni, lr_inner, k,K)\n",
        "    # Obtain the loss\n",
        "    loss = evaluation(new_model, omni,K)\n",
        "    # Store loss\n",
        "    store_test_loss_meta.append(loss)\n",
        "\n",
        "def train_set_evaluation(new_model,omni,store_train_loss_meta,K):\n",
        "    loss = evaluation(new_model, omni,K)\n",
        "    store_train_loss_meta.append(loss) \n",
        "\n",
        "def print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step=1000):\n",
        "  if epoch % printing_step == 0:\n",
        "    print(f'Epochh : {epoch}, Average Train Meta Loss : {np.mean(store_train_loss_meta)}, Average Test Meta Loss : {np.mean(store_test_loss_meta)}')\n",
        "\n",
        "#This is based on the paper update rule, we calculate the difference between parameters and then this is used by the optimizer, rather than doing the update by hand\n",
        "def reptile_parameter_update(model,new_model):\n",
        "  # Zip models for the loop\n",
        "  zip_models = zip(model.parameters(), new_model.parameters())\n",
        "  for parameter, new_parameter in zip_models:\n",
        "    if parameter.grad is None:\n",
        "      parameter.grad = torch.tensor(torch.zeros_like(parameter))\n",
        "    # Here we are adding the gradient that will later be used by the optimizer\n",
        "    parameter.grad.data.add_(parameter.data - new_parameter.data)\n",
        "\n",
        "# Define commands in order needed for the metaupdate\n",
        "# Note that if we change the order it doesn't behave the same\n",
        "def metaoptimizer_update(metaoptimizer):\n",
        "  # Take step\n",
        "  metaoptimizer.step()\n",
        "  # Reset gradients\n",
        "  metaoptimizer.zero_grad()\n",
        "\n",
        "def metaupdate(model,new_model,metaoptimizer):\n",
        "  # Combine the two previous functions into a single metaupdate function\n",
        "  # First we calculate the gradients\n",
        "  reptile_parameter_update(model,new_model)\n",
        "  # Use those gradients in the optimizer\n",
        "  metaoptimizer_update(metaoptimizer)\n",
        "\n",
        "def evaluation(new_model, omni, K, item = True):\n",
        "    # Get data\n",
        "    x, label = omni.x,omni.y\n",
        "    # Make model prediction\n",
        "    prediction = new_model(x)\n",
        "    # Get loss\n",
        "    if item == True: #Depending on whether we need to return the loss value for storing or for backprop\n",
        "      loss = criterion(prediction,transform_label(label,K=K)).item()\n",
        "    else:\n",
        "      loss = criterion(prediction,transform_label(label,K=K))\n",
        "    return loss\n",
        "\n",
        "def training(model, omni, lr_k, k, K):\n",
        "    # Create new model which we will train on\n",
        "    new_model = copy_existing_model(model)\n",
        "    # Define new optimizer\n",
        "    koptimizer = torch.optim.SGD(new_model.parameters(), lr=lr_k)\n",
        "    # Update the model multiple times, note that k>1 (do not confuse k with K)\n",
        "    for i in range(k):\n",
        "        # Reset optimizer\n",
        "        koptimizer.zero_grad()\n",
        "        # Evaluate the model\n",
        "        loss = evaluation(new_model, omni, K, item = False)\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        koptimizer.step()\n",
        "    return new_model"
      ],
      "metadata": {
        "id": "1zyNHFXdOnug"
      },
      "id": "1zyNHFXdOnug",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Reptile"
      ],
      "metadata": {
        "id": "-4Ps8P2IRCmF"
      },
      "id": "-4Ps8P2IRCmF"
    },
    {
      "cell_type": "code",
      "source": [
        "#Define important variables\n",
        "epochs = int(1e5) # number of epochs \n",
        "lr_meta=0.001 # Learning rate for meta model (outer loop)\n",
        "printing_step=10 # how many epochs should we wait to print the loss\n",
        "lr_k=0.0005 # Internal learning rate\n",
        "k=3 # Number of internal updates for each task\n",
        "\n",
        "# Training loop\n",
        "K = 20 #Max is 20\n",
        "number_of_tasks = 20 #Max 964\n",
        "\n",
        "# Initializations\n",
        "initialization_to_store_meta_losses()\n",
        "model = Neural_Network(output_size=number_of_tasks)\n",
        "metaoptimizer = torch.optim.Adam(model.parameters(), lr=lr_meta,betas=(0, 0.999))"
      ],
      "metadata": {
        "id": "8ogpg_DHizlC"
      },
      "id": "8ogpg_DHizlC",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "        \n",
        "    # Sample a sine omni (Task from training data)\n",
        "    label = random.randint(0,number_of_tasks-1)\n",
        "    #need a random shuffle function\n",
        "    data = dataset_classes[f'{label}'][0:K,:,:,:]\n",
        "    omni = Omni()\n",
        "    omni.x = data\n",
        "    omni.y = label\n",
        "\n",
        "    # Update model predefined number of times based on k\n",
        "    new_model = training(model, omni, lr_k, k,K=K)\n",
        "\n",
        "    # Evalaute the loss for the training data\n",
        "    train_set_evaluation(new_model,omni,store_train_loss_meta,K=K)     \n",
        "    \n",
        "    #Meta-update --> Get gradient for meta loop and update\n",
        "    metaupdate(model,new_model,metaoptimizer)\n",
        "    \n",
        "    # Evalaute the loss for the test data\n",
        "    # Note that we need to sample the omni from the test data\n",
        "    # Sample a sine omni (Task from training data)\n",
        "    label = random.randint(0,number_of_tasks-1)\n",
        "    data = dataset_classes[f'{label}'][0,:,:,:][None,:,:,:]\n",
        "      \n",
        "    omni = Omni()\n",
        "    omni.x = data\n",
        "    omni.y = label\n",
        "\n",
        "\n",
        "    test_set_validation(model,new_model,omni,lr_k,k,store_test_loss_meta,K=1)\n",
        "\n",
        "    # Print losses every 'printing_step' epochs\n",
        "    print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-4-zQWWKFt3s",
        "outputId": "8a16877d-f6ed-4cb5-c1ed-1e7995ad5169"
      },
      "id": "-4-zQWWKFt3s",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochh : 0, Average Train Meta Loss : 0.16319586336612701, Average Test Meta Loss : 0.19966642558574677\n",
            "Epochh : 10, Average Train Meta Loss : 0.247772678732872, Average Test Meta Loss : 0.21417584812099283\n",
            "Epochh : 20, Average Train Meta Loss : 0.2260817326161833, Average Test Meta Loss : 0.22723884295139993\n",
            "Epochh : 30, Average Train Meta Loss : 0.22782214595786027, Average Test Meta Loss : 0.219878239737403\n",
            "Epochh : 40, Average Train Meta Loss : 0.22405931396727882, Average Test Meta Loss : 0.20866878367051844\n",
            "Epochh : 50, Average Train Meta Loss : 0.22236560568140418, Average Test Meta Loss : 0.20393037489231894\n",
            "Epochh : 60, Average Train Meta Loss : 0.21729641131384939, Average Test Meta Loss : 0.20549485001896248\n",
            "Epochh : 70, Average Train Meta Loss : 0.2122869021849523, Average Test Meta Loss : 0.202840551576564\n",
            "Epochh : 80, Average Train Meta Loss : 0.20423819985884575, Average Test Meta Loss : 0.1979321349458194\n",
            "Epochh : 90, Average Train Meta Loss : 0.20108444707283935, Average Test Meta Loss : 0.19397319001810892\n",
            "Epochh : 100, Average Train Meta Loss : 0.19994947680336708, Average Test Meta Loss : 0.18571249336594403\n",
            "Epochh : 110, Average Train Meta Loss : 0.19490052173165856, Average Test Meta Loss : 0.1824805363005883\n",
            "Epochh : 120, Average Train Meta Loss : 0.19171331432640307, Average Test Meta Loss : 0.17810020192472403\n",
            "Epochh : 130, Average Train Meta Loss : 0.18790518131306153, Average Test Meta Loss : 0.17816735468520464\n",
            "Epochh : 140, Average Train Meta Loss : 0.18474504399172803, Average Test Meta Loss : 0.17705817636868632\n",
            "Epochh : 150, Average Train Meta Loss : 0.18131988394438037, Average Test Meta Loss : 0.1702008998601247\n",
            "Epochh : 160, Average Train Meta Loss : 0.17883896885543876, Average Test Meta Loss : 0.16698812853012765\n",
            "Epochh : 170, Average Train Meta Loss : 0.17502605700484145, Average Test Meta Loss : 0.16346084404932826\n",
            "Epochh : 180, Average Train Meta Loss : 0.1717771471514063, Average Test Meta Loss : 0.16105013270733765\n",
            "Epochh : 190, Average Train Meta Loss : 0.16747864520089475, Average Test Meta Loss : 0.15755026317469736\n",
            "Epochh : 200, Average Train Meta Loss : 0.16456540003048248, Average Test Meta Loss : 0.15514864652088625\n",
            "Epochh : 210, Average Train Meta Loss : 0.16259824370709358, Average Test Meta Loss : 0.15193857637485622\n",
            "Epochh : 220, Average Train Meta Loss : 0.15780615887280167, Average Test Meta Loss : 0.1475853238621415\n",
            "Epochh : 230, Average Train Meta Loss : 0.15509830882724424, Average Test Meta Loss : 0.14478963006940948\n",
            "Epochh : 240, Average Train Meta Loss : 0.15194310644000028, Average Test Meta Loss : 0.14169841052150872\n",
            "Epochh : 250, Average Train Meta Loss : 0.14922186173869675, Average Test Meta Loss : 0.13899498050652104\n",
            "Epochh : 260, Average Train Meta Loss : 0.14627701372543342, Average Test Meta Loss : 0.1369142956631187\n",
            "Epochh : 270, Average Train Meta Loss : 0.14392709667619139, Average Test Meta Loss : 0.13443010040761635\n",
            "Epochh : 280, Average Train Meta Loss : 0.14070716054341176, Average Test Meta Loss : 0.1310481596622099\n",
            "Epochh : 290, Average Train Meta Loss : 0.1373243750844317, Average Test Meta Loss : 0.12864266714829067\n",
            "Epochh : 300, Average Train Meta Loss : 0.1338171400941995, Average Test Meta Loss : 0.12632046610046763\n",
            "Epochh : 310, Average Train Meta Loss : 0.13119321469879347, Average Test Meta Loss : 0.12389091287060736\n",
            "Epochh : 320, Average Train Meta Loss : 0.12803413043149697, Average Test Meta Loss : 0.1217891442511217\n",
            "Epochh : 330, Average Train Meta Loss : 0.12585432727622808, Average Test Meta Loss : 0.12050306569424203\n",
            "Epochh : 340, Average Train Meta Loss : 0.12332178253640866, Average Test Meta Loss : 0.11813261428366714\n",
            "Epochh : 350, Average Train Meta Loss : 0.12059316300737945, Average Test Meta Loss : 0.11608467637484149\n",
            "Epochh : 360, Average Train Meta Loss : 0.11859607635552213, Average Test Meta Loss : 0.11347286941555691\n",
            "Epochh : 370, Average Train Meta Loss : 0.11600066398708477, Average Test Meta Loss : 0.11072693898331097\n",
            "Epochh : 380, Average Train Meta Loss : 0.11349634530561734, Average Test Meta Loss : 0.10888649007375627\n",
            "Epochh : 390, Average Train Meta Loss : 0.11127590817426715, Average Test Meta Loss : 0.10672482407646333\n",
            "Epochh : 400, Average Train Meta Loss : 0.109101997985983, Average Test Meta Loss : 0.10429590661902453\n",
            "Epochh : 410, Average Train Meta Loss : 0.10718128232000998, Average Test Meta Loss : 0.10229129060958268\n",
            "Epochh : 420, Average Train Meta Loss : 0.10505055516516909, Average Test Meta Loss : 0.10017472244737612\n",
            "Epochh : 430, Average Train Meta Loss : 0.1029608638068767, Average Test Meta Loss : 0.09817083242152555\n",
            "Epochh : 440, Average Train Meta Loss : 0.10092513218071722, Average Test Meta Loss : 0.0961959701883245\n",
            "Epochh : 450, Average Train Meta Loss : 0.0988596518034324, Average Test Meta Loss : 0.09433179345483837\n",
            "Epochh : 460, Average Train Meta Loss : 0.0970555296559907, Average Test Meta Loss : 0.09301747531676059\n",
            "Epochh : 470, Average Train Meta Loss : 0.09524754205540183, Average Test Meta Loss : 0.09204228579723167\n",
            "Epochh : 480, Average Train Meta Loss : 0.09355029070073653, Average Test Meta Loss : 0.09047093383012836\n",
            "Epochh : 490, Average Train Meta Loss : 0.09172266694447291, Average Test Meta Loss : 0.08874589915490708\n",
            "Epochh : 500, Average Train Meta Loss : 0.09006597298746297, Average Test Meta Loss : 0.08713583243911353\n",
            "Epochh : 510, Average Train Meta Loss : 0.08845846866083891, Average Test Meta Loss : 0.08559527435137684\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-0f3425f94624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Evalaute the loss for the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_set_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0momni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstore_train_loss_meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#Meta-update --> Get gradient for meta loop and update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-ecdd748a8ec6>\u001b[0m in \u001b[0;36mtrain_set_evaluation\u001b[0;34m(new_model, omni, store_train_loss_meta, K)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_set_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0momni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstore_train_loss_meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mstore_train_loss_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-ecdd748a8ec6>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(new_model, omni, K, item)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momni\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0momni\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Make model prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Depending on whether we need to return the loss value for storing or for backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-bc0a7a739b9f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Few Shot learning with new meta-model"
      ],
      "metadata": {
        "id": "bQjoz6FYctJM"
      },
      "id": "bQjoz6FYctJM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs good few shot learning"
      ],
      "metadata": {
        "id": "m-SPUG5Bfpe9"
      },
      "id": "m-SPUG5Bfpe9"
    },
    {
      "cell_type": "code",
      "source": [
        "label = random.randint(0,number_of_tasks-1)\n",
        "chosen = dataset_classes[f'{label}']\n",
        "data = random.choice(chosen)[None,:,:,:]\n",
        "omni = Omni()\n",
        "omni.x = data\n",
        "omni.y = label\n",
        "k_shot_updates = 5\n",
        "initialization_to_store_meta_losses()\n",
        "for shots in range(k_shot_updates):\n",
        "    new_model = training(model, omni, lr_k, shots,K=1)\n",
        "    train_set_evaluation(new_model,omni,store_train_loss_meta,K=1) \n",
        "\n",
        "plt.plot(store_train_loss_meta,label = 'Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('k shots')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "GY84TNs8JXVH",
        "outputId": "a39cb539-4148-4464-bae5-f1ccdc75457e"
      },
      "id": "GY84TNs8JXVH",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'k shots')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B3HnhyvJRa_H"
      },
      "id": "B3HnhyvJRa_H",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Reptile_classification_omniglot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}