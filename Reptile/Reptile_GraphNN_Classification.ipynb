{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Imports and Installs"
      ],
      "metadata": {
        "id": "UrjQGgr5nUHC"
      },
      "id": "UrjQGgr5nUHC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "S2WfmJiVTDpE"
      },
      "id": "S2WfmJiVTDpE",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Required imports for neural network\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "\n",
        "# For GNNs\n",
        "from torch.nn import Linear\n",
        "from torch.nn import BatchNorm1d\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import GraphConv\n",
        "from torch_geometric.nn import GraphNorm\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.nn import global_max_pool\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "eGl9mcc0nOMP"
      },
      "id": "eGl9mcc0nOMP",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Data Loading and Generation\n",
        "\n",
        "Reptile for regression task using GNNs\n",
        "\n",
        "Some common GNN datasets are here:\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.GNNBenchmarkDataset\n",
        "\n",
        "For classification we use the TUDataset --> ENZYMES"
      ],
      "metadata": {
        "id": "T3KVOwFXFOY0"
      },
      "id": "T3KVOwFXFOY0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENZYMES has 6 classes\n",
        "\n",
        "According to Reptile paper notation\n",
        "\n",
        "C = Total number of classes in dataset (ENZYMES C=6)\n",
        "N = Number of selected classes (We will use C=N=6)\n",
        "K = Number of examples given to the model for each class (In the paper they use K+1 instead of K for some reason)\n",
        "\n",
        "5-shot learning (5 updates of testing class)\n",
        "\n",
        "In general we train on N classes with K examples from each, then train on a single unseen example from a randomly selected class.\n"
      ],
      "metadata": {
        "id": "jvvZSOiQe-Ne"
      },
      "id": "jvvZSOiQe-Ne"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset',name='ENZYMES')"
      ],
      "metadata": {
        "id": "PIExsutGTQcB"
      },
      "id": "PIExsutGTQcB",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is based on https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html\n",
        "\n",
        "#Function to display properties of the dataset (it is not necessary for anything)\n",
        "def display_graph_dataset_properties(dataset):\n",
        "  print()\n",
        "  print(f'Dataset: {dataset}:')\n",
        "  print('====================')\n",
        "  print(f'Number of graphs: {len(dataset)}')\n",
        "  print(f'Number of features: {dataset.num_features}')\n",
        "  print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "  data = dataset[0]  # Get the first graph object.Ã‡\n",
        "\n",
        "  print()\n",
        "  print('Look at a sample graph of the dataset')\n",
        "  print(data)\n",
        "  print('=============================================================')\n",
        "\n",
        "  # Gather some statistics about the first graph.\n",
        "  print(f'Number of nodes: {data.num_nodes}')\n",
        "  print(f'Number of edges: {data.num_edges}')\n",
        "  print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "  print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "  print(f'Has self-loops: {data.has_self_loops()}')\n",
        "  print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "id": "LxK2rDRNTSxd"
      },
      "id": "LxK2rDRNTSxd",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_graph_dataset_properties(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC6T1VZPF9Ba",
        "outputId": "5240f583-e193-4858-c1cf-177c8bb781c9"
      },
      "id": "GC6T1VZPF9Ba",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: ENZYMES(600):\n",
            "====================\n",
            "Number of graphs: 600\n",
            "Number of features: 3\n",
            "Number of classes: 6\n",
            "\n",
            "Look at a sample graph of the dataset\n",
            "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
            "=============================================================\n",
            "Number of nodes: 37\n",
            "Number of edges: 168\n",
            "Average node degree: 4.54\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the dataset into a list\n",
        "dataset_list_0 = []\n",
        "dataset_list_1 = []\n",
        "dataset_list_2 = []\n",
        "dataset_list_3 = []\n",
        "dataset_list_4 = []\n",
        "dataset_list_5 = []\n",
        "\n",
        "#Loop over the dataset and separate the different classes into different lists\n",
        "for i in range(len(dataset)):\n",
        "  if int(dataset[i].y[0].item())==0:\n",
        "    dataset_list_0.append(dataset[i])\n",
        "  elif int(dataset[i].y[0].item())==1:\n",
        "    dataset_list_1.append(dataset[i])\n",
        "  elif int(dataset[i].y[0].item())==2:\n",
        "    dataset_list_2.append(dataset[i])\n",
        "  elif int(dataset[i].y[0].item())==3:\n",
        "    dataset_list_3.append(dataset[i])\n",
        "  elif int(dataset[i].y[0].item())==4:\n",
        "    dataset_list_4.append(dataset[i])\n",
        "  elif int(dataset[i].y[0].item())==5:\n",
        "    dataset_list_5.append(dataset[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "lD_MRHC8T8Za"
      },
      "id": "lD_MRHC8T8Za",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffle the dataset list\n",
        "random.shuffle(dataset_list_0)\n",
        "random.shuffle(dataset_list_1)\n",
        "random.shuffle(dataset_list_2)\n",
        "random.shuffle(dataset_list_3)\n",
        "random.shuffle(dataset_list_4)\n",
        "random.shuffle(dataset_list_5)"
      ],
      "metadata": {
        "id": "wwVU9xu9NCdg"
      },
      "id": "wwVU9xu9NCdg",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split into train and test\n",
        "\n",
        "training_proportion = 0.99\n",
        "GRAPH_TRAIN = dataset_list_0[:int(np.floor(len(dataset_list_0)*training_proportion))]+dataset_list_1[:int(np.floor(len(dataset_list_1)*training_proportion))]\\\n",
        "  +dataset_list_2[:int(np.floor(len(dataset_list_2)*training_proportion))]+dataset_list_3[:int(np.floor(len(dataset_list_3)*training_proportion))]\\\n",
        "  +dataset_list_4[:int(np.floor(len(dataset_list_4)*training_proportion))]+dataset_list_5[:int(np.floor(len(dataset_list_5)*training_proportion))]\n",
        "\n",
        "GRAPH_TEST = [dataset_list_0[-1]]+[dataset_list_1[-1]]+[dataset_list_2[-1]]+[dataset_list_3[-1]]+[dataset_list_4[-1]]+[dataset_list_5[-1]]\n",
        "\n",
        "random.shuffle(GRAPH_TRAIN)\n",
        "random.shuffle(GRAPH_TEST)"
      ],
      "metadata": {
        "id": "a3X51uGHDvSV"
      },
      "id": "a3X51uGHDvSV",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Neural Network Model"
      ],
      "metadata": {
        "id": "cu4urLF7Q88A"
      },
      "id": "cu4urLF7Q88A"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=100, output_dim=6):\n",
        "        super(GNN, self).__init__()\n",
        "\n",
        "        #Hidden Layers\n",
        "        self.hidden1 = GraphConv(input_dim, hidden_dim)\n",
        "        self.hidden2 = GraphConv(hidden_dim, hidden_dim)\n",
        "        self.hidden3 = GraphConv(hidden_dim, output_dim)\n",
        "        self.norm = GraphNorm(hidden_dim)\n",
        "\n",
        "        #Activation Function\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, input_x, edge_index, batch):\n",
        "      \n",
        "        #Standard forward\n",
        "        x = self.hidden1(input_x,edge_index)\n",
        "        x = self.norm(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden2(x,edge_index)\n",
        "        x = self.norm(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden3(x,edge_index)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        #Global mean pool across batches\n",
        "        x = global_max_pool(x, batch)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        \n",
        "        return x\n"
      ],
      "metadata": {
        "id": "R1B0YTz6ytyN"
      },
      "id": "R1B0YTz6ytyN",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Helper functions"
      ],
      "metadata": {
        "id": "G-ExWACxQ3mt"
      },
      "id": "G-ExWACxQ3mt"
    },
    {
      "cell_type": "code",
      "source": [
        "# The Minimum Square Error is used to evaluate the difference between prediction and ground truth\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "def transform_label(label):\n",
        "    label=label.item()\n",
        "    if label==0:\n",
        "      label = torch.Tensor([1,0,0,0,0,0])[None,:].float()\n",
        "    elif label==1:\n",
        "      label = torch.Tensor([0,1,0,0,0,0])[None,:].float()\n",
        "    elif label==2:\n",
        "      label = torch.Tensor([0,0,1,0,0,0])[None,:].float()\n",
        "    elif label==3:\n",
        "      label = torch.Tensor([0,0,0,1,0,0])[None,:].float()\n",
        "    elif label==4:\n",
        "      label = torch.Tensor([0,0,0,0,1,0])[None,:].float()\n",
        "    elif label==5:\n",
        "      label = torch.Tensor([0,0,0,0,0,1])[None,:].float()\n",
        "\n",
        "    return label\n",
        "\n",
        "def copy_existing_model(model):\n",
        "    # Function to copy an existing model\n",
        "    # We initialize a new model\n",
        "    new_model = GNN()\n",
        "    # Copy the previous model's parameters into the new model\n",
        "    new_model.load_state_dict(model.state_dict())\n",
        "    return new_model\n",
        "\n",
        "def initialization_to_store_meta_losses():\n",
        "  # This function creates lists to store the meta losses\n",
        "  global store_train_loss_meta; store_train_loss_meta = []\n",
        "  global store_test_loss_meta; store_test_loss_meta = []\n",
        "\n",
        "def test_set_validation(model,new_model,graph,lr_inner,k,store_test_loss_meta):\n",
        "    # This functions does not actually affect the main algorithm, it is just used to evaluate the new model\n",
        "    new_model = training(model, graph, lr_inner, k)\n",
        "    # Obtain the loss\n",
        "    loss = evaluation(new_model, graph)\n",
        "    # Store loss\n",
        "    store_test_loss_meta.append(loss)\n",
        "\n",
        "def train_set_evaluation(new_model,graph,store_train_loss_meta):\n",
        "    loss = evaluation(new_model, graph)\n",
        "    store_train_loss_meta.append(loss) \n",
        "\n",
        "def print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step=1000):\n",
        "  if epoch % printing_step == 0:\n",
        "    print(f'Epochh : {epoch}, Average Train Meta Loss : {np.mean(store_train_loss_meta)}, Average Test Meta Loss : {np.mean(store_test_loss_meta)}')\n",
        "\n",
        "#This is based on the paper update rule, we calculate the difference between parameters and then this is used by the optimizer, rather than doing the update by hand\n",
        "def reptile_parameter_update(model,new_model):\n",
        "  # Zip models for the loop\n",
        "  zip_models = zip(model.parameters(), new_model.parameters())\n",
        "  for parameter, new_parameter in zip_models:\n",
        "    if parameter.grad is None:\n",
        "      parameter.grad = torch.tensor(torch.zeros_like(parameter))\n",
        "    # Here we are adding the gradient that will later be used by the optimizer\n",
        "    parameter.grad.data.add_(parameter.data - new_parameter.data)\n",
        "\n",
        "# Define commands in order needed for the metaupdate\n",
        "# Note that if we change the order it doesn't behave the same\n",
        "def metaoptimizer_update(metaoptimizer):\n",
        "  # Take step\n",
        "  metaoptimizer.step()\n",
        "  # Reset gradients\n",
        "  metaoptimizer.zero_grad()\n",
        "\n",
        "def metaupdate(model,new_model,metaoptimizer):\n",
        "  # Combine the two previous functions into a single metaupdate function\n",
        "  # First we calculate the gradients\n",
        "  reptile_parameter_update(model,new_model)\n",
        "  # Use those gradients in the optimizer\n",
        "  metaoptimizer_update(metaoptimizer)\n",
        "\n",
        "def evaluation(new_model, graph, item = True):\n",
        "    # Make model prediction\n",
        "    prediction = new_model(graph.x,graph.edge_index,graph.batch)\n",
        "    # Get loss\n",
        "    if item == True: #Depending on whether we need to return the loss value for storing or for backprop\n",
        "      loss = criterion(prediction,transform_label(graph.y)).item()\n",
        "    else:\n",
        "      loss = criterion(prediction,transform_label(graph.y))\n",
        "    return loss\n",
        "\n",
        "def training(model, graph, lr_k, k):\n",
        "    # Create new model which we will train on\n",
        "    new_model = copy_existing_model(model)\n",
        "    # Define new optimizer\n",
        "    koptimizer = torch.optim.SGD(new_model.parameters(), lr=lr_k)\n",
        "    # Update the model multiple times, note that k>1 (do not confuse k with K)\n",
        "    for i in range(k):\n",
        "        # Reset optimizer\n",
        "        koptimizer.zero_grad()\n",
        "        # Evaluate the model\n",
        "        loss = evaluation(new_model, graph, item = False)\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        koptimizer.step()\n",
        "    return new_model"
      ],
      "metadata": {
        "id": "1zyNHFXdOnug"
      },
      "id": "1zyNHFXdOnug",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Reptile"
      ],
      "metadata": {
        "id": "-4Ps8P2IRCmF"
      },
      "id": "-4Ps8P2IRCmF"
    },
    {
      "cell_type": "code",
      "source": [
        "#Define important variables\n",
        "epochs = int(1e5) # number of epochs \n",
        "lr_meta=0.001 # Learning rate for meta model (outer loop)\n",
        "printing_step=100 # how many epochs should we wait to print the loss\n",
        "lr_k=0.0005 # Internal learning rate\n",
        "k=5 # Number of internal updates for each task\n",
        "\n",
        "# Initializations\n",
        "initialization_to_store_meta_losses()\n",
        "model = GNN()\n",
        "metaoptimizer = torch.optim.Adam(model.parameters(), lr=lr_meta)"
      ],
      "metadata": {
        "id": "8ogpg_DHizlC"
      },
      "id": "8ogpg_DHizlC",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "        \n",
        "    # Sample a sine graph (Task from training data)\n",
        "    graph = random.sample(GRAPH_TRAIN, 1)\n",
        "\n",
        "    # Update model predefined number of times based on k\n",
        "    new_model = training(model, graph[0], lr_k, k)\n",
        "\n",
        "    # Evalaute the loss for the training data\n",
        "    train_set_evaluation(new_model,graph[0],store_train_loss_meta)     \n",
        "    \n",
        "    #Meta-update --> Get gradient for meta loop and update\n",
        "    metaupdate(model,new_model,metaoptimizer)\n",
        "    \n",
        "    # Evalaute the loss for the test data\n",
        "    # Note that we need to sample the graph from the test data\n",
        "    graph = random.sample(GRAPH_TEST, 1)\n",
        "    test_set_validation(model,new_model,graph[0],lr_k,k,store_test_loss_meta)\n",
        "\n",
        "    # Print losses every 'printing_step' epochs\n",
        "    print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-4-zQWWKFt3s",
        "outputId": "004e43ec-cdd5-4b66-c5f8-5e8061dacec9"
      },
      "id": "-4-zQWWKFt3s",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochh : 0, Average Train Meta Loss : 0.5752590894699097, Average Test Meta Loss : 0.044029876589775085\n",
            "Epochh : 100, Average Train Meta Loss : 0.4020413566048783, Average Test Meta Loss : 0.3779718298280593\n",
            "Epochh : 200, Average Train Meta Loss : 0.39383334229093286, Average Test Meta Loss : 0.41661883582968023\n",
            "Epochh : 300, Average Train Meta Loss : 0.3944755988659653, Average Test Meta Loss : 0.40520283488241143\n",
            "Epochh : 400, Average Train Meta Loss : 0.39060635316624603, Average Test Meta Loss : 0.41361292336424094\n",
            "Epochh : 500, Average Train Meta Loss : 0.3838583746043865, Average Test Meta Loss : 0.417793189634582\n",
            "Epochh : 600, Average Train Meta Loss : 0.3821976190609563, Average Test Meta Loss : 0.41376096726753153\n",
            "Epochh : 700, Average Train Meta Loss : 0.38347762397433316, Average Test Meta Loss : 0.4060127007510625\n",
            "Epochh : 800, Average Train Meta Loss : 0.3800885248999024, Average Test Meta Loss : 0.40409461952997056\n",
            "Epochh : 900, Average Train Meta Loss : 0.37837655466467507, Average Test Meta Loss : 0.40243529891812313\n",
            "Epochh : 1000, Average Train Meta Loss : 0.37558691797928734, Average Test Meta Loss : 0.39844750527378087\n",
            "Epochh : 1100, Average Train Meta Loss : 0.3737247218931945, Average Test Meta Loss : 0.3980591724559895\n",
            "Epochh : 1200, Average Train Meta Loss : 0.3711688593867468, Average Test Meta Loss : 0.397513889961199\n",
            "Epochh : 1300, Average Train Meta Loss : 0.3674281211133236, Average Test Meta Loss : 0.4005358610001552\n",
            "Epochh : 1400, Average Train Meta Loss : 0.3647597309283056, Average Test Meta Loss : 0.399804650891644\n",
            "Epochh : 1500, Average Train Meta Loss : 0.3620656329305777, Average Test Meta Loss : 0.40062783089907544\n",
            "Epochh : 1600, Average Train Meta Loss : 0.35988694321606157, Average Test Meta Loss : 0.39720890802481934\n",
            "Epochh : 1700, Average Train Meta Loss : 0.35938731323366985, Average Test Meta Loss : 0.39524651126939364\n",
            "Epochh : 1800, Average Train Meta Loss : 0.3575794940559956, Average Test Meta Loss : 0.3923342676542654\n",
            "Epochh : 1900, Average Train Meta Loss : 0.35770966624130723, Average Test Meta Loss : 0.3947494531903751\n",
            "Epochh : 2000, Average Train Meta Loss : 0.3548211002981161, Average Test Meta Loss : 0.3942268835357402\n",
            "Epochh : 2100, Average Train Meta Loss : 0.3537365374093727, Average Test Meta Loss : 0.39157143896928803\n",
            "Epochh : 2200, Average Train Meta Loss : 0.35220247956127376, Average Test Meta Loss : 0.3909282246092567\n",
            "Epochh : 2300, Average Train Meta Loss : 0.351157424943129, Average Test Meta Loss : 0.38793779659315275\n",
            "Epochh : 2400, Average Train Meta Loss : 0.35006523800405615, Average Test Meta Loss : 0.387031362964921\n",
            "Epochh : 2500, Average Train Meta Loss : 0.34884334793008986, Average Test Meta Loss : 0.38607693794014\n",
            "Epochh : 2600, Average Train Meta Loss : 0.34722944387220067, Average Test Meta Loss : 0.3849649251929582\n",
            "Epochh : 2700, Average Train Meta Loss : 0.3446915245951822, Average Test Meta Loss : 0.3830108865309384\n",
            "Epochh : 2800, Average Train Meta Loss : 0.3455352559020714, Average Test Meta Loss : 0.3818862841673054\n",
            "Epochh : 2900, Average Train Meta Loss : 0.34425997322406626, Average Test Meta Loss : 0.38055935401568347\n",
            "Epochh : 3000, Average Train Meta Loss : 0.34241670059834345, Average Test Meta Loss : 0.3811126380584084\n",
            "Epochh : 3100, Average Train Meta Loss : 0.3408699711669544, Average Test Meta Loss : 0.38186693054818216\n",
            "Epochh : 3200, Average Train Meta Loss : 0.3388130553226024, Average Test Meta Loss : 0.3794822071570413\n",
            "Epochh : 3300, Average Train Meta Loss : 0.3365768628577642, Average Test Meta Loss : 0.3780318723363202\n",
            "Epochh : 3400, Average Train Meta Loss : 0.33576905757652403, Average Test Meta Loss : 0.37644194935014985\n",
            "Epochh : 3500, Average Train Meta Loss : 0.3339030352448106, Average Test Meta Loss : 0.37579125864697277\n",
            "Epochh : 3600, Average Train Meta Loss : 0.3318232508703794, Average Test Meta Loss : 0.37509781201335146\n",
            "Epochh : 3700, Average Train Meta Loss : 0.33163115054369474, Average Test Meta Loss : 0.37507161735807404\n",
            "Epochh : 3800, Average Train Meta Loss : 0.32968077336170193, Average Test Meta Loss : 0.374443288531715\n",
            "Epochh : 3900, Average Train Meta Loss : 0.3281541099416046, Average Test Meta Loss : 0.37360846016819116\n",
            "Epochh : 4000, Average Train Meta Loss : 0.32722028924032204, Average Test Meta Loss : 0.3740830540714538\n",
            "Epochh : 4100, Average Train Meta Loss : 0.3245325687000492, Average Test Meta Loss : 0.3725611157646056\n",
            "Epochh : 4200, Average Train Meta Loss : 0.323607555740613, Average Test Meta Loss : 0.370856449101874\n",
            "Epochh : 4300, Average Train Meta Loss : 0.32245515899380456, Average Test Meta Loss : 0.3715305865079232\n",
            "Epochh : 4400, Average Train Meta Loss : 0.3205300488537832, Average Test Meta Loss : 0.37054732286205594\n",
            "Epochh : 4500, Average Train Meta Loss : 0.3186507133851042, Average Test Meta Loss : 0.3710904315330661\n",
            "Epochh : 4600, Average Train Meta Loss : 0.3177131808265766, Average Test Meta Loss : 0.3708932303614383\n",
            "Epochh : 4700, Average Train Meta Loss : 0.3165035613824418, Average Test Meta Loss : 0.3709564442169391\n",
            "Epochh : 4800, Average Train Meta Loss : 0.3151954821764693, Average Test Meta Loss : 0.37102470499238066\n",
            "Epochh : 4900, Average Train Meta Loss : 0.31385441043085094, Average Test Meta Loss : 0.3715411197840269\n",
            "Epochh : 5000, Average Train Meta Loss : 0.3122189695330297, Average Test Meta Loss : 0.370786000329548\n",
            "Epochh : 5100, Average Train Meta Loss : 0.31108811595554986, Average Test Meta Loss : 0.37017808456628265\n",
            "Epochh : 5200, Average Train Meta Loss : 0.31040051921721007, Average Test Meta Loss : 0.3704372252385378\n",
            "Epochh : 5300, Average Train Meta Loss : 0.30919217960760437, Average Test Meta Loss : 0.37051795624238737\n",
            "Epochh : 5400, Average Train Meta Loss : 0.3079646392976635, Average Test Meta Loss : 0.3699263216805499\n",
            "Epochh : 5500, Average Train Meta Loss : 0.306983692916567, Average Test Meta Loss : 0.3692255448798117\n",
            "Epochh : 5600, Average Train Meta Loss : 0.305713105226589, Average Test Meta Loss : 0.369242920558491\n",
            "Epochh : 5700, Average Train Meta Loss : 0.3042209443021683, Average Test Meta Loss : 0.368825659649942\n",
            "Epochh : 5800, Average Train Meta Loss : 0.3026493534664033, Average Test Meta Loss : 0.36829799878428215\n",
            "Epochh : 5900, Average Train Meta Loss : 0.3010143470553955, Average Test Meta Loss : 0.3679758587584233\n",
            "Epochh : 6000, Average Train Meta Loss : 0.2994890720733576, Average Test Meta Loss : 0.3698565720422215\n",
            "Epochh : 6100, Average Train Meta Loss : 0.29796965474897735, Average Test Meta Loss : 0.37042432589287866\n",
            "Epochh : 6200, Average Train Meta Loss : 0.2962011684685404, Average Test Meta Loss : 0.37036149858823997\n",
            "Epochh : 6300, Average Train Meta Loss : 0.29523443527699944, Average Test Meta Loss : 0.3698465242997721\n",
            "Epochh : 6400, Average Train Meta Loss : 0.2941771478407729, Average Test Meta Loss : 0.3697362784956532\n",
            "Epochh : 6500, Average Train Meta Loss : 0.2921872154330317, Average Test Meta Loss : 0.37004823998370834\n",
            "Epochh : 6600, Average Train Meta Loss : 0.2908704119950622, Average Test Meta Loss : 0.36914379648611184\n",
            "Epochh : 6700, Average Train Meta Loss : 0.2898536321061087, Average Test Meta Loss : 0.3685229945651661\n",
            "Epochh : 6800, Average Train Meta Loss : 0.28856236473769226, Average Test Meta Loss : 0.36816206355161835\n",
            "Epochh : 6900, Average Train Meta Loss : 0.2874008469390166, Average Test Meta Loss : 0.3672129647830001\n",
            "Epochh : 7000, Average Train Meta Loss : 0.28693076614694585, Average Test Meta Loss : 0.3673522531371348\n",
            "Epochh : 7100, Average Train Meta Loss : 0.28606922878907626, Average Test Meta Loss : 0.3679359473394257\n",
            "Epochh : 7200, Average Train Meta Loss : 0.2849523840440064, Average Test Meta Loss : 0.3672748590738798\n",
            "Epochh : 7300, Average Train Meta Loss : 0.283742332286286, Average Test Meta Loss : 0.36609653548974075\n",
            "Epochh : 7400, Average Train Meta Loss : 0.2828142783254707, Average Test Meta Loss : 0.3660543110616215\n",
            "Epochh : 7500, Average Train Meta Loss : 0.28146128742596704, Average Test Meta Loss : 0.36569630390313007\n",
            "Epochh : 7600, Average Train Meta Loss : 0.2802630324080114, Average Test Meta Loss : 0.3656379132646785\n",
            "Epochh : 7700, Average Train Meta Loss : 0.2790030899468909, Average Test Meta Loss : 0.36476079219487617\n",
            "Epochh : 7800, Average Train Meta Loss : 0.2780822524866885, Average Test Meta Loss : 0.3640244391160187\n",
            "Epochh : 7900, Average Train Meta Loss : 0.27652340201298836, Average Test Meta Loss : 0.3643471503924078\n",
            "Epochh : 8000, Average Train Meta Loss : 0.2749924572082524, Average Test Meta Loss : 0.3644075674719492\n",
            "Epochh : 8100, Average Train Meta Loss : 0.27399610063799484, Average Test Meta Loss : 0.3638691760457629\n",
            "Epochh : 8200, Average Train Meta Loss : 0.27283559815629177, Average Test Meta Loss : 0.36350760663952747\n",
            "Epochh : 8300, Average Train Meta Loss : 0.2718195117833846, Average Test Meta Loss : 0.36339297650509567\n",
            "Epochh : 8400, Average Train Meta Loss : 0.27012978132366033, Average Test Meta Loss : 0.3639157528882035\n",
            "Epochh : 8500, Average Train Meta Loss : 0.2688954798726379, Average Test Meta Loss : 0.3643511067956806\n",
            "Epochh : 8600, Average Train Meta Loss : 0.26773797677548156, Average Test Meta Loss : 0.3642344521323198\n",
            "Epochh : 8700, Average Train Meta Loss : 0.2667042211887373, Average Test Meta Loss : 0.3637792388900454\n",
            "Epochh : 8800, Average Train Meta Loss : 0.26582864061606654, Average Test Meta Loss : 0.3638808084756507\n",
            "Epochh : 8900, Average Train Meta Loss : 0.2652235371463557, Average Test Meta Loss : 0.3633493086094546\n",
            "Epochh : 9000, Average Train Meta Loss : 0.26425994455272395, Average Test Meta Loss : 0.3629616999959497\n",
            "Epochh : 9100, Average Train Meta Loss : 0.26303049016207636, Average Test Meta Loss : 0.3636892375815242\n",
            "Epochh : 9200, Average Train Meta Loss : 0.26168555478818095, Average Test Meta Loss : 0.36413483281420966\n",
            "Epochh : 9300, Average Train Meta Loss : 0.2607666475079365, Average Test Meta Loss : 0.364572362190249\n",
            "Epochh : 9400, Average Train Meta Loss : 0.25984629536400355, Average Test Meta Loss : 0.3645645899428799\n",
            "Epochh : 9500, Average Train Meta Loss : 0.258923045078493, Average Test Meta Loss : 0.36463581869048955\n",
            "Epochh : 9600, Average Train Meta Loss : 0.2579117474781906, Average Test Meta Loss : 0.3639840777566971\n",
            "Epochh : 9700, Average Train Meta Loss : 0.25700256791846626, Average Test Meta Loss : 0.3633180314558927\n",
            "Epochh : 9800, Average Train Meta Loss : 0.25563289626910807, Average Test Meta Loss : 0.3633030818716973\n",
            "Epochh : 9900, Average Train Meta Loss : 0.2549033934084249, Average Test Meta Loss : 0.3629826793908257\n",
            "Epochh : 10000, Average Train Meta Loss : 0.2538568360771686, Average Test Meta Loss : 0.36220164995260035\n",
            "Epochh : 10100, Average Train Meta Loss : 0.2524664678665412, Average Test Meta Loss : 0.3620745649911301\n",
            "Epochh : 10200, Average Train Meta Loss : 0.2514898712078353, Average Test Meta Loss : 0.3612403341724779\n",
            "Epochh : 10300, Average Train Meta Loss : 0.2505440029547096, Average Test Meta Loss : 0.3601146517846684\n",
            "Epochh : 10400, Average Train Meta Loss : 0.24933303793232062, Average Test Meta Loss : 0.35923976216972303\n",
            "Epochh : 10500, Average Train Meta Loss : 0.24818265157188563, Average Test Meta Loss : 0.3582035873638932\n",
            "Epochh : 10600, Average Train Meta Loss : 0.2471718748138929, Average Test Meta Loss : 0.357725937228448\n",
            "Epochh : 10700, Average Train Meta Loss : 0.24622617877385886, Average Test Meta Loss : 0.3569298388070935\n",
            "Epochh : 10800, Average Train Meta Loss : 0.2450007903083718, Average Test Meta Loss : 0.35760426198854084\n",
            "Epochh : 10900, Average Train Meta Loss : 0.24418699477006423, Average Test Meta Loss : 0.3576062459886888\n",
            "Epochh : 11000, Average Train Meta Loss : 0.2431601511133912, Average Test Meta Loss : 0.35735056969965534\n",
            "Epochh : 11100, Average Train Meta Loss : 0.24215178784214395, Average Test Meta Loss : 0.3577594733298033\n",
            "Epochh : 11200, Average Train Meta Loss : 0.2413114444897715, Average Test Meta Loss : 0.3575229758071255\n",
            "Epochh : 11300, Average Train Meta Loss : 0.24032788943142808, Average Test Meta Loss : 0.35722489518733197\n",
            "Epochh : 11400, Average Train Meta Loss : 0.23923056232403184, Average Test Meta Loss : 0.35696727801306566\n",
            "Epochh : 11500, Average Train Meta Loss : 0.23805403847448994, Average Test Meta Loss : 0.35672862386160703\n",
            "Epochh : 11600, Average Train Meta Loss : 0.2372467864444305, Average Test Meta Loss : 0.3572148766653221\n",
            "Epochh : 11700, Average Train Meta Loss : 0.23626284764935995, Average Test Meta Loss : 0.35736074051446476\n",
            "Epochh : 11800, Average Train Meta Loss : 0.23522282220461402, Average Test Meta Loss : 0.3579139437587747\n",
            "Epochh : 11900, Average Train Meta Loss : 0.23441087325174603, Average Test Meta Loss : 0.35788127381955054\n",
            "Epochh : 12000, Average Train Meta Loss : 0.23349397303853042, Average Test Meta Loss : 0.35819322459874114\n",
            "Epochh : 12100, Average Train Meta Loss : 0.232630523241168, Average Test Meta Loss : 0.3580943701102954\n",
            "Epochh : 12200, Average Train Meta Loss : 0.23173061018623264, Average Test Meta Loss : 0.3583925350224656\n",
            "Epochh : 12300, Average Train Meta Loss : 0.2309209092533196, Average Test Meta Loss : 0.35802425572471397\n",
            "Epochh : 12400, Average Train Meta Loss : 0.22989396142699356, Average Test Meta Loss : 0.3579161373890269\n",
            "Epochh : 12500, Average Train Meta Loss : 0.2288820831357823, Average Test Meta Loss : 0.3576487337757375\n",
            "Epochh : 12600, Average Train Meta Loss : 0.22794024048363612, Average Test Meta Loss : 0.3576370562580413\n",
            "Epochh : 12700, Average Train Meta Loss : 0.22730256512405095, Average Test Meta Loss : 0.3573910835020134\n",
            "Epochh : 12800, Average Train Meta Loss : 0.22629812431976842, Average Test Meta Loss : 0.35724737896990166\n",
            "Epochh : 12900, Average Train Meta Loss : 0.22519335255081666, Average Test Meta Loss : 0.35756067991325297\n",
            "Epochh : 13000, Average Train Meta Loss : 0.22426958821621204, Average Test Meta Loss : 0.35750038821446983\n",
            "Epochh : 13100, Average Train Meta Loss : 0.2232278970341578, Average Test Meta Loss : 0.35743931438989235\n",
            "Epochh : 13200, Average Train Meta Loss : 0.22230535910983956, Average Test Meta Loss : 0.3582362593587189\n",
            "Epochh : 13300, Average Train Meta Loss : 0.22146806928239657, Average Test Meta Loss : 0.35837903970405094\n",
            "Epochh : 13400, Average Train Meta Loss : 0.220431111465523, Average Test Meta Loss : 0.3583323184763522\n",
            "Epochh : 13500, Average Train Meta Loss : 0.21952986678735567, Average Test Meta Loss : 0.3585699740128595\n",
            "Epochh : 13600, Average Train Meta Loss : 0.21863495723992377, Average Test Meta Loss : 0.3588662724342437\n",
            "Epochh : 13700, Average Train Meta Loss : 0.21768342038227564, Average Test Meta Loss : 0.359339737685582\n",
            "Epochh : 13800, Average Train Meta Loss : 0.21697297109958072, Average Test Meta Loss : 0.3591842507712786\n",
            "Epochh : 13900, Average Train Meta Loss : 0.21616470099717155, Average Test Meta Loss : 0.358830505666198\n",
            "Epochh : 14000, Average Train Meta Loss : 0.21527114040007092, Average Test Meta Loss : 0.3590996378322645\n",
            "Epochh : 14100, Average Train Meta Loss : 0.21442978780638025, Average Test Meta Loss : 0.3595524572607808\n",
            "Epochh : 14200, Average Train Meta Loss : 0.21343222803031625, Average Test Meta Loss : 0.3595259187912343\n",
            "Epochh : 14300, Average Train Meta Loss : 0.21245678044218566, Average Test Meta Loss : 0.3595616264856939\n",
            "Epochh : 14400, Average Train Meta Loss : 0.2115282423327345, Average Test Meta Loss : 0.35981283887348503\n",
            "Epochh : 14500, Average Train Meta Loss : 0.21059488177080216, Average Test Meta Loss : 0.36021448156393626\n",
            "Epochh : 14600, Average Train Meta Loss : 0.20953136686638807, Average Test Meta Loss : 0.36069942562051976\n",
            "Epochh : 14700, Average Train Meta Loss : 0.20874616349732136, Average Test Meta Loss : 0.360884113574131\n",
            "Epochh : 14800, Average Train Meta Loss : 0.20782325752379666, Average Test Meta Loss : 0.36121421748531696\n",
            "Epochh : 14900, Average Train Meta Loss : 0.20681879201432743, Average Test Meta Loss : 0.36216362401621793\n",
            "Epochh : 15000, Average Train Meta Loss : 0.20582667856036296, Average Test Meta Loss : 0.3622894097190793\n",
            "Epochh : 15100, Average Train Meta Loss : 0.20508504261673918, Average Test Meta Loss : 0.36266351953058745\n",
            "Epochh : 15200, Average Train Meta Loss : 0.20411954500746682, Average Test Meta Loss : 0.3631538637640053\n",
            "Epochh : 15300, Average Train Meta Loss : 0.20324903512630582, Average Test Meta Loss : 0.36397561885993046\n",
            "Epochh : 15400, Average Train Meta Loss : 0.20245876712123154, Average Test Meta Loss : 0.36432370232813915\n",
            "Epochh : 15500, Average Train Meta Loss : 0.20171399666992565, Average Test Meta Loss : 0.3649998270894493\n",
            "Epochh : 15600, Average Train Meta Loss : 0.20096771194584043, Average Test Meta Loss : 0.3653870423712566\n",
            "Epochh : 15700, Average Train Meta Loss : 0.2001031044419282, Average Test Meta Loss : 0.36594354072209684\n",
            "Epochh : 15800, Average Train Meta Loss : 0.19910861907726565, Average Test Meta Loss : 0.36702332903721213\n",
            "Epochh : 15900, Average Train Meta Loss : 0.19847147391205977, Average Test Meta Loss : 0.3676255958112991\n",
            "Epochh : 16000, Average Train Meta Loss : 0.19759918787734862, Average Test Meta Loss : 0.36824025115090514\n",
            "Epochh : 16100, Average Train Meta Loss : 0.1968343648170291, Average Test Meta Loss : 0.3689973416315801\n",
            "Epochh : 16200, Average Train Meta Loss : 0.19604930370518442, Average Test Meta Loss : 0.3697958932206967\n",
            "Epochh : 16300, Average Train Meta Loss : 0.1951704770987635, Average Test Meta Loss : 0.369976375938365\n",
            "Epochh : 16400, Average Train Meta Loss : 0.19430059213947995, Average Test Meta Loss : 0.3708123012496655\n",
            "Epochh : 16500, Average Train Meta Loss : 0.1933807891430687, Average Test Meta Loss : 0.3711285542417245\n",
            "Epochh : 16600, Average Train Meta Loss : 0.19263648654115087, Average Test Meta Loss : 0.37144722999492913\n",
            "Epochh : 16700, Average Train Meta Loss : 0.1918410724776261, Average Test Meta Loss : 0.3714474071522796\n",
            "Epochh : 16800, Average Train Meta Loss : 0.19104711121281365, Average Test Meta Loss : 0.37197949363019056\n",
            "Epochh : 16900, Average Train Meta Loss : 0.19030925979647617, Average Test Meta Loss : 0.37211438975018574\n",
            "Epochh : 17000, Average Train Meta Loss : 0.1897183081313213, Average Test Meta Loss : 0.37271683498547825\n",
            "Epochh : 17100, Average Train Meta Loss : 0.18902274204487665, Average Test Meta Loss : 0.3733133383496583\n",
            "Epochh : 17200, Average Train Meta Loss : 0.18834427314471555, Average Test Meta Loss : 0.373781465151387\n",
            "Epochh : 17300, Average Train Meta Loss : 0.18755602293271928, Average Test Meta Loss : 0.37395154533698227\n",
            "Epochh : 17400, Average Train Meta Loss : 0.18685336072256611, Average Test Meta Loss : 0.3739973816465989\n",
            "Epochh : 17500, Average Train Meta Loss : 0.18609504840380162, Average Test Meta Loss : 0.3740717591080826\n",
            "Epochh : 17600, Average Train Meta Loss : 0.18527796635939006, Average Test Meta Loss : 0.3739351571519169\n",
            "Epochh : 17700, Average Train Meta Loss : 0.18451620759166826, Average Test Meta Loss : 0.37443408265094674\n",
            "Epochh : 17800, Average Train Meta Loss : 0.18378875850049328, Average Test Meta Loss : 0.37488512515733313\n",
            "Epochh : 17900, Average Train Meta Loss : 0.18301957679838604, Average Test Meta Loss : 0.3753450905113766\n",
            "Epochh : 18000, Average Train Meta Loss : 0.18229873874711391, Average Test Meta Loss : 0.37546093262645697\n",
            "Epochh : 18100, Average Train Meta Loss : 0.1816472245439635, Average Test Meta Loss : 0.37618916803640323\n",
            "Epochh : 18200, Average Train Meta Loss : 0.1808427479012882, Average Test Meta Loss : 0.37682950606881893\n",
            "Epochh : 18300, Average Train Meta Loss : 0.1800573292084925, Average Test Meta Loss : 0.3774096265994696\n",
            "Epochh : 18400, Average Train Meta Loss : 0.1793560721741391, Average Test Meta Loss : 0.3777704701507884\n",
            "Epochh : 18500, Average Train Meta Loss : 0.17866547361345206, Average Test Meta Loss : 0.37810721053554575\n",
            "Epochh : 18600, Average Train Meta Loss : 0.17802299233849245, Average Test Meta Loss : 0.3784517007749837\n",
            "Epochh : 18700, Average Train Meta Loss : 0.17734360313652325, Average Test Meta Loss : 0.37889645154242874\n",
            "Epochh : 18800, Average Train Meta Loss : 0.17657340822135473, Average Test Meta Loss : 0.37922424140381494\n",
            "Epochh : 18900, Average Train Meta Loss : 0.1758435675712625, Average Test Meta Loss : 0.3792093718016504\n",
            "Epochh : 19000, Average Train Meta Loss : 0.17506887918443867, Average Test Meta Loss : 0.3798777354459204\n",
            "Epochh : 19100, Average Train Meta Loss : 0.17449158257006347, Average Test Meta Loss : 0.3805192379054983\n",
            "Epochh : 19200, Average Train Meta Loss : 0.17382880087020353, Average Test Meta Loss : 0.38079590343682385\n",
            "Epochh : 19300, Average Train Meta Loss : 0.17316583377715686, Average Test Meta Loss : 0.3806477693963666\n",
            "Epochh : 19400, Average Train Meta Loss : 0.17247075187141117, Average Test Meta Loss : 0.3805059421544192\n",
            "Epochh : 19500, Average Train Meta Loss : 0.1717939506433373, Average Test Meta Loss : 0.3810897536386496\n",
            "Epochh : 19600, Average Train Meta Loss : 0.1710758700865837, Average Test Meta Loss : 0.38121287081688\n",
            "Epochh : 19700, Average Train Meta Loss : 0.17037932959551058, Average Test Meta Loss : 0.3814189129095585\n",
            "Epochh : 19800, Average Train Meta Loss : 0.16967741306967335, Average Test Meta Loss : 0.3815538828845409\n",
            "Epochh : 19900, Average Train Meta Loss : 0.16905923037442672, Average Test Meta Loss : 0.38304358638013875\n",
            "Epochh : 20000, Average Train Meta Loss : 0.1684492228989705, Average Test Meta Loss : 0.38420513286058716\n",
            "Epochh : 20100, Average Train Meta Loss : 0.1678459966671565, Average Test Meta Loss : 0.3842010328861822\n",
            "Epochh : 20200, Average Train Meta Loss : 0.16715223542283544, Average Test Meta Loss : 0.3841198410478379\n",
            "Epochh : 20300, Average Train Meta Loss : 0.1665167176533984, Average Test Meta Loss : 0.3849399024699871\n",
            "Epochh : 20400, Average Train Meta Loss : 0.16595194815123088, Average Test Meta Loss : 0.3846044393030241\n",
            "Epochh : 20500, Average Train Meta Loss : 0.16529527967691987, Average Test Meta Loss : 0.3850840148545752\n",
            "Epochh : 20600, Average Train Meta Loss : 0.1646467953682893, Average Test Meta Loss : 0.38532586896052295\n",
            "Epochh : 20700, Average Train Meta Loss : 0.16399570466120422, Average Test Meta Loss : 0.38530208342750205\n",
            "Epochh : 20800, Average Train Meta Loss : 0.16337189031281954, Average Test Meta Loss : 0.3855643365454474\n",
            "Epochh : 20900, Average Train Meta Loss : 0.16277198268525528, Average Test Meta Loss : 0.3857651799843695\n",
            "Epochh : 21000, Average Train Meta Loss : 0.16219414399060494, Average Test Meta Loss : 0.3855820304335774\n",
            "Epochh : 21100, Average Train Meta Loss : 0.16166006065597935, Average Test Meta Loss : 0.38616062401988566\n",
            "Epochh : 21200, Average Train Meta Loss : 0.1609777531516233, Average Test Meta Loss : 0.3867287609576591\n",
            "Epochh : 21300, Average Train Meta Loss : 0.16034327995510278, Average Test Meta Loss : 0.3869994250813885\n",
            "Epochh : 21400, Average Train Meta Loss : 0.15981347129249732, Average Test Meta Loss : 0.3872230846701137\n",
            "Epochh : 21500, Average Train Meta Loss : 0.1593545539161171, Average Test Meta Loss : 0.3874818779426061\n",
            "Epochh : 21600, Average Train Meta Loss : 0.15884808932972172, Average Test Meta Loss : 0.38735622836517924\n",
            "Epochh : 21700, Average Train Meta Loss : 0.15832444321677727, Average Test Meta Loss : 0.3875673982212619\n",
            "Epochh : 21800, Average Train Meta Loss : 0.15776746311805104, Average Test Meta Loss : 0.38746322992782073\n",
            "Epochh : 21900, Average Train Meta Loss : 0.1571733095889816, Average Test Meta Loss : 0.38773998602946735\n",
            "Epochh : 22000, Average Train Meta Loss : 0.1566053561272304, Average Test Meta Loss : 0.38784416251179976\n",
            "Epochh : 22100, Average Train Meta Loss : 0.15599248485121608, Average Test Meta Loss : 0.3880749140715825\n",
            "Epochh : 22200, Average Train Meta Loss : 0.155402956383649, Average Test Meta Loss : 0.3883691562701021\n",
            "Epochh : 22300, Average Train Meta Loss : 0.15479483977436484, Average Test Meta Loss : 0.388041572981752\n",
            "Epochh : 22400, Average Train Meta Loss : 0.15423263392688727, Average Test Meta Loss : 0.3880607427184971\n",
            "Epochh : 22500, Average Train Meta Loss : 0.1537218031823816, Average Test Meta Loss : 0.38827566299339444\n",
            "Epochh : 22600, Average Train Meta Loss : 0.15317063952512666, Average Test Meta Loss : 0.38817303461568514\n",
            "Epochh : 22700, Average Train Meta Loss : 0.1525863933510818, Average Test Meta Loss : 0.3880515809943977\n",
            "Epochh : 22800, Average Train Meta Loss : 0.1520068825196576, Average Test Meta Loss : 0.3880574698611629\n",
            "Epochh : 22900, Average Train Meta Loss : 0.15144091484494177, Average Test Meta Loss : 0.38792994356392335\n",
            "Epochh : 23000, Average Train Meta Loss : 0.1509126939667669, Average Test Meta Loss : 0.3880936328398515\n",
            "Epochh : 23100, Average Train Meta Loss : 0.15044136116596504, Average Test Meta Loss : 0.38812287416679087\n",
            "Epochh : 23200, Average Train Meta Loss : 0.1500103510596689, Average Test Meta Loss : 0.3884608786498034\n",
            "Epochh : 23300, Average Train Meta Loss : 0.14951871559770827, Average Test Meta Loss : 0.388622578239735\n",
            "Epochh : 23400, Average Train Meta Loss : 0.14907612659440528, Average Test Meta Loss : 0.3891740382340044\n",
            "Epochh : 23500, Average Train Meta Loss : 0.1485737361991765, Average Test Meta Loss : 0.3894796890811201\n",
            "Epochh : 23600, Average Train Meta Loss : 0.14804434191541516, Average Test Meta Loss : 0.39004594778404544\n",
            "Epochh : 23700, Average Train Meta Loss : 0.1475573183998695, Average Test Meta Loss : 0.39009984687789456\n",
            "Epochh : 23800, Average Train Meta Loss : 0.14702448959867556, Average Test Meta Loss : 0.38978860229898155\n",
            "Epochh : 23900, Average Train Meta Loss : 0.14649659297062592, Average Test Meta Loss : 0.3895544424858649\n",
            "Epochh : 24000, Average Train Meta Loss : 0.14596229263985797, Average Test Meta Loss : 0.3891517098087375\n",
            "Epochh : 24100, Average Train Meta Loss : 0.14546597442987003, Average Test Meta Loss : 0.38881520988874096\n",
            "Epochh : 24200, Average Train Meta Loss : 0.14491426862583104, Average Test Meta Loss : 0.38868153325509247\n",
            "Epochh : 24300, Average Train Meta Loss : 0.14439940794365802, Average Test Meta Loss : 0.3887311065281626\n",
            "Epochh : 24400, Average Train Meta Loss : 0.14397441098664654, Average Test Meta Loss : 0.3890620199389107\n",
            "Epochh : 24500, Average Train Meta Loss : 0.14345884004452947, Average Test Meta Loss : 0.3891424773546443\n",
            "Epochh : 24600, Average Train Meta Loss : 0.14297305646450528, Average Test Meta Loss : 0.38887937024872954\n",
            "Epochh : 24700, Average Train Meta Loss : 0.14251636381498348, Average Test Meta Loss : 0.38902140201205226\n",
            "Epochh : 24800, Average Train Meta Loss : 0.14203107045920021, Average Test Meta Loss : 0.3891359646980952\n",
            "Epochh : 24900, Average Train Meta Loss : 0.14154725540936625, Average Test Meta Loss : 0.3890074542252826\n",
            "Epochh : 25000, Average Train Meta Loss : 0.14110881097106, Average Test Meta Loss : 0.3892949334763306\n",
            "Epochh : 25100, Average Train Meta Loss : 0.1406722325189684, Average Test Meta Loss : 0.3896751513982324\n",
            "Epochh : 25200, Average Train Meta Loss : 0.14023079079130815, Average Test Meta Loss : 0.3904841945867225\n",
            "Epochh : 25300, Average Train Meta Loss : 0.13976948624982202, Average Test Meta Loss : 0.39087465075697697\n",
            "Epochh : 25400, Average Train Meta Loss : 0.13932074873972017, Average Test Meta Loss : 0.39108544951599644\n",
            "Epochh : 25500, Average Train Meta Loss : 0.1389244491948964, Average Test Meta Loss : 0.39198002842428714\n",
            "Epochh : 25600, Average Train Meta Loss : 0.13843426305795228, Average Test Meta Loss : 0.39231146594936606\n",
            "Epochh : 25700, Average Train Meta Loss : 0.13799713636028652, Average Test Meta Loss : 0.3924705780385512\n",
            "Epochh : 25800, Average Train Meta Loss : 0.13750221250287886, Average Test Meta Loss : 0.3927342413191677\n",
            "Epochh : 25900, Average Train Meta Loss : 0.13701014202555337, Average Test Meta Loss : 0.39282275191475813\n",
            "Epochh : 26000, Average Train Meta Loss : 0.13653706205898702, Average Test Meta Loss : 0.3926860161827034\n",
            "Epochh : 26100, Average Train Meta Loss : 0.13604490141497244, Average Test Meta Loss : 0.39291604946620345\n",
            "Epochh : 26200, Average Train Meta Loss : 0.1356120447767333, Average Test Meta Loss : 0.3929762517770438\n",
            "Epochh : 26300, Average Train Meta Loss : 0.13514971359781314, Average Test Meta Loss : 0.39309190725587745\n",
            "Epochh : 26400, Average Train Meta Loss : 0.1346921320423007, Average Test Meta Loss : 0.39325454095385304\n",
            "Epochh : 26500, Average Train Meta Loss : 0.13421902850374753, Average Test Meta Loss : 0.39325512751347763\n",
            "Epochh : 26600, Average Train Meta Loss : 0.13376579484040602, Average Test Meta Loss : 0.3934306254637894\n",
            "Epochh : 26700, Average Train Meta Loss : 0.13332479868497998, Average Test Meta Loss : 0.39348568674048995\n",
            "Epochh : 26800, Average Train Meta Loss : 0.13287236746559838, Average Test Meta Loss : 0.39346915164627805\n",
            "Epochh : 26900, Average Train Meta Loss : 0.13246454399998742, Average Test Meta Loss : 0.3938105968115813\n",
            "Epochh : 27000, Average Train Meta Loss : 0.13206881092189054, Average Test Meta Loss : 0.39392474582776793\n",
            "Epochh : 27100, Average Train Meta Loss : 0.13173144586296268, Average Test Meta Loss : 0.3941041068881059\n",
            "Epochh : 27200, Average Train Meta Loss : 0.13139875956583222, Average Test Meta Loss : 0.39488938384721056\n",
            "Epochh : 27300, Average Train Meta Loss : 0.13106973955656434, Average Test Meta Loss : 0.39516158678583435\n",
            "Epochh : 27400, Average Train Meta Loss : 0.13068460645210922, Average Test Meta Loss : 0.39512454428629307\n",
            "Epochh : 27500, Average Train Meta Loss : 0.13027230353016997, Average Test Meta Loss : 0.39519962701075195\n",
            "Epochh : 27600, Average Train Meta Loss : 0.12988406864277702, Average Test Meta Loss : 0.3958177698285237\n",
            "Epochh : 27700, Average Train Meta Loss : 0.12943825576474016, Average Test Meta Loss : 0.3957957597006409\n",
            "Epochh : 27800, Average Train Meta Loss : 0.12904883167697606, Average Test Meta Loss : 0.3960545126076181\n",
            "Epochh : 27900, Average Train Meta Loss : 0.12863795684267243, Average Test Meta Loss : 0.3963275843917975\n",
            "Epochh : 28000, Average Train Meta Loss : 0.12824440024395756, Average Test Meta Loss : 0.3968577458867654\n",
            "Epochh : 28100, Average Train Meta Loss : 0.1278822941530286, Average Test Meta Loss : 0.39696281466836686\n",
            "Epochh : 28200, Average Train Meta Loss : 0.12747505881235283, Average Test Meta Loss : 0.39730819096793923\n",
            "Epochh : 28300, Average Train Meta Loss : 0.12705719111358466, Average Test Meta Loss : 0.3971033783550244\n",
            "Epochh : 28400, Average Train Meta Loss : 0.12663417638319116, Average Test Meta Loss : 0.3971799746509236\n",
            "Epochh : 28500, Average Train Meta Loss : 0.1262528097151707, Average Test Meta Loss : 0.39763026626411396\n",
            "Epochh : 28600, Average Train Meta Loss : 0.12590617679396088, Average Test Meta Loss : 0.3978354725327778\n",
            "Epochh : 28700, Average Train Meta Loss : 0.1255233588507481, Average Test Meta Loss : 0.39790577142569866\n",
            "Epochh : 28800, Average Train Meta Loss : 0.12516582691829498, Average Test Meta Loss : 0.39775433130897303\n",
            "Epochh : 28900, Average Train Meta Loss : 0.12479890444981248, Average Test Meta Loss : 0.3977497881290296\n",
            "Epochh : 29000, Average Train Meta Loss : 0.12443329324173319, Average Test Meta Loss : 0.39832183325814624\n",
            "Epochh : 29100, Average Train Meta Loss : 0.12410805671562568, Average Test Meta Loss : 0.3981325655055803\n",
            "Epochh : 29200, Average Train Meta Loss : 0.1237478881686522, Average Test Meta Loss : 0.39849046433939755\n",
            "Epochh : 29300, Average Train Meta Loss : 0.12336423777220411, Average Test Meta Loss : 0.3989262412310822\n",
            "Epochh : 29400, Average Train Meta Loss : 0.12300762896331843, Average Test Meta Loss : 0.3994144768290834\n",
            "Epochh : 29500, Average Train Meta Loss : 0.12262765458185462, Average Test Meta Loss : 0.4000674496168697\n",
            "Epochh : 29600, Average Train Meta Loss : 0.12226930229923907, Average Test Meta Loss : 0.40024284464341975\n",
            "Epochh : 29700, Average Train Meta Loss : 0.1218802662885998, Average Test Meta Loss : 0.4005739361633059\n",
            "Epochh : 29800, Average Train Meta Loss : 0.12149346941662521, Average Test Meta Loss : 0.4012328995364667\n",
            "Epochh : 29900, Average Train Meta Loss : 0.12111424175855746, Average Test Meta Loss : 0.40139032650797307\n",
            "Epochh : 30000, Average Train Meta Loss : 0.12072605927389447, Average Test Meta Loss : 0.401377009673743\n",
            "Epochh : 30100, Average Train Meta Loss : 0.12036001428925362, Average Test Meta Loss : 0.401526251513891\n",
            "Epochh : 30200, Average Train Meta Loss : 0.11997852022739053, Average Test Meta Loss : 0.40202773578877965\n",
            "Epochh : 30300, Average Train Meta Loss : 0.11962229476513474, Average Test Meta Loss : 0.40249244585468175\n",
            "Epochh : 30400, Average Train Meta Loss : 0.1192812447096247, Average Test Meta Loss : 0.4027811911416063\n",
            "Epochh : 30500, Average Train Meta Loss : 0.11889884376160491, Average Test Meta Loss : 0.4032855474598162\n",
            "Epochh : 30600, Average Train Meta Loss : 0.11857234228278221, Average Test Meta Loss : 0.40347323256875584\n",
            "Epochh : 30700, Average Train Meta Loss : 0.11820198503926149, Average Test Meta Loss : 0.4039412631004808\n",
            "Epochh : 30800, Average Train Meta Loss : 0.1178351330261396, Average Test Meta Loss : 0.4042262496465863\n",
            "Epochh : 30900, Average Train Meta Loss : 0.11748418932819284, Average Test Meta Loss : 0.40441355993481826\n",
            "Epochh : 31000, Average Train Meta Loss : 0.11711752586436959, Average Test Meta Loss : 0.4047208658707709\n",
            "Epochh : 31100, Average Train Meta Loss : 0.11676450616820713, Average Test Meta Loss : 0.404904779287245\n",
            "Epochh : 31200, Average Train Meta Loss : 0.1163982105464171, Average Test Meta Loss : 0.4051638850032348\n",
            "Epochh : 31300, Average Train Meta Loss : 0.11604833260268672, Average Test Meta Loss : 0.4055164976002636\n",
            "Epochh : 31400, Average Train Meta Loss : 0.115705990590048, Average Test Meta Loss : 0.40591672683087465\n",
            "Epochh : 31500, Average Train Meta Loss : 0.11534869261082058, Average Test Meta Loss : 0.4057093879168734\n",
            "Epochh : 31600, Average Train Meta Loss : 0.11502012476134403, Average Test Meta Loss : 0.40600297611902264\n",
            "Epochh : 31700, Average Train Meta Loss : 0.11481943499242511, Average Test Meta Loss : 0.4071208576510128\n",
            "Epochh : 31800, Average Train Meta Loss : 0.11485566633138894, Average Test Meta Loss : 0.4088967752436611\n",
            "Epochh : 31900, Average Train Meta Loss : 0.11469147259254643, Average Test Meta Loss : 0.40975527712854853\n",
            "Epochh : 32000, Average Train Meta Loss : 0.11445919438077741, Average Test Meta Loss : 0.4104983549646248\n",
            "Epochh : 32100, Average Train Meta Loss : 0.11423973858166803, Average Test Meta Loss : 0.4108963862309331\n",
            "Epochh : 32200, Average Train Meta Loss : 0.11393969299795045, Average Test Meta Loss : 0.41113583094417416\n",
            "Epochh : 32300, Average Train Meta Loss : 0.11365331924285009, Average Test Meta Loss : 0.41169871012605797\n",
            "Epochh : 32400, Average Train Meta Loss : 0.11335042637948081, Average Test Meta Loss : 0.41235794621462535\n",
            "Epochh : 32500, Average Train Meta Loss : 0.11305525719545113, Average Test Meta Loss : 0.4125412461360626\n",
            "Epochh : 32600, Average Train Meta Loss : 0.11275963509791417, Average Test Meta Loss : 0.4129109148887376\n",
            "Epochh : 32700, Average Train Meta Loss : 0.11245840342166799, Average Test Meta Loss : 0.4130825325184418\n",
            "Epochh : 32800, Average Train Meta Loss : 0.112135534684993, Average Test Meta Loss : 0.41306044151939575\n",
            "Epochh : 32900, Average Train Meta Loss : 0.11181941981890749, Average Test Meta Loss : 0.4131114010178383\n",
            "Epochh : 33000, Average Train Meta Loss : 0.1115395944433349, Average Test Meta Loss : 0.413596014168094\n",
            "Epochh : 33100, Average Train Meta Loss : 0.11123743881541641, Average Test Meta Loss : 0.41387852468770303\n",
            "Epochh : 33200, Average Train Meta Loss : 0.1109100988623249, Average Test Meta Loss : 0.4139551741878235\n",
            "Epochh : 33300, Average Train Meta Loss : 0.11060182946553802, Average Test Meta Loss : 0.4140842324446256\n",
            "Epochh : 33400, Average Train Meta Loss : 0.11029386686546451, Average Test Meta Loss : 0.41426968355036514\n",
            "Epochh : 33500, Average Train Meta Loss : 0.1099851022486368, Average Test Meta Loss : 0.4146923156331648\n",
            "Epochh : 33600, Average Train Meta Loss : 0.10969262590773622, Average Test Meta Loss : 0.4149627076943213\n",
            "Epochh : 33700, Average Train Meta Loss : 0.10942199955842474, Average Test Meta Loss : 0.4151706945333053\n",
            "Epochh : 33800, Average Train Meta Loss : 0.10911132713792276, Average Test Meta Loss : 0.4156013112039451\n",
            "Epochh : 33900, Average Train Meta Loss : 0.10881094427508792, Average Test Meta Loss : 0.4159055819693836\n",
            "Epochh : 34000, Average Train Meta Loss : 0.10849940654705123, Average Test Meta Loss : 0.4161149150435433\n",
            "Epochh : 34100, Average Train Meta Loss : 0.10820721298515684, Average Test Meta Loss : 0.41624049021024645\n",
            "Epochh : 34200, Average Train Meta Loss : 0.10791223912916721, Average Test Meta Loss : 0.4163209954235125\n",
            "Epochh : 34300, Average Train Meta Loss : 0.10760540561690794, Average Test Meta Loss : 0.416550678477804\n",
            "Epochh : 34400, Average Train Meta Loss : 0.10729925066113698, Average Test Meta Loss : 0.41671333800453775\n",
            "Epochh : 34500, Average Train Meta Loss : 0.1069947825577471, Average Test Meta Loss : 0.4170068777779469\n",
            "Epochh : 34600, Average Train Meta Loss : 0.10670289776826652, Average Test Meta Loss : 0.41697947532174595\n",
            "Epochh : 34700, Average Train Meta Loss : 0.10647088662077911, Average Test Meta Loss : 0.41687959289157817\n",
            "Epochh : 34800, Average Train Meta Loss : 0.10626508018503719, Average Test Meta Loss : 0.4170166657810599\n",
            "Epochh : 34900, Average Train Meta Loss : 0.10609916466107251, Average Test Meta Loss : 0.4173000729145112\n",
            "Epochh : 35000, Average Train Meta Loss : 0.10598268566151506, Average Test Meta Loss : 0.4171902311385345\n",
            "Epochh : 35100, Average Train Meta Loss : 0.10576086696396819, Average Test Meta Loss : 0.4175144237515474\n",
            "Epochh : 35200, Average Train Meta Loss : 0.10553038124495023, Average Test Meta Loss : 0.4175662041713239\n",
            "Epochh : 35300, Average Train Meta Loss : 0.10531372466765038, Average Test Meta Loss : 0.41724587568506755\n",
            "Epochh : 35400, Average Train Meta Loss : 0.10506083206065259, Average Test Meta Loss : 0.41720957878163323\n",
            "Epochh : 35500, Average Train Meta Loss : 0.1047843466094004, Average Test Meta Loss : 0.4174343765671141\n",
            "Epochh : 35600, Average Train Meta Loss : 0.10454243030660416, Average Test Meta Loss : 0.41731787473827947\n",
            "Epochh : 35700, Average Train Meta Loss : 0.10428935138198617, Average Test Meta Loss : 0.41736269696239414\n",
            "Epochh : 35800, Average Train Meta Loss : 0.10401493514340748, Average Test Meta Loss : 0.41731229882951526\n",
            "Epochh : 35900, Average Train Meta Loss : 0.1037519336697246, Average Test Meta Loss : 0.41749759078520554\n",
            "Epochh : 36000, Average Train Meta Loss : 0.10348904402896365, Average Test Meta Loss : 0.41730613683221024\n",
            "Epochh : 36100, Average Train Meta Loss : 0.10321624444866898, Average Test Meta Loss : 0.4173525593223392\n",
            "Epochh : 36200, Average Train Meta Loss : 0.10296687654721134, Average Test Meta Loss : 0.4172939426135142\n",
            "Epochh : 36300, Average Train Meta Loss : 0.10270648691960775, Average Test Meta Loss : 0.417481686643771\n",
            "Epochh : 36400, Average Train Meta Loss : 0.10244334351368413, Average Test Meta Loss : 0.4175556014708664\n",
            "Epochh : 36500, Average Train Meta Loss : 0.10219707946608603, Average Test Meta Loss : 0.4175480011629022\n",
            "Epochh : 36600, Average Train Meta Loss : 0.10192739778792846, Average Test Meta Loss : 0.41747165965916444\n",
            "Epochh : 36700, Average Train Meta Loss : 0.10166046800061732, Average Test Meta Loss : 0.41736604350457307\n",
            "Epochh : 36800, Average Train Meta Loss : 0.10140040352427757, Average Test Meta Loss : 0.4171621216153017\n",
            "Epochh : 36900, Average Train Meta Loss : 0.10116800898265564, Average Test Meta Loss : 0.41729786755954745\n",
            "Epochh : 37000, Average Train Meta Loss : 0.1009128781823686, Average Test Meta Loss : 0.41723253484357364\n",
            "Epochh : 37100, Average Train Meta Loss : 0.10065970871413943, Average Test Meta Loss : 0.4172591018953162\n",
            "Epochh : 37200, Average Train Meta Loss : 0.10039444100016791, Average Test Meta Loss : 0.41735785201002523\n",
            "Epochh : 37300, Average Train Meta Loss : 0.10013021574160133, Average Test Meta Loss : 0.41723561768932416\n",
            "Epochh : 37400, Average Train Meta Loss : 0.09990189834347575, Average Test Meta Loss : 0.4173078043263028\n",
            "Epochh : 37500, Average Train Meta Loss : 0.09964040560715795, Average Test Meta Loss : 0.41737570135509755\n",
            "Epochh : 37600, Average Train Meta Loss : 0.09939091292264896, Average Test Meta Loss : 0.41738005758604724\n",
            "Epochh : 37700, Average Train Meta Loss : 0.09914320979277966, Average Test Meta Loss : 0.41753212743587476\n",
            "Epochh : 37800, Average Train Meta Loss : 0.09888587980553297, Average Test Meta Loss : 0.41772060253476084\n",
            "Epochh : 37900, Average Train Meta Loss : 0.09862949644107515, Average Test Meta Loss : 0.4179140189804674\n",
            "Epochh : 38000, Average Train Meta Loss : 0.09838581452040597, Average Test Meta Loss : 0.4179897752903195\n",
            "Epochh : 38100, Average Train Meta Loss : 0.09813684587906078, Average Test Meta Loss : 0.418308186572091\n",
            "Epochh : 38200, Average Train Meta Loss : 0.097928918932922, Average Test Meta Loss : 0.418670229445653\n",
            "Epochh : 38300, Average Train Meta Loss : 0.09775755952441469, Average Test Meta Loss : 0.4189388760764627\n",
            "Epochh : 38400, Average Train Meta Loss : 0.09771191701518157, Average Test Meta Loss : 0.4193342752283856\n",
            "Epochh : 38500, Average Train Meta Loss : 0.09765382939284753, Average Test Meta Loss : 0.4200087068428636\n",
            "Epochh : 38600, Average Train Meta Loss : 0.09751991204902427, Average Test Meta Loss : 0.4202259099265427\n",
            "Epochh : 38700, Average Train Meta Loss : 0.09733048897311421, Average Test Meta Loss : 0.4204012284043662\n",
            "Epochh : 38800, Average Train Meta Loss : 0.09711161624154183, Average Test Meta Loss : 0.42047633341256524\n",
            "Epochh : 38900, Average Train Meta Loss : 0.096915455568977, Average Test Meta Loss : 0.42051572921063385\n",
            "Epochh : 39000, Average Train Meta Loss : 0.09670505523976011, Average Test Meta Loss : 0.4206831494184812\n",
            "Epochh : 39100, Average Train Meta Loss : 0.09647940150576265, Average Test Meta Loss : 0.42081941698624786\n",
            "Epochh : 39200, Average Train Meta Loss : 0.09629590665120115, Average Test Meta Loss : 0.42089572293890787\n",
            "Epochh : 39300, Average Train Meta Loss : 0.09608906352300588, Average Test Meta Loss : 0.4211628513277885\n",
            "Epochh : 39400, Average Train Meta Loss : 0.09587257238527384, Average Test Meta Loss : 0.42139443217087086\n",
            "Epochh : 39500, Average Train Meta Loss : 0.09564272338500017, Average Test Meta Loss : 0.4216532915405356\n",
            "Epochh : 39600, Average Train Meta Loss : 0.09542664767623385, Average Test Meta Loss : 0.4218941121121646\n",
            "Epochh : 39700, Average Train Meta Loss : 0.09519753082796822, Average Test Meta Loss : 0.42221047769414805\n",
            "Epochh : 39800, Average Train Meta Loss : 0.09496855650418784, Average Test Meta Loss : 0.4221839374578015\n",
            "Epochh : 39900, Average Train Meta Loss : 0.09475306904246905, Average Test Meta Loss : 0.42223466186375996\n",
            "Epochh : 40000, Average Train Meta Loss : 0.09453875178633009, Average Test Meta Loss : 0.4224057309782726\n",
            "Epochh : 40100, Average Train Meta Loss : 0.09430893366855785, Average Test Meta Loss : 0.42260595368431136\n",
            "Epochh : 40200, Average Train Meta Loss : 0.0940927572403381, Average Test Meta Loss : 0.4225788364686066\n",
            "Epochh : 40300, Average Train Meta Loss : 0.09387570681061183, Average Test Meta Loss : 0.4223830823068104\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-65e0fb4a9f05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Update model predefined number of times based on k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Evalaute the loss for the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d87a029f92c6>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, graph, lr_k, k)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mkoptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d87a029f92c6>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(new_model, graph, item)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Make model prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Depending on whether we need to return the loss value for storing or for backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e19ca32b049c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_x, edge_index, batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#Standard forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/norm/graph_norm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Few Shot learning with new meta-model"
      ],
      "metadata": {
        "id": "bQjoz6FYctJM"
      },
      "id": "bQjoz6FYctJM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs good few shot learning"
      ],
      "metadata": {
        "id": "m-SPUG5Bfpe9"
      },
      "id": "m-SPUG5Bfpe9"
    },
    {
      "cell_type": "code",
      "source": [
        "graph = GRAPH_TEST[0] \n",
        "k_shot_updates = 5\n",
        "initialization_to_store_meta_losses()\n",
        "for shots in range(k_shot_updates):\n",
        "    new_model = training(model, graph, lr_k, shots)\n",
        "    train_set_evaluation(new_model,graph,store_train_loss_meta) \n",
        "\n",
        "plt.plot(store_train_loss_meta,label = 'Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('k shots')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "GY84TNs8JXVH",
        "outputId": "c2d4bdd0-ad29-4d70-88c9-55dd93844c5c"
      },
      "id": "GY84TNs8JXVH",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'k shots')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Reptile_GraphNN_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}