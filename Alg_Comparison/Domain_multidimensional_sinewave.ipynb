{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UrjQGgr5nUHC",
   "metadata": {
    "id": "UrjQGgr5nUHC"
   },
   "source": [
    "<h1> Imports and Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eGl9mcc0nOMP",
   "metadata": {
    "id": "eGl9mcc0nOMP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: higher in /Users/kcollins/opt/anaconda3/lib/python3.8/site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in /Users/kcollins/opt/anaconda3/lib/python3.8/site-packages (from higher) (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/kcollins/opt/anaconda3/lib/python3.8/site-packages (from torch->higher) (3.7.4.3)\n",
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "!pip3 install higher\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from higher import innerloop_ctx\n",
    "import warnings\n",
    "\n",
    "#The code includes extensive warnings when run so have used this to ignore them\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Set random seeds for reproducibility of results \n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# set GPU or CPU depending on available hardware\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Available device: {device}\")\n",
    "\n",
    "if device == \"cuda:0\": \n",
    "  # set default so all tensors are on GPU, if available\n",
    "  # help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "\n",
    "domain_type = \"multidim_sine\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T3KVOwFXFOY0",
   "metadata": {
    "id": "T3KVOwFXFOY0"
   },
   "source": [
    "<h1> Data Loading and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nMUUm70ufKHH",
   "metadata": {
    "id": "nMUUm70ufKHH"
   },
   "source": [
    "This Sine function generator is based on the repostory: https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3X51uGHDvSV",
   "metadata": {
    "id": "a3X51uGHDvSV"
   },
   "outputs": [],
   "source": [
    "class SineWaveTask_multi:\n",
    "    def __init__(self,dimensions=20):\n",
    "        self.dimensions = dimensions\n",
    "        self.a = []\n",
    "        self.b = []\n",
    "        for dim in range(self.dimensions):\n",
    "          self.a.append(np.random.uniform(0.1, 5.0))\n",
    "          self.b.append(np.random.uniform(0, 2*np.pi))\n",
    "        self.train_x = None\n",
    "        \n",
    "    def f(self, x,a,b):\n",
    "        return a * np.sin(x + b)\n",
    "        \n",
    "    def training_set(self, size=10, force_new=False):\n",
    "        if self.train_x is None and not force_new:\n",
    "            self.train_x = np.random.uniform(-5, 5, size)\n",
    "            x = self.train_x\n",
    "\n",
    "        elif not force_new:\n",
    "            x = self.train_x\n",
    "        else:\n",
    "            x = np.random.uniform(-5, 5, size)\n",
    "\n",
    "        y = self.f(x,self.a[0],self.b[0])[:,None]\n",
    "\n",
    "        for dim in range(self.dimensions-1):\n",
    "          y = np.concatenate((y,self.f(x,self.a[dim+1],self.b[dim+1])[:,None]),axis=-1)\n",
    "\n",
    "        return torch.Tensor(x[:,None]), torch.Tensor(y)\n",
    "    \n",
    "    def test_set(self, size=50):\n",
    "        x = np.linspace(-5, 5, size)\n",
    "        y = self.f(x,self.a[0],self.b[0])[:,None]\n",
    "\n",
    "        for dim in range(self.dimensions-1):\n",
    "          y = np.concatenate((y,self.f(x,self.a[dim+1],self.b[dim+1])[:,None]),axis=-1)\n",
    "\n",
    "        return torch.Tensor(x[:,None]), torch.Tensor(y)\n",
    "    \n",
    "\n",
    "TRAIN_SIZE = 20000\n",
    "TEST_SIZE = 1000\n",
    "SINE_TRAIN = [SineWaveTask_multi() for _ in range(TRAIN_SIZE)]\n",
    "SINE_TEST = [SineWaveTask_multi() for _ in range(TEST_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Ahv9M3tAJrG7",
   "metadata": {
    "id": "Ahv9M3tAJrG7"
   },
   "outputs": [],
   "source": [
    "x, y_true = SINE_TRAIN[0].training_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "jInHQnwIKKxH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jInHQnwIKKxH",
    "outputId": "8760f64a-0585-432f-fa26-e9189ee9a6dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cu4urLF7Q88A",
   "metadata": {
    "id": "cu4urLF7Q88A"
   },
   "source": [
    "<h1> Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "R1B0YTz6ytyN",
   "metadata": {
    "id": "R1B0YTz6ytyN"
   },
   "outputs": [],
   "source": [
    "# Define network\n",
    "class Neural_Network_multi(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=40, output_size=20):\n",
    "        super(Neural_Network_multi, self).__init__()\n",
    "        # network layers\n",
    "        self.hidden1 = nn.Linear(input_size,hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "        #Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        y = x\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G-ExWACxQ3mt",
   "metadata": {
    "id": "G-ExWACxQ3mt"
   },
   "source": [
    "<h1> Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1zyNHFXdOnug",
   "metadata": {
    "id": "1zyNHFXdOnug"
   },
   "outputs": [],
   "source": [
    "# The Minimum Square Error is used to evaluate the difference between prediction and ground truth\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def copy_existing_model(model):\n",
    "    # Function to copy an existing model\n",
    "    # We initialize a new model\n",
    "    new_model = Neural_Network_multi()\n",
    "    # Copy the previous model's parameters into the new model\n",
    "    new_model.load_state_dict(model.state_dict())\n",
    "    return new_model\n",
    "\n",
    "def get_samples_in_good_format(wave, num_samples=10, force_new=False):\n",
    "  #This function is used to sample data from a wave\n",
    "  x, y_true = wave.training_set(size=num_samples, force_new=force_new)\n",
    "  # We add [:,None] to get the right dimensions to pass to the model: we want K x 1 (we have scalars inputs hence the x 1)\n",
    "  # Note that we convert everything torch tensors\n",
    "  x = torch.tensor(x)\n",
    "  y_true = torch.tensor(y_true)\n",
    "  return x.to(device),y_true.to(device)\n",
    "\n",
    "def initialization_to_store_meta_losses():\n",
    "  # This function creates lists to store the meta losses\n",
    "  global store_train_loss_meta; store_train_loss_meta = []\n",
    "  global store_test_loss_meta; store_test_loss_meta = []\n",
    "\n",
    "def test_set_validation(model,new_model,wave,lr_inner,k,store_test_loss_meta):\n",
    "    # This functions does not actually affect the main algorithm, it is just used to evaluate the new model\n",
    "    new_model = training(model, wave, lr_inner, k)\n",
    "    # Obtain the loss\n",
    "    loss = evaluation(new_model, wave)\n",
    "    # Store loss\n",
    "    store_test_loss_meta.append(loss)\n",
    "\n",
    "def train_set_evaluation(new_model,wave,store_train_loss_meta):\n",
    "    loss = evaluation(new_model, wave)\n",
    "    store_train_loss_meta.append(loss) \n",
    "\n",
    "def print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step=1000):\n",
    "  if epoch % printing_step == 0:\n",
    "    print(f'Epochh : {epoch}, Average Train Meta Loss : {np.mean(store_train_loss_meta)}, Average Test Meta Loss : {np.mean(store_test_loss_meta)}')\n",
    "\n",
    "#This is based on the paper update rule, we calculate the difference between parameters and then this is used by the optimizer, rather than doing the update by hand\n",
    "def reptile_parameter_update(model,new_model):\n",
    "  # Zip models for the loop\n",
    "  zip_models = zip(model.parameters(), new_model.parameters())\n",
    "  for parameter, new_parameter in zip_models:\n",
    "    if parameter.grad is None:\n",
    "      parameter.grad = torch.tensor(torch.zeros_like(parameter))\n",
    "    # Here we are adding the gradient that will later be used by the optimizer\n",
    "    parameter.grad.data.add_(parameter.data - new_parameter.data)\n",
    "\n",
    "# Define commands in order needed for the metaupdate\n",
    "# Note that if we change the order it doesn't behave the same\n",
    "def metaoptimizer_update(metaoptimizer):\n",
    "  # Take step\n",
    "  metaoptimizer.step()\n",
    "  # Reset gradients\n",
    "  metaoptimizer.zero_grad()\n",
    "\n",
    "def metaupdate(model,new_model,metaoptimizer):\n",
    "  # Combine the two previous functions into a single metaupdate function\n",
    "  # First we calculate the gradients\n",
    "  reptile_parameter_update(model,new_model)\n",
    "  # Use those gradients in the optimizer\n",
    "  metaoptimizer_update(metaoptimizer)\n",
    "\n",
    "def evaluation(new_model, wave, num_samples=10, force_new=False, item = False):\n",
    "    # Get data\n",
    "    x, label = get_samples_in_good_format(wave,num_samples=num_samples, force_new=force_new)\n",
    "    # Make model prediction\n",
    "    prediction = new_model(x)\n",
    "    # Get loss\n",
    "    if item == True: #Depending on whether we need to return the loss value for storing or for backprop\n",
    "      loss = criterion(prediction,label).item()\n",
    "    else:\n",
    "      loss = criterion(prediction,label)\n",
    "    return loss\n",
    "\n",
    "def training(model, wave, lr_k, k):\n",
    "    # Create new model which we will train on\n",
    "    new_model = copy_existing_model(model)\n",
    "    # Define new optimizer\n",
    "    koptimizer = torch.optim.SGD(new_model.parameters(), lr=lr_k)\n",
    "    # Update the model multiple times, note that k>1 (do not confuse k with K)\n",
    "    for i in range(k):\n",
    "        # Reset optimizer\n",
    "        koptimizer.zero_grad()\n",
    "        # Evaluate the model\n",
    "        loss = evaluation(new_model, wave, item = False)\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        koptimizer.step()\n",
    "    return new_model\n",
    "\n",
    "#Â for MAML -- see MAML cell for additional citations around structure inspiration\n",
    "def task_specific_train_and_eval(model, T_i, inner_loop_optimizer, N=1):\n",
    "    #Description of the loop formulation from https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "    with innerloop_ctx(model, inner_loop_optimizer, copy_initial_weights = False) as (fmodel,diffopt):\n",
    "        #get our input data and our label\n",
    "        x, label = get_samples_in_good_format(T_i,num_samples=num_samples, force_new= True)\n",
    "        per_step_loss = []\n",
    "        for _ in range(N):\n",
    "            #Get the task specific loss for our model\n",
    "            task_specifc_loss = criterion(fmodel(x), label)\n",
    "\n",
    "            #Step through the inner gradient\n",
    "            diffopt.step(task_specifc_loss)\n",
    "            \n",
    "            per_step_loss.append(task_specifc_loss.item())\n",
    "            \n",
    "        held_out_task_specific_loss = evaluation(fmodel, T_i, num_samples=num_samples, force_new=True)\n",
    "        \n",
    "        return held_out_task_specific_loss, per_step_loss, fmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-4Ps8P2IRCmF",
   "metadata": {
    "id": "-4Ps8P2IRCmF"
   },
   "source": [
    "<h1> Reptile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ogpg_DHizlC",
   "metadata": {
    "id": "8ogpg_DHizlC"
   },
   "outputs": [],
   "source": [
    "#Define important variables\n",
    "epochs = int(1e5) # number of epochs \n",
    "lr_meta=0.001 # Learning rate for meta model (outer loop)\n",
    "printing_step=1000 # how many epochs should we wait to print the loss\n",
    "lr_k=0.01 # Internal learning rate\n",
    "k=5 # Number of internal updates for each task\n",
    "\n",
    "# Initializations\n",
    "initialization_to_store_meta_losses()\n",
    "model = Neural_Network_multi()\n",
    "metaoptimizer = torch.optim.Adam(model.parameters(), lr=lr_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "-4-zQWWKFt3s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-4-zQWWKFt3s",
    "outputId": "747b2ee4-fc6a-487e-98a4-a35d0cf2f7b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 0, Average Train Meta Loss : 4.744680404663086, Average Test Meta Loss : 3.1243340969085693\n",
      "Epochh : 1000, Average Train Meta Loss : 4.126384561473912, Average Test Meta Loss : 4.2037262361604615\n",
      "Epochh : 2000, Average Train Meta Loss : 3.8655842161369227, Average Test Meta Loss : 3.9251257088230824\n",
      "Epochh : 3000, Average Train Meta Loss : 3.6657534340587072, Average Test Meta Loss : 3.7080528013748313\n",
      "Epochh : 4000, Average Train Meta Loss : 3.5173155824651245, Average Test Meta Loss : 3.5404870014463596\n",
      "Epochh : 5000, Average Train Meta Loss : 3.388938419939494, Average Test Meta Loss : 3.412119527395619\n",
      "Epochh : 6000, Average Train Meta Loss : 3.2925204327555027, Average Test Meta Loss : 3.321483292483902\n",
      "Epochh : 7000, Average Train Meta Loss : 3.2239420816585382, Average Test Meta Loss : 3.2542117909607997\n",
      "Epochh : 8000, Average Train Meta Loss : 3.169752593681732, Average Test Meta Loss : 3.2057592086815236\n",
      "Epochh : 9000, Average Train Meta Loss : 3.136593209638978, Average Test Meta Loss : 3.1615019086969784\n",
      "Epochh : 10000, Average Train Meta Loss : 3.1052962327322926, Average Test Meta Loss : 3.1276765389450074\n",
      "Epochh : 11000, Average Train Meta Loss : 3.075070533050471, Average Test Meta Loss : 3.101650482767788\n",
      "Epochh : 12000, Average Train Meta Loss : 3.0470134202177035, Average Test Meta Loss : 3.0756117426726273\n",
      "Epochh : 13000, Average Train Meta Loss : 3.022298805627096, Average Test Meta Loss : 3.0562873980868974\n",
      "Epochh : 14000, Average Train Meta Loss : 3.007465878343354, Average Test Meta Loss : 3.0377470182057884\n",
      "Epochh : 15000, Average Train Meta Loss : 2.9939885417641534, Average Test Meta Loss : 3.0242352608831458\n",
      "Epochh : 16000, Average Train Meta Loss : 2.983129062542177, Average Test Meta Loss : 3.00959424927163\n",
      "Epochh : 17000, Average Train Meta Loss : 2.971097851078634, Average Test Meta Loss : 2.9985821044702377\n",
      "Epochh : 18000, Average Train Meta Loss : 2.957192650027265, Average Test Meta Loss : 2.9875673664774434\n",
      "Epochh : 19000, Average Train Meta Loss : 2.9442536476524084, Average Test Meta Loss : 2.975433169095229\n",
      "Epochh : 20000, Average Train Meta Loss : 2.9282407201616105, Average Test Meta Loss : 2.9608481404161604\n",
      "Epochh : 21000, Average Train Meta Loss : 2.912358712164608, Average Test Meta Loss : 2.9456552187066256\n",
      "Epochh : 22000, Average Train Meta Loss : 2.8973391310506873, Average Test Meta Loss : 2.926022713457767\n",
      "Epochh : 23000, Average Train Meta Loss : 2.8802099712780933, Average Test Meta Loss : 2.907947436915103\n",
      "Epochh : 24000, Average Train Meta Loss : 2.8626207199167606, Average Test Meta Loss : 2.8885745741040463\n",
      "Epochh : 25000, Average Train Meta Loss : 2.838560246864818, Average Test Meta Loss : 2.8646398438181353\n",
      "Epochh : 26000, Average Train Meta Loss : 2.8106413000336308, Average Test Meta Loss : 2.840936234641876\n",
      "Epochh : 27000, Average Train Meta Loss : 2.783415939558577, Average Test Meta Loss : 2.8133721797891265\n",
      "Epochh : 28000, Average Train Meta Loss : 2.7562680294256476, Average Test Meta Loss : 2.786204209926192\n",
      "Epochh : 29000, Average Train Meta Loss : 2.7314848055683667, Average Test Meta Loss : 2.7616799934356937\n",
      "Epochh : 30000, Average Train Meta Loss : 2.7067551485065904, Average Test Meta Loss : 2.7365548375246824\n",
      "Epochh : 31000, Average Train Meta Loss : 2.6835308742756414, Average Test Meta Loss : 2.7128084554596796\n",
      "Epochh : 32000, Average Train Meta Loss : 2.6606203283747787, Average Test Meta Loss : 2.6897111996248855\n",
      "Epochh : 33000, Average Train Meta Loss : 2.6384129988938936, Average Test Meta Loss : 2.66713882950446\n",
      "Epochh : 34000, Average Train Meta Loss : 2.6182128918785015, Average Test Meta Loss : 2.645254269120013\n",
      "Epochh : 35000, Average Train Meta Loss : 2.598701587454588, Average Test Meta Loss : 2.625226677135891\n",
      "Epochh : 36000, Average Train Meta Loss : 2.5797890475171563, Average Test Meta Loss : 2.6061398266540667\n",
      "Epochh : 37000, Average Train Meta Loss : 2.5601320836097723, Average Test Meta Loss : 2.58728709112006\n",
      "Epochh : 38000, Average Train Meta Loss : 2.543578188101046, Average Test Meta Loss : 2.56960105714442\n",
      "Epochh : 39000, Average Train Meta Loss : 2.5268060987997365, Average Test Meta Loss : 2.5529697241225255\n",
      "Epochh : 40000, Average Train Meta Loss : 2.510013569134381, Average Test Meta Loss : 2.536843846832698\n",
      "Epochh : 41000, Average Train Meta Loss : 2.4951686287670727, Average Test Meta Loss : 2.521981095904121\n",
      "Epochh : 42000, Average Train Meta Loss : 2.4801928262383917, Average Test Meta Loss : 2.50671593652209\n",
      "Epochh : 43000, Average Train Meta Loss : 2.465855417798814, Average Test Meta Loss : 2.491884450465202\n",
      "Epochh : 44000, Average Train Meta Loss : 2.452440348981169, Average Test Meta Loss : 2.4783077234334696\n",
      "Epochh : 45000, Average Train Meta Loss : 2.4396387894184297, Average Test Meta Loss : 2.4647069657363616\n",
      "Epochh : 46000, Average Train Meta Loss : 2.425429240553908, Average Test Meta Loss : 2.4517849375651153\n",
      "Epochh : 47000, Average Train Meta Loss : 2.4122084358027456, Average Test Meta Loss : 2.4391358956786227\n",
      "Epochh : 48000, Average Train Meta Loss : 2.3994904139506446, Average Test Meta Loss : 2.4262875973338898\n",
      "Epochh : 49000, Average Train Meta Loss : 2.3867731100164082, Average Test Meta Loss : 2.4138609691957136\n",
      "Epochh : 50000, Average Train Meta Loss : 2.3739769859808115, Average Test Meta Loss : 2.4016119063327306\n",
      "Epochh : 51000, Average Train Meta Loss : 2.361532881229398, Average Test Meta Loss : 2.389439706443485\n",
      "Epochh : 52000, Average Train Meta Loss : 2.3491268385895085, Average Test Meta Loss : 2.378088794339884\n",
      "Epochh : 53000, Average Train Meta Loss : 2.3367224598098866, Average Test Meta Loss : 2.365887557495783\n",
      "Epochh : 54000, Average Train Meta Loss : 2.324484592490757, Average Test Meta Loss : 2.3535653854097025\n",
      "Epochh : 55000, Average Train Meta Loss : 2.312951556526004, Average Test Meta Loss : 2.3409113794458456\n",
      "Epochh : 56000, Average Train Meta Loss : 2.3010351522499453, Average Test Meta Loss : 2.328999504025235\n",
      "Epochh : 57000, Average Train Meta Loss : 2.289021973154579, Average Test Meta Loss : 2.31629045190808\n",
      "Epochh : 58000, Average Train Meta Loss : 2.2763238252019815, Average Test Meta Loss : 2.3038961048248594\n",
      "Epochh : 59000, Average Train Meta Loss : 2.264285107487886, Average Test Meta Loss : 2.2917494390118365\n",
      "Epochh : 60000, Average Train Meta Loss : 2.2521588531020584, Average Test Meta Loss : 2.279637878260925\n",
      "Epochh : 61000, Average Train Meta Loss : 2.2405216759377473, Average Test Meta Loss : 2.2680584964421975\n",
      "Epochh : 62000, Average Train Meta Loss : 2.2289352528382578, Average Test Meta Loss : 2.256273531885013\n",
      "Epochh : 63000, Average Train Meta Loss : 2.2169801268699802, Average Test Meta Loss : 2.2442444345751116\n",
      "Epochh : 64000, Average Train Meta Loss : 2.205319085153038, Average Test Meta Loss : 2.2323704335126515\n",
      "Epochh : 65000, Average Train Meta Loss : 2.193856204356791, Average Test Meta Loss : 2.220965901091327\n",
      "Epochh : 66000, Average Train Meta Loss : 2.1821466609729514, Average Test Meta Loss : 2.2091745217355516\n",
      "Epochh : 67000, Average Train Meta Loss : 2.170633523986087, Average Test Meta Loss : 2.1976115573224013\n",
      "Epochh : 68000, Average Train Meta Loss : 2.1588226286046464, Average Test Meta Loss : 2.185830558890213\n",
      "Epochh : 69000, Average Train Meta Loss : 2.1473592158376578, Average Test Meta Loss : 2.1745238913766087\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3cd69cebc74b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Note that we need to sample the wave from the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mwave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINE_TEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtest_set_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwave\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstore_test_loss_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Print losses every 'printing_step' epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7e1e6714c74e>\u001b[0m in \u001b[0;36mtest_set_validation\u001b[0;34m(model, new_model, wave, lr_inner, k, store_test_loss_meta)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_set_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_inner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstore_test_loss_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# This functions does not actually affect the main algorithm, it is just used to evaluate the new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Obtain the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7e1e6714c74e>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, wave, lr_k, k)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mkoptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7e1e6714c74e>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(new_model, wave, item)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_samples_in_good_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Make model prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Depending on whether we need to return the loss value for storing or for backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c1aae08626a2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # Sample a sine wave (Task from training data)\n",
    "    wave = random.sample(SINE_TRAIN, 1)\n",
    "\n",
    "    # Update model predefined number of times based on k\n",
    "    new_model = training(model, wave[0], lr_k, k)\n",
    "\n",
    "    # Evalaute the loss for the training data\n",
    "    train_set_evaluation(new_model,wave[0],store_train_loss_meta)     \n",
    "    \n",
    "    #Meta-update --> Get gradient for meta loop and update\n",
    "    metaupdate(model,new_model,metaoptimizer)\n",
    "    \n",
    "    # Evalaute the loss for the test data\n",
    "    # Note that we need to sample the wave from the test data\n",
    "    wave = random.sample(SINE_TEST, 1)\n",
    "    test_set_validation(model,new_model,wave[0],lr_k,k,store_test_loss_meta)\n",
    "\n",
    "    # Print losses every 'printing_step' epochs\n",
    "    print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQjoz6FYctJM",
   "metadata": {
    "id": "bQjoz6FYctJM"
   },
   "source": [
    "<h1> Few Shot learning with new meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m-SPUG5Bfpe9",
   "metadata": {
    "id": "m-SPUG5Bfpe9"
   },
   "source": [
    "The model performs good few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "GY84TNs8JXVH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "GY84TNs8JXVH",
    "outputId": "1f8f1545-9c7f-4c55-bd7a-520642859f4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'k shots')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wave = SineWaveTask_multi(); \n",
    "k_shot_updates = 4\n",
    "initialization_to_store_meta_losses()\n",
    "for shots in range(k_shot_updates):\n",
    "    new_model = training(model, wave, lr_k, shots)\n",
    "    train_set_evaluation(new_model,wave,store_train_loss_meta) \n",
    "\n",
    "plt.plot(store_train_loss_meta,label = 'Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('k shots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f7a50",
   "metadata": {
    "id": "5lL1NN2OPBSD"
   },
   "source": [
    "## Second-Order MAML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b19ec9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter =  0  Current Loss 4.405134677886963  Val Loss:  3.7866437435150146\n",
      "Iter =  5000  Current Loss 3.3949404376860834  Val Loss:  3.385582734503476\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Handling computation graphs and second-order backprop help and partial inspiration from: \n",
    "- https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2 \n",
    "- https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853/3 \n",
    "- https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "- https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "- https://discuss.pytorch.org/t/how-to-manually-update-network-parameters-while-keeping-track-of-its-computational-graph/131642/2\n",
    "- https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "- https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "- https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "\n",
    "Neural network configuration and helper class functions copied directly from \n",
    "-https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "Note, different ways to refer to the task-specific vs. meta/aggregate updates to the parameters\n",
    "Sometimes called \"inner\" and \"outer\" loop, respectively\n",
    "Here, refered to as \"task_specific\" and \"agg\"/meta\" (the latter, for consistency w/ ocariz code)\n",
    "'''\n",
    "\n",
    "\n",
    "#Instantiate the model network\n",
    "model = Neural_Network_multi()\n",
    "# move to the current device (GPU or CPU)\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "model.to(device)\n",
    "\n",
    "T = 25 # num tasks\n",
    "N = 1 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "num_samples = 10 # number of samples to draw from the task\n",
    "lr_task_specific = 0.01 # task specific learning rate\n",
    "lr_meta = 0.001 # meta-update learning rate\n",
    "num_epochs = 10000#70001 #Number of iterations for outer loop\n",
    "printing_step = 5000 # show log of loss every x epochs\n",
    "\n",
    "#Used to store the validation losses\n",
    "metaLosses = []\n",
    "metaValLosses = []\n",
    "\n",
    "#Meta-optimizer for the outer loop\n",
    "meta_optimizer = torch.optim.Adam(model.parameters(), lr = lr_meta)\n",
    "\n",
    "#Inner optimizer, we were doing this by hand previously\n",
    "inner_loop_optimizer = torch.optim.SGD(model.parameters(), lr = lr_task_specific)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # store loss over all tasks to then do a large meta-level update of initial params\n",
    "    # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "    meta_loss = None\n",
    "    \n",
    "    #Sample a new wave each time\n",
    "    waves = [SineWaveTask_multi() for _ in range(T)]\n",
    "    \n",
    "    #Loop through all of the tasks\n",
    "    for i, T_i in enumerate(waves): \n",
    "        held_out_task_specific_loss, _, _ = task_specific_train_and_eval(model, T_i, inner_loop_optimizer, N)\n",
    "        if meta_loss is None: \n",
    "            meta_loss = held_out_task_specific_loss\n",
    "        else:\n",
    "            meta_loss += held_out_task_specific_loss\n",
    "            \n",
    "    meta_optimizer.zero_grad()\n",
    "    meta_loss /= T\n",
    "    meta_loss.backward()\n",
    "    meta_optimizer.step()\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    # validation \n",
    "    val_wave = SineWaveTask_multi() # our own addition -- can vary\n",
    "    val_loss, _, _ = task_specific_train_and_eval(model, val_wave, inner_loop_optimizer, N)\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step == 0:\n",
    "        print(\"Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), f\"{domain_type}_maml_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07479119",
   "metadata": {},
   "source": [
    "<h1> Few Shot learning with new meta-model (MAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "effd0720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 10.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbZ0lEQVR4nO3da2xc93nn8e/DufDO4VW3GclKYie2JFJ2ow2cGrtI7WbXcb3yAk0XWaDZtMjCQJFukm0WRZIXKZpXLbBI293sNjCStAkapNl13I1iOG3dxN0kQOJGdiRKsuL6EsciKYkUyeGdHF6efXFmSJqmNENyyDNz5vcBhDkzc3jmySD+/c/5X86YuyMiItFVF3YBIiKyuxT0IiIRp6AXEYk4Bb2ISMQp6EVEIi4e1gd3d3f70aNHw/p4EZGq9Nxzz91w956t/E1oQX/06FHOnj0b1seLiFQlM/vFVv9GXTciIhGnoBcRiTgFvYhIxCnoRUQiTkEvIhJxCnoRkYhT0IuIRFxoQT88tRDWR4uI1JTQgv765Dzzi8thfbyISM0Itevm8tXJMD9eRKQmhBr0FwYnwvx4EZGaEFrQx+uM/gEFvYjIbgst6BsTMS4o6EVEdl3JQW9mMTP7qZk9ucl79Wb2DTN72cyeNbOjxY7XmIzx0vAUs7mlLZYsIiJbsZUz+o8Bl2/y3oeBcXe/HfgT4I+LHawxGWPFNSArIrLbSgp6M8sAvwZ88Sa7PAJ8Jb/9OPCAmdmtjtmYCG6Fr356EZHdVeoZ/Z8Cvw+s3OT9NHAFwN2XgAmga+NOZvaomZ01s7PZsRvsa61XP72IyC4rGvRm9jAw7O7P7fTD3P0xdz/l7qd6enroy6To1xRLEZFdVcoZ/X3AaTN7Dfhr4H4z+6sN+wwChwHMLA6kgNFiB+5Nt/PKyDQzCxqQFRHZLUWD3t0/5e4Zdz8KfAD4nrv/5obdzgAfym+/P7+PFzt2b6YNd7g0pAFZEZHdsu159Gb2WTM7nX/6JaDLzF4Gfg/4ZCnHOJFOAdA/kN1uGSIiUkR8Kzu7+z8C/5jf/sy61+eB39jqh+9rbeBgqkG3QhAR2UWh34++N53SzBsRkV0UetD3ZVK8emOGqfnFsEsREYmk0IO+0E9/cVADsiIiuyH0oO/NB/2FwWy4hYiIRFToQd/VUk+6vVG3QhAR2SWhBz0E/fQXNfNGRGRXVETQn0ineG10lolZDciKiJRbRQR9XyY/IDuks3oRkXKriKDvXV0hq6AXESm3igj69qYkRzqb1E8vIrILKiLoITir79cUSxGRsqucoM+kuDI2x/hMLuxSREQipWKCvm914ZS6b0REyqligv64gl5EZFdUTNCnGhO8pbtZd7IUESmzigl6CBZO6YxeRKS8Kiro+9IpBrNz3JheCLsUEZHIqKig782on15EpNwqKuiPH2rDDC6qn15EpGwqKuhbG4IB2X6d0YuIlE1FBT0E/fSaeSMiUj4VF/S9mXauTc4zPDkfdikiIpFQcUHfpwFZEZGyqrigP3awjTpT0IuIlEvFBX1zfZy39bSon15EpEwqLughmE/fPziBu4ddiohI1Ssa9GbWYGb/ZGbnzeySmf3hJvv8lpmNmNm5/L//tJOi+tIpRqYWuD6pFbIiIjsVL2GfBeB+d582swTwQzP7jrv/eMN+33D33y1HUb2ZdiDopz+QaijHIUVEalbRM3oPTOefJvL/drVPZXVAdiC7mx8jIlITSuqjN7OYmZ0DhoGn3f3ZTXb7dTPrN7PHzezwTY7zqJmdNbOzIyMjN/28xmSMt+9v1QpZEZEyKCno3X3Z3e8GMsC7zOzEhl2+DRx19z7gaeArNznOY+5+yt1P9fT03PIze/MrZDUgKyKyM1uadePuWeAZ4MENr4+6e2Hk9IvAO3daWF8mxehMjqEJrZAVEdmJUmbd9JhZe367EXgv8LMN+xxc9/Q0cHmnhZ0o/LSg5tOLiOxIKWf0B4FnzKwf+AlBH/2TZvZZMzud3+ej+amX54GPAr+108LuOthGvM64MJjd6aFERGpa0emV7t4P3LPJ659Zt/0p4FPlLKwhkR+Q1Rm9iMiOVOTK2IK+TPAbshqQFRHZvooO+t5MiuzsIgPjc2GXIiJStSo76NO6ZbGIyE5VdNC/40AriZipn15EZAcqOujr4zHuPNCmmTciIjtQ0UEPQT+9VsiKiGxf5Qd9OsXk/BKvj82GXYqISFWqiqAH1E8vIrJNFR/0b9/fSjJep5k3IiLbVPFBn4zXcdfBNvp1b3oRkW2p+KAH6E23cWlwkpUVDciKiGxVVQR9X7qdqYUlXhudCbsUEZGqUxVB35vRClkRke2qiqC/Y18L9fE6zbwREdmGqgj6eKyO44fa9CMkIiLbUBVBD8F8+ktDEyxrQFZEZEuqJ+gz7czklvn5jemwSxERqSpVE/R9Ga2QFRHZjqoJ+rf1tNCYiCnoRUS2qGqCPlZnHD/UxkVNsRQR2ZKqCXoI5tNfGppkaXkl7FJERKpGVQV9XybF3OIyr4xohayISKmqKuh70+0AusGZiMgWVFXQv7W7meZkTP30IiJbUFVBX1dnHE+n6FfQi4iUrKqCHqAvneKFoUkWNSArIlKSokFvZg1m9k9mdt7MLpnZH26yT72ZfcPMXjazZ83s6K5USzDzZmFphZeua4WsiEgpSjmjXwDud/eTwN3Ag2Z274Z9PgyMu/vtwJ8Af1zWKtfpy7QDcGEwu1sfISISKUWD3gOF0+dE/t/GO4s9Anwlv/048ICZWdmqXOe2ziZa6+O6N72ISIlK6qM3s5iZnQOGgafd/dkNu6SBKwDuvgRMAF2bHOdRMztrZmdHRka2V3CdcSKd0i2LRURKVFLQu/uyu98NZIB3mdmJ7XyYuz/m7qfc/VRPT892DgEEC6cuX50it6QBWRGRYrY068bds8AzwIMb3hoEDgOYWRxIAaNlqG9TvZkUueUV/vn61G59hIhIZJQy66bHzNrz243Ae4GfbdjtDPCh/Pb7ge+5+679QkhvWr8hKyJSqlLO6A8Cz5hZP/ATgj76J83ss2Z2Or/Pl4AuM3sZ+D3gk7tTbuBIZxNtDXHdslhEpATxYju4ez9wzyavf2bd9jzwG+Ut7ebMjL5Mu6ZYioiUoOpWxhb0ZlK8eG2K+cXlsEsREaloVRv0fekUi8vOi9c0ICsicitVG/QnNCArIlKSqg36TEcjHU0JLZwSESmiaoPezOjNtOuWxSIiRVRt0EPQT//P1zUgKyJyK1Ud9CfSKZZXnMtXJ8MuRUSkYlV10PdlNCArIlJMVQf9wVQD3S1JrZAVEbmFqg56M6NXtywWEbmlqg56gN5MOy8NTzGbWwq7FBGRilT9QZ9OseJoQFZE5CaqPugLA7LqpxcR2VzVB/3+tgb2tdarn15E5CaqPughOKvXClkRkc1FIuhPpFO8MjLNzIIGZEVENopE0PdlUrjDpSENyIqIbBSJoC/csrh/IBtuISIiFSgSQb+vtYGDqQbdCkFEZBORCHoIzuo180ZE5M0iE/R96RSv3phhan4x7FJERCpKZIK+N79w6uKgBmRFRNaLTtCv/oZsNtxCREQqTGSCvqulnnR7o26FICKyQWSCHoKz+ouaeSMi8gbRCvpMitdGZ5mY1YCsiEhB0aA3s8Nm9oyZvWBml8zsY5vs8x4zmzCzc/l/n9mdcm+tcCfLi0M6qxcRKYiXsM8S8Al3f97MWoHnzOxpd39hw34/cPeHy19i6XrTa7csvu/27jBLERGpGEXP6N39qrs/n9+eAi4D6d0ubDvam5Ic7mxUP72IyDpb6qM3s6PAPcCzm7z9bjM7b2bfMbPjN/n7R83srJmdHRkZ2Xq1JehLt9OvKZYiIqtKDnozawG+CXzc3TeuSnoeuM3dTwL/A/i/mx3D3R9z91Pufqqnp2ebJd9abybFlbE5xmdyu3J8EZFqU1LQm1mCIOS/5u5PbHzf3SfdfTq//RSQMLNQOsn7VhdOqftGRARKm3VjwJeAy+7+uZvscyC/H2b2rvxxR8tZaKmOK+hFRN6glFk39wEfBC6Y2bn8a58GjgC4+xeA9wO/Y2ZLwBzwAXf38pdbXKoxwdGuJt3JUkQkr2jQu/sPASuyz+eBz5erqJ3qzbTz/C/Gwy5DRKQiRGplbEFfOsVgdo4b0wthlyIiErpIBn3hlsXqpxcRiWjQHz/UBsBF9dOLiEQz6FsbEry1p5l+ndGLiEQz6CHop9fMGxGRCAd9b6ada5PzDE/Oh12KiEioohv0WjglIgJEOOiPH2rDTEEvIhLZoG+uj3N7T4v66UWk5kU26CGYT98/OEFId2MQEakIkQ76vnSKkakFrk9qhayI1K5IB71WyIqIRDzojx1MUWdwYSAbdikiIqGJdNA3JmO8fX+rVsiKSE2LdNBDMJ/+woAGZEWkdkU/6DMpRmdyDE1ohayI1KboB31hhazm04tIjYp80N91sI14nXFhMBt2KSIioYh80Dck8gOyOqMXkRoV+aCH/ICsVsiKSI2qjaDPpMjOLjIwPhd2KSIie64mgr5PK2RFpIbVRNC/40AriZipn15EalJNBH19PMadB9o080ZEalJNBD3ACa2QFZEaVTTozeywmT1jZi+Y2SUz+9gm+5iZ/Xcze9nM+s3sl3an3O3ry6SYnF/i9bHZsEsREdlTpZzRLwGfcPdjwL3AR8zs2IZ93gfckf/3KPDnZa2yDAorZNVPLyK1pmjQu/tVd38+vz0FXAbSG3Z7BPiqB34MtJvZwbJXuwNv399KMl6nmTciUnO21EdvZkeBe4BnN7yVBq6sez7AmxuDUCXjddx1oJV+3ZteRGpMyUFvZi3AN4GPu/vkdj7MzB41s7NmdnZkZGQ7h9iR3kyKS4OTrKxoQFZEakdJQW9mCYKQ/5q7P7HJLoPA4XXPM/nX3sDdH3P3U+5+qqenZzv17khfup2phSVeG53Z888WEQlLKbNuDPgScNndP3eT3c4A/zE/++ZeYMLdr5axzrLQb8iKSC2Kl7DPfcAHgQtmdi7/2qeBIwDu/gXgKeAh4GVgFvjtsldaBnfsa6E+Xkf/wASP3F1RQwgiIrumaNC7+w8BK7KPAx8pV1G7JR6r49ihNp3Ri0hNqZmVsQV96RSXBidY1oCsiNSImgv63kw7M7llfn5jOuxSRET2RM0FfeGWxVohKyK1ouaC/m09LTQmYgp6EakZNRf0sTrj+KE2LmpAVkRqRM0FPeRXyA5NsrS8EnYpIiK7riaDvi+TYm5xmVdGtEJWRKKvJoN+7ZbF2XALERHZAzUZ9G/pbqE5GVM/vYjUhJoM+lidcTydol9BLyI1oCaDHoIVsi8MTbKoAVkRibiaDfreTIqFpRVeuq4VsiISbTUb9H2ZdgA+862LfPO5AabmF8MtSERkl5Rym+JIOtrVxKcfupOv/ugXfOL/nKf+b+q4/859nD55iF+5cx8NiVjYJYqIlIUFdxjee6dOnfKzZ8+G8tnruTvPv57l2+eHeLL/KjemF2ipj/Ovj+/n9MlD3Hd7N4lYzV74iEiFMbPn3P3Ulv6m1oN+vaXlFX786hhnzg/ynYvXmJpforM5yUO9Bzh9Ms2p2zqoq7vlrflFRHaVgr6MFpaW+X8vjnDm/BD/cPk684srHEw18HDfQU6fTHMi3UbwK4siIntHQb9LZhaW+IfL1zlzbojvvzTC4rLz1u5mHj55iNMnD3H7vpawSxSRGqGg3wPZ2Rx/e/EaZ84P8aNXR3GHYwfbOH33If7tyUOk2xvDLlFEIkxBv8eGJ+d5sv8qZ84Pce5KFoBTt3Vw+u5DPNR7kO6W+nALFJHIUdCH6PXRWb7dP8SZc0O8eH2KWJ3xy2/r4vTJQ/ybEwdoa0iEXaKIRICCvkK8eG2KM+cHOXN+iCtjcyTjdfzKO3o4fTLNA3dpjr6IbJ+CvsK4O+euZDmTn6M/MrVAczLGe4/t5/Tdh/iXd/Rojr6IbImCvoItrzjPvjrKmfNDfOfiNSbmFmlvSvC+Ewf51bv2cVtXE5mOJp3ti8gtKeirRG5phR+8FMzRf/qF68zmllff62mtJ9PRyOGOJg53NpLpaFrdPphqJBnXFYBILdtO0NfsvW7ClIzX8cBd+3ngrv3M5pa4fHWSK2NzDIzPcmVsjivjs5y7kuWpC1dZWllriOsMDrQ1kOloItMZNAaZjkYOdzZxuLOJA20NxLRyV0Q2KBr0ZvZl4GFg2N1PbPL+e4BvAT/Pv/SEu3+2jDVGWlMyzjtv6+Sdt735vaXlFa5NzjMwPseVsVmujAeNwcDYHD96ZZS/mRxk/QVZvM441N4YXAm0r7siyDcK3S31uoWDSA0q5Yz+L4HPA1+9xT4/cPeHy1KRrIrH6oKz944m7n1r15vezy2tMJQNrgA2Ngbf/dkwN6YX3rB/fbyOdMeGK4F12x1NCd3WQSSCiga9u3/fzI7uQS2yRcl4HUe7mzna3bzp+3O5ZQaza91Ba43BLOcHsmRnF990vH2t9exrrWd/W0OwnX/c39bAvrZ69rc20K4GQaSqlKuP/t1mdh4YAv6ru1/abCczexR4FODIkSNl+mi5mcZkjNv3tXL7vtZN35+cX2SgMDYwPsfw5DzDUwtcn5znpeFpfvjyDabml970d8lYHT2t9avBv68taAh61jUQ+9sadIUgUiFKmnWTP6N/8iZ99G3AirtPm9lDwJ+5+x3FjlnLs26qyVxumeGptQZgeHKB61PzjOQfhyeD1yc3aRASMWNfa6EBuMlVQms9HU1JjR2IlCiUWTfuPrlu+ykz+19m1u3uN3Z6bAlfYzLGbV3N3Na1efdQwfziMsOTCwxPzXN9w+Pw5AI/vzHDj18dY2LuzT/ZmIgZPS1BA9DTWk93Sz3dLUm6W+rpyj8WnqcadZUgslU7DnozOwBcd3c3s3cR/A7t6I4rk6rSkIhxpKuJI11Nt9xvfnGZkal1DcHkPNenFlYbiStjs/z09XHGZnKsbHKxGa8zulqSdDXX09261gB0r3utqzlJT2s9nc1JrTwWobTplV8H3gN0m9kA8AdAAsDdvwC8H/gdM1sC5oAPeFirsKTiNSRiq/P+b2V5xRmfzTE6nePG9EL+X47R/Hbh9VeGpxmZXiC3tLLpcdqbEnQ1FxqDfIOQ3954tdBcr2UlEk1aGStVz92ZXlha1yjk3tAYjM4scGNqrcHYbDwBoDERWw3/zuYknc1JupqTdOS3O5uSdLasvdZaH1c3kuw5rYyVmmRmtDYkaG1I3HSq6XoLS8uMzeSC8J9Z4MbUAqMzubXH6QWuTcxz+eokozO5m14tJGN1dDQn6GhK0tWSDB6bk3Q219PZnKCzuZ6O5gRdzUHD0dGUIK6uJAmBgl5qTn08xsFUcO+gYtyd2VzQMIzO5Bi/yePYzAJD2UlGb3HFAJBqTNz8KiG/3dmUXL2iaErGdNUgO6agF7kFM6O5Pk5zfbzouELB4vIK47M5xmcWGZ1ZYGyzBmI2x5WxWc5fyTI+m2NxefMu1PVXDe1Nhccknc1r2x1NifxrwXZbQ0LTVeUNFPQiZZaI1bGvtYF9rQ3A5ovV1nN3phaWNr1aGJ9dzD/myM4u8vLw9Or20mbTkghufpdqDBqCjua1hqCjKZF/vv61JB3NCdobk7ozaoQp6EVCZma0NQRn4sXWKxQUGofszCLjsznGZnNk81cR2fzz8dlgeyg7z6WhScZnc8wvbj7eANBSH1+9aig0EIUrifbGoGFozzcQwXNdPVQLBb1IFVrfOBRbu7De/OJy0DDMBFcF44UGYabQWKy99tqNGcZnc5veBmOtjuDq4Q0NwYbtjuZksE/hSqIxSWtDXA3EHlLQi9SQhkTpA9EFS8srTM4vrXYZTcwVGoRFJmZzZOcWV68exmZyvDIyTXZ2saQGoqOp0Ai8ebu9KfGGBiLVGMys0m8ubJ2CXkRuKR6rW50FtBVLyytMzC2SnVtcbSDGZ4LnE4UribmggRidLr2BaGtIrF4tpAqNQ76BKDQMwXuJ1cdUY4L6eO3+TKeCXkR2RTxWR1dLPV0t9Vv6u40NRDZ/JZGdW2Ribu0qovDalbFZsrM5JuYWN71tRkFTMkaqMbF61dDemL9qKDQQjcl1DUjQYKQaEzRHYIqrgl5EKsp2G4iVlWCAemJ2kexcEPyrDcSbGotFXr0xvfrazRbFQXB/pcLVwh/9eh//4mjnTv8n7jkFvYhEQl2drZ6xH6H0AWoIBqmz+QYiuIoIupre2Fgs0taQ2KXqd5eCXkRqXkMixoFUjAOphrBL2RVaISEiEnEKehGRiFPQi4hEnIJeRCTiFPQiIhGnoBcRiTgFvYhIxCnoRUQiLrQfBzezKeDFUD688nQDN8IuokLou1ij72KNvos173D34r9os06YK2Nf3OovmUeVmZ3VdxHQd7FG38UafRdrzOzsVv9GXTciIhGnoBcRibgwg/6xED+70ui7WKPvYo2+izX6LtZs+bsIbTBWRET2hrpuREQiTkEvIhJxoQS9mT1oZi+a2ctm9skwaqgEZnbYzJ4xsxfM7JKZfSzsmsJkZjEz+6mZPRl2LWEzs3Yze9zMfmZml83s3WHXFBYz+y/5/z4umtnXzSyavw6yCTP7spkNm9nFda91mtnTZvZS/rGj2HH2POjNLAb8T+B9wDHgP5jZsb2uo0IsAZ9w92PAvcBHavi7APgYcDnsIirEnwF/6+53Aiep0e/FzNLAR4FT7n4CiAEfCLeqPfWXwIMbXvsk8F13vwP4bv75LYVxRv8u4GV3f9Xdc8BfA4+EUEfo3P2quz+f354i+I85HW5V4TCzDPBrwBfDriVsZpYC/hXwJQB3z7l7NtSiwhUHGs0sDjQBQyHXs2fc/fvA2IaXHwG+kt/+CvDvih0njKBPA1fWPR+gRsNtPTM7CtwDPBtyKWH5U+D3gZWQ66gEbwFGgL/Id2V90cyawy4qDO4+CPw34HXgKjDh7n8fblWh2+/uV/Pb14D9xf5Ag7EVwMxagG8CH3f3ybDr2Wtm9jAw7O7PhV1LhYgDvwT8ubvfA8xQwuV5FOX7nx8haPwOAc1m9pvhVlU5PJgfX3SOfBhBPwgcXvc8k3+tJplZgiDkv+buT4RdT0juA06b2WsEXXn3m9lfhVtSqAaAAXcvXN09ThD8tehXgZ+7+4i7LwJPAL8cck1hu25mBwHyj8PF/iCMoP8JcIeZvcXMkgQDK2dCqCN0ZmYE/bCX3f1zYdcTFnf/lLtn3P0owf8fvufuNXvW5u7XgCtm9o78Sw8AL4RYUpheB+41s6b8fy8PUKMD0+ucAT6U3/4Q8K1if7Dnd6909yUz+13g7whG0L/s7pf2uo4KcR/wQeCCmZ3Lv/Zpd38qvJKkQvxn4Gv5k6FXgd8OuZ5QuPuzZvY48DzBLLWfUkO3QzCzrwPvAbrNbAD4A+CPgP9tZh8GfgH8+6LH0S0QRESiTYOxIiIRp6AXEYk4Bb2ISMQp6EVEIk5BLyIScQp6EZGIU9CLiETc/weoSi7pOMMC3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run k-shot to check how rapidly we are able to adapt to unseen tasks\n",
    "# starting w/ a single unseen task\n",
    "\n",
    "test_wave = SineWaveTask_multi()\n",
    "num_k_shots = 10\n",
    "\n",
    "# use model returned from earlier optimization\n",
    "inner_loop_optimizer = torch.optim.SGD(model.parameters(), lr = lr_task_specific)\n",
    "held_out_task_specific_loss, metaTrainLosses, _ = task_specific_train_and_eval(model, test_wave, inner_loop_optimizer, num_k_shots)\n",
    "\n",
    "plt.plot(metaTrainLosses)\n",
    "plt.xlim([0,num_k_shots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a737d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Reptile_multidimensional_sinewave.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
