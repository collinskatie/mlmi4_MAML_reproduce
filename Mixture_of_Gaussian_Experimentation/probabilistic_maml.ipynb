{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308b967c-1101-4ba5-aa0e-501aec71f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n",
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Baseline second-order MAML with uncertainty prediction\n",
    "Based on Ocariz's code for uncertainty quantification w/ Reptile\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns \n",
    "from math import pi as PI\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "from higher import innerloop_ctx\n",
    "import warnings\n",
    "import importlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 0 # set seed for reproducibility\n",
    "\n",
    "#Set random seeds for reproducibility of results \n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# set GPU or CPU depending on available hardware\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Available device: {device}\")\n",
    "\n",
    "if device == \"cuda:0\": \n",
    "  # set default so all tensors are on GPU, if available\n",
    "  # help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# import backbone model, dataset, and code utils\n",
    "from models import Neural_Network, Prob_Neural_Network\n",
    "from constants import *\n",
    "from utils import *\n",
    "import analysis_utils\n",
    "from data import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f622897",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dataset\n",
    "'''\n",
    "# specify the number of tasks to sample per meta-set\n",
    "# note: we end up sampling tasks at random, so sizes are not particularly relevant\n",
    "# artifact of the way we structured the dataset earlier \n",
    "meta_train_size=10000\n",
    "meta_val_size=1000\n",
    "meta_test_size=1000\n",
    "meta_train_eval_size = 20\n",
    "\n",
    "dataset = RegressionDomain(amp_min=amp_min, amp_max=amp_max, \n",
    "                           phase_min=phase_min, phase_max=phase_max, \n",
    "                           train_size=meta_train_size, val_size=meta_val_size, test_size=meta_test_size)\n",
    "\n",
    "meta_val_set = dataset.get_meta_val_batch()\n",
    "meta_test_set = dataset.get_meta_test_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0b0448-3fb2-4b40-b649-9939da4f78ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter =  0  Current Loss 6.258881092071533  Val Loss:  nan\n",
      "Iter =  2  Current Loss nan  Val Loss:  nan\n",
      "Iter =  4  Current Loss nan  Val Loss:  nan\n",
      "Iter =  6  Current Loss nan  Val Loss:  nan\n",
      "Iter =  8  Current Loss nan  Val Loss:  nan\n",
      "Iter =  10  Current Loss nan  Val Loss:  nan\n",
      "Iter =  12  Current Loss nan  Val Loss:  nan\n",
      "Iter =  14  Current Loss nan  Val Loss:  nan\n",
      "Iter =  16  Current Loss nan  Val Loss:  nan\n",
      "Iter =  18  Current Loss nan  Val Loss:  nan\n",
      "Iter =  20  Current Loss nan  Val Loss:  nan\n",
      "Iter =  22  Current Loss nan  Val Loss:  nan\n",
      "Iter =  24  Current Loss nan  Val Loss:  nan\n",
      "Iter =  26  Current Loss nan  Val Loss:  nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-934795249fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mmeta_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mwaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_meta_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#Loop through all of the tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlmi4_MAML_reproduce/data.py\u001b[0m in \u001b[0;36mget_meta_train_batch\u001b[0;34m(self, task_batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_meta_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_of_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_meta_val_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlmi4_MAML_reproduce/data.py\u001b[0m in \u001b[0;36mget_batch_of_tasks\u001b[0;34m(self, task_type, task_batch_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# note: we could investigate impact of weighted sub-sampling in batch (?)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m# see documentation: https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mtask_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Handling computation graphs and second-order backprop help and partial inspiration from: \n",
    "- https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2 \n",
    "- https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853/3 \n",
    "- https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "- https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "- https://discuss.pytorch.org/t/how-to-manually-update-network-parameters-while-keeping-track-of-its-computational-graph/131642/2\n",
    "- https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "- https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "- https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "\n",
    "Neural network configuration and helper class functions copied directly from \n",
    "-https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "Note, different ways to refer to the task-specific vs. meta/aggregate updates to the parameters\n",
    "Sometimes called \"inner\" and \"outer\" loop, respectively\n",
    "Here, refered to as \"task_specific\" and \"agg\"/meta\" (the latter, for consistency w/ ocariz code)\n",
    "'''\n",
    "\n",
    "re_run = True\n",
    "\n",
    "criterion = loss_gaussian # custom loss for prob modeling, partly based on https://towardsdatascience.com/predicting-probability-distributions-using-neural-networks-abef7db10eac\n",
    "\n",
    "printing_step = 2\n",
    "\n",
    "if re_run: \n",
    "\n",
    "    #Instantiate the model network\n",
    "    model = Prob_Neural_Network()\n",
    "    # move to the current device (GPU or CPU)\n",
    "    # help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "    model.to(device)\n",
    "\n",
    "    N = 1 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "    K = 10 # number of samples to draw from the task\n",
    "\n",
    "    #Used to store the validation losses\n",
    "    metaLosses = []\n",
    "    metaValLosses = []\n",
    "\n",
    "    #Meta-optimizer for the outer loop\n",
    "    meta_optimizer = torch.optim.Adam(model.parameters(), lr = lr_meta)\n",
    "\n",
    "    #Inner optimizer, we were doing this by hand previously\n",
    "    inner_loop_optimizer = torch.optim.SGD(model.parameters(), lr = lr_task_specific)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # store loss over all tasks to then do a large meta-level update of initial params\n",
    "        # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "        meta_loss = None\n",
    "\n",
    "        waves = dataset.get_meta_train_batch(task_batch_size=T)\n",
    "\n",
    "        #Loop through all of the tasks\n",
    "        for i, T_i in enumerate(waves): \n",
    "            train_eval_info = task_specific_train_and_eval(model, T_i, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "            held_out_task_specific_loss = train_eval_info[0]\n",
    "            if meta_loss is None: \n",
    "                meta_loss = held_out_task_specific_loss\n",
    "            else:\n",
    "                meta_loss += held_out_task_specific_loss\n",
    "\n",
    "        meta_optimizer.zero_grad()\n",
    "        meta_loss /= T\n",
    "        meta_loss.backward()\n",
    "        meta_optimizer.step()\n",
    "        metaLosses.append(meta_loss.item())\n",
    "\n",
    "        # validation \n",
    "        val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "        val_train_eval_info = task_specific_train_and_eval(model, val_wave, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "        val_loss = val_train_eval_info[0]\n",
    "        metaValLosses.append(val_loss.item())\n",
    "\n",
    "        if epoch % printing_step == 0:\n",
    "            print(\"Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "            # saving model help from: \n",
    "            # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "            torch.save(model.state_dict(), \"prob_maml.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36137ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-10\n",
    "std = 1000\n",
    "a = 1/(np.sqrt(2*math.pi*std**2)+epsilon )\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"baseline_maml.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_run is False: \n",
    "    # help from: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    model = Prob_Neural_Network()\n",
    "    model.load_state_dict(torch.load(\"prob_maml.pt\"))\n",
    "    criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(analysis_utils)\n",
    "\n",
    "file_tag = \"prob_maml\"\n",
    "plot_lims = [-5.0, 5.0]\n",
    "analysis_utils.compare_K_shot(model, dataset, criterion, K_vals = [5,10], num_k_shots=10, seed=7,\n",
    "                              file_tag=file_tag, title=\"MAML K-Shot Learning Comparison\", plot_lims=plot_lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804e161-9383-419e-8948-11fd829167fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(analysis_utils)\n",
    "\n",
    "num_k_shots = 10\n",
    "K = 10\n",
    "num_eval=1000\n",
    "\n",
    "res = analysis_utils.k_shot_evaluation(model, dataset, criterion, num_k_shots=num_k_shots, K=K, num_eval=num_eval,\n",
    "                        file_tag=file_tag, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Draw ood samples\n",
    "'''\n",
    "importlib.reload(analysis_utils)\n",
    "wave = dataset.get_meta_test_batch(task_batch_size=1)[0]\n",
    "ood_range = [-10, 10] # includes some in-dist and ood\n",
    "\n",
    "file_tag = \"prob_ood_x_maml\"\n",
    "plot_lims = [-5.0, 5.0]\n",
    "analysis_utils.compare_K_shot(model, dataset, criterion, K_vals = [10,20], num_k_shots=11, seed=7,\n",
    "                              file_tag=file_tag, title=\"Evaluation Outside of Training Input Range\", plot_lims=plot_lims,\n",
    "                             input_range=ood_range, legend_locs=[\"lower right\", \"upper right\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
