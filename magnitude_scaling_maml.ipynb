{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308b967c-1101-4ba5-aa0e-501aec71f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n",
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Learn to scale magnitude of output\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns \n",
    "from math import pi as PI\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "from higher import innerloop_ctx\n",
    "import warnings\n",
    "import importlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Set random seeds for reproducibility of results \n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# set GPU or CPU depending on available hardware\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Available device: {device}\")\n",
    "\n",
    "if device == \"cuda:0\": \n",
    "  # set default so all tensors are on GPU, if available\n",
    "  # help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# import backbone model, dataset, and code utils\n",
    "from models import Neural_Network, Neural_Network_Magnitude_Scaling\n",
    "from constants import *\n",
    "from utils import *\n",
    "import analysis_utils\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f622897",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dataset\n",
    "'''\n",
    "# specify the number of tasks to sample per meta-set\n",
    "# note: we end up sampling tasks at random, so sizes are not particularly relevant\n",
    "# artifact of the way we structured the dataset earlier \n",
    "meta_train_size=10000\n",
    "meta_val_size=1000\n",
    "meta_test_size=1000\n",
    "meta_train_eval_size = 20\n",
    "\n",
    "dataset = RegressionDomain(amp_min=amp_min, amp_max=amp_max, \n",
    "                           phase_min=phase_min, phase_max=phase_max, \n",
    "                           train_size=meta_train_size, val_size=meta_val_size, test_size=meta_test_size)\n",
    "\n",
    "meta_val_set = dataset.get_meta_val_batch()\n",
    "meta_test_set = dataset.get_meta_test_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0b0448-3fb2-4b40-b649-9939da4f78ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter =  0  Current Loss 3.7132179737091064  Val Loss:  10.136510848999023\n",
      "Iter =  500  Current Loss 2.9180158061419657  Val Loss:  2.6600197761685784\n",
      "Iter =  1000  Current Loss 1.9754434523287114  Val Loss:  1.7960099319692004\n",
      "Iter =  1500  Current Loss 1.5122412971899082  Val Loss:  1.3778507288279809\n",
      "Iter =  2000  Current Loss 1.2539600576909407  Val Loss:  1.13836636092269\n",
      "Iter =  2500  Current Loss 1.0876468207801833  Val Loss:  0.9934477118453326\n",
      "Iter =  3000  Current Loss 0.9687081815371391  Val Loss:  0.8865008770122159\n",
      "Iter =  3500  Current Loss 0.880826129909244  Val Loss:  0.8004242114376775\n",
      "Iter =  4000  Current Loss 0.8130095371400615  Val Loss:  0.7420658173139764\n",
      "Iter =  4500  Current Loss 0.7600970435635139  Val Loss:  0.6928776358737917\n",
      "Iter =  5000  Current Loss 0.7171234872949717  Val Loss:  0.6560697685465775\n",
      "Iter =  5500  Current Loss 0.6804481701262755  Val Loss:  0.6219634838152807\n",
      "Iter =  6000  Current Loss 0.6487868744752702  Val Loss:  0.5947389818685758\n",
      "Iter =  6500  Current Loss 0.6211449580318358  Val Loss:  0.5684904449547293\n",
      "Iter =  7000  Current Loss 0.5958350787178446  Val Loss:  0.5460071246801648\n",
      "Iter =  7500  Current Loss 0.5728209454726655  Val Loss:  0.5242776163987363\n",
      "Iter =  8000  Current Loss 0.5516378696825843  Val Loss:  0.5059969912314493\n",
      "Iter =  8500  Current Loss 0.5322509806530011  Val Loss:  0.49006051311883236\n",
      "Iter =  9000  Current Loss 0.5147812017721277  Val Loss:  0.47370793956416546\n",
      "Iter =  9500  Current Loss 0.49886028736879945  Val Loss:  0.4580098162193094\n",
      "Iter =  10000  Current Loss 0.4839718987370548  Val Loss:  0.4445752899272636\n",
      "Iter =  10500  Current Loss 0.4700886774139454  Val Loss:  0.43219066256805\n",
      "Iter =  11000  Current Loss 0.4568728101723802  Val Loss:  0.41994472933381505\n",
      "Iter =  11500  Current Loss 0.44469393412639663  Val Loss:  0.40797302492857346\n",
      "Iter =  12000  Current Loss 0.4334451481746506  Val Loss:  0.39758849710343785\n",
      "Iter =  12500  Current Loss 0.4228139776969235  Val Loss:  0.38807462610176735\n",
      "Iter =  13000  Current Loss 0.41279347779703646  Val Loss:  0.3795757827576232\n",
      "Iter =  13500  Current Loss 0.4034022779753253  Val Loss:  0.37100552277232585\n",
      "Iter =  14000  Current Loss 0.3945506694335851  Val Loss:  0.36212651550510183\n",
      "Iter =  14500  Current Loss 0.38634494319583834  Val Loss:  0.3543269784617272\n",
      "Iter =  15000  Current Loss 0.3785831496724446  Val Loss:  0.3472672486955514\n",
      "Iter =  15500  Current Loss 0.3710422242332713  Val Loss:  0.3406425633138232\n",
      "Iter =  16000  Current Loss 0.36414658647636144  Val Loss:  0.3344684109230153\n",
      "Iter =  16500  Current Loss 0.35756660330652085  Val Loss:  0.3287410568231095\n",
      "Iter =  17000  Current Loss 0.3512535927991756  Val Loss:  0.3231689288566872\n",
      "Iter =  17500  Current Loss 0.34534475617917476  Val Loss:  0.3172579717149504\n",
      "Iter =  18000  Current Loss 0.33981379975393167  Val Loss:  0.3122178469387668\n",
      "Iter =  18500  Current Loss 0.334496472756767  Val Loss:  0.30767409741688806\n",
      "Iter =  19000  Current Loss 0.3293360145677768  Val Loss:  0.3027673787500179\n",
      "Iter =  19500  Current Loss 0.32447418380576637  Val Loss:  0.297633467680388\n",
      "Iter =  20000  Current Loss 0.3198936635878949  Val Loss:  0.29334073786556003\n",
      "Iter =  20500  Current Loss 0.31549068861202834  Val Loss:  0.289301499320759\n",
      "Iter =  21000  Current Loss 0.3112607976403709  Val Loss:  0.2855179640832395\n",
      "Iter =  21500  Current Loss 0.30721565493292896  Val Loss:  0.2813434320159511\n",
      "Iter =  22000  Current Loss 0.30328887658602033  Val Loss:  0.2773711894837887\n",
      "Iter =  22500  Current Loss 0.29960875812957516  Val Loss:  0.2739455623774345\n",
      "Iter =  23000  Current Loss 0.2959913453482803  Val Loss:  0.27077273033845345\n",
      "Iter =  23500  Current Loss 0.2924615290613701  Val Loss:  0.26747169103250906\n",
      "Iter =  24000  Current Loss 0.28911079189829253  Val Loss:  0.2642245877200179\n",
      "Iter =  24500  Current Loss 0.28586998085439397  Val Loss:  0.2612046409569902\n",
      "Iter =  25000  Current Loss 0.2828629983344758  Val Loss:  0.25841133090097795\n",
      "Iter =  25500  Current Loss 0.2799367079298221  Val Loss:  0.2558542062052572\n",
      "Iter =  26000  Current Loss 0.2770987832028668  Val Loss:  0.2533900160448657\n",
      "Iter =  26500  Current Loss 0.27430776682995495  Val Loss:  0.25056692047017504\n",
      "Iter =  27000  Current Loss 0.2716328026387094  Val Loss:  0.2481493586367525\n",
      "Iter =  27500  Current Loss 0.2690890940672064  Val Loss:  0.24588562133873385\n",
      "Iter =  28000  Current Loss 0.2666712631678833  Val Loss:  0.2435235529808759\n",
      "Iter =  28500  Current Loss 0.2642047216117221  Val Loss:  0.24108912406424282\n",
      "Iter =  29000  Current Loss 0.26189307225183767  Val Loss:  0.23892339478115934\n",
      "Iter =  29500  Current Loss 0.25959885870627253  Val Loss:  0.2367933459168239\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Handling computation graphs and second-order backprop help and partial inspiration from: \n",
    "- https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2 \n",
    "- https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853/3 \n",
    "- https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "- https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "- https://discuss.pytorch.org/t/how-to-manually-update-network-parameters-while-keeping-track-of-its-computational-graph/131642/2\n",
    "- https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "- https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "- https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "\n",
    "Neural network configuration and helper class functions copied directly from \n",
    "-https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "Note, different ways to refer to the task-specific vs. meta/aggregate updates to the parameters\n",
    "Sometimes called \"inner\" and \"outer\" loop, respectively\n",
    "Here, refered to as \"task_specific\" and \"agg\"/meta\" (the latter, for consistency w/ ocariz code)\n",
    "'''\n",
    "\n",
    "\n",
    "#Instantiate the model network\n",
    "model = Neural_Network_Magnitude_Scaling()\n",
    "# move to the current device (GPU or CPU)\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "model.to(device)\n",
    "\n",
    "N = 3 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "K = 10 # number of samples to draw from the task\n",
    "\n",
    "#Used to store the validation losses\n",
    "metaLosses = []\n",
    "metaValLosses = []\n",
    "\n",
    "#Meta-optimizer for the outer loop\n",
    "meta_optimizer = torch.optim.Adam(model.parameters(), lr = lr_meta)\n",
    "\n",
    "#Inner optimizer, we were doing this by hand previously\n",
    "inner_loop_optimizer = torch.optim.SGD(model.parameters(), lr = lr_task_specific)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(num_epochs):\n",
    "    # store loss over all tasks to then do a large meta-level update of initial params\n",
    "    # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "    meta_loss = None\n",
    "    \n",
    "    waves = dataset.get_meta_train_batch(task_batch_size=T)\n",
    "    \n",
    "    #Loop through all of the tasks\n",
    "    for i, T_i in enumerate(waves): \n",
    "        train_eval_info = task_specific_train_and_eval(model, T_i, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "        held_out_task_specific_loss = train_eval_info[0]\n",
    "        if meta_loss is None: \n",
    "            meta_loss = held_out_task_specific_loss\n",
    "        else:\n",
    "            meta_loss += held_out_task_specific_loss\n",
    "            \n",
    "    meta_optimizer.zero_grad()\n",
    "    meta_loss /= T\n",
    "    meta_loss.backward()\n",
    "    meta_optimizer.step()\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    # validation \n",
    "    val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "    val_train_eval_info = task_specific_train_and_eval(model, val_wave, inner_loop_optimizer, criterion, K=K, N=N)\n",
    "    val_loss = val_train_eval_info[0]\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step == 0:\n",
    "        print(\"Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), \"sample_maml_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b804e161-9383-419e-8948-11fd829167fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "task_specific_train_and_eval() got an unexpected keyword argument 'input_range'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2ff55cfb4553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"magnitude_scaling_maml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m res = analysis_utils.k_shot_evaluation(model, dataset, criterion, num_k_shots=num_k_shots, K=K, num_eval=num_eval,\n\u001b[0m\u001b[1;32m      8\u001b[0m                         file_tag=file_tag)\n",
      "\u001b[0;32m~/mlmi4_MAML_reproduce/analysis_utils.py\u001b[0m in \u001b[0;36mk_shot_evaluation\u001b[0;34m(model, dataset, criterion, num_k_shots, K, num_eval, file_tag, seed, analysis_steps, input_range)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# use model returned from earlier optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0minner_loop_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_task_specific\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mheld_out_task_specific_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetaTrainLosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_specific_train_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_wave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_loop_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_k_shots\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0minput_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaTrainLosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: task_specific_train_and_eval() got an unexpected keyword argument 'input_range'"
     ]
    }
   ],
   "source": [
    "importlib.reload(analysis_utils)\n",
    "\n",
    "num_k_shots = 10\n",
    "K = 10\n",
    "num_eval=100\n",
    "file_tag = \"magnitude_scaling_maml\"\n",
    "res = analysis_utils.k_shot_evaluation(model, dataset, criterion, num_k_shots=num_k_shots, K=K, num_eval=num_eval,\n",
    "                        file_tag=file_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81614284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
