{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UrjQGgr5nUHC",
   "metadata": {
    "id": "UrjQGgr5nUHC"
   },
   "source": [
    "<h1> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eGl9mcc0nOMP",
   "metadata": {
    "id": "eGl9mcc0nOMP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T3KVOwFXFOY0",
   "metadata": {
    "id": "T3KVOwFXFOY0"
   },
   "source": [
    "<h1> Data Loading and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nMUUm70ufKHH",
   "metadata": {
    "id": "nMUUm70ufKHH"
   },
   "source": [
    "This Sine function generator is based on the repostory: https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69513696-5625-410e-9cd6-8c5490d99029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class and everything within this cell is copied directly from:\n",
    "#https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "class ModifiableModule(nn.Module):\n",
    "    def params(self):\n",
    "        return [p for _, p in self.named_params()]\n",
    "    \n",
    "    def named_leaves(self):\n",
    "        return []\n",
    "    \n",
    "    def named_submodules(self):\n",
    "        return []\n",
    "    \n",
    "    def named_params(self):\n",
    "        subparams = []\n",
    "        for name, mod in self.named_submodules():\n",
    "            for subname, param in mod.named_params():\n",
    "                subparams.append((name + '.' + subname, param))\n",
    "        return self.named_leaves() + subparams\n",
    "    \n",
    "    def set_param(self, name, param):\n",
    "        if '.' in name:\n",
    "            n = name.split('.')\n",
    "            module_name = n[0]\n",
    "            rest = '.'.join(n[1:])\n",
    "            for name, mod in self.named_submodules():\n",
    "                if module_name == name:\n",
    "                    mod.set_param(rest, param)\n",
    "                    break\n",
    "        else:\n",
    "            setattr(self, name, param)\n",
    "            \n",
    "    def copy(self, other, same_var=False):\n",
    "        for name, param in other.named_params():\n",
    "            if not same_var:\n",
    "                param = V(param.data.clone(), requires_grad=True)\n",
    "            self.set_param(name, param)\n",
    "\n",
    "class GradLinear(ModifiableModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        ignore = nn.Linear(*args, **kwargs)\n",
    "        self.weights = V(ignore.weight.data, requires_grad=True)\n",
    "        self.bias = V(ignore.bias.data, requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weights, self.bias)\n",
    "    \n",
    "    def named_leaves(self):\n",
    "        return [('weights', self.weights), ('bias', self.bias)]\n",
    "\n",
    "class SineModel(ModifiableModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = GradLinear(1, 40)\n",
    "        self.hidden2 = GradLinear(40, 40)\n",
    "        self.out = GradLinear(40, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "    def named_submodules(self):\n",
    "        return [('hidden1', self.hidden1), ('hidden2', self.hidden2), ('out', self.out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3X51uGHDvSV",
   "metadata": {
    "id": "a3X51uGHDvSV"
   },
   "outputs": [],
   "source": [
    "class SineWaveTask:\n",
    "    def __init__(self):\n",
    "        self.a = np.random.uniform(0.1, 5.0)\n",
    "        self.b = np.random.uniform(0, 2*np.pi)\n",
    "        self.train_x = None\n",
    "        \n",
    "    def f(self, x):\n",
    "        return self.a * np.sin(x + self.b)\n",
    "        \n",
    "    def training_set(self, size=10, force_new=False):\n",
    "        if self.train_x is None and not force_new:\n",
    "            self.train_x = np.random.uniform(-5, 5, size)\n",
    "            x = self.train_x\n",
    "        elif not force_new:\n",
    "            x = self.train_x\n",
    "        else:\n",
    "            x = np.random.uniform(-5, 5, size)\n",
    "        y = self.f(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "    \n",
    "    def test_set(self, size=50):\n",
    "        x = np.linspace(-5, 5, size)\n",
    "        y = self.f(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "    \n",
    "    def plot(self, *args, **kwargs):\n",
    "        x, y = self.test_set(size=100)\n",
    "        return plt.plot(x.numpy(), y.numpy(), *args, **kwargs)\n",
    "    \n",
    "    def plot_model(self, new_model, *args, **kwargs):\n",
    "        x, y_true = self.test_set(size=100)\n",
    "        x = Variable(x[:, None])\n",
    "        y_true = Variable(y_true[:, None])    \n",
    "\n",
    "        y_pred = new_model(x)\n",
    "\n",
    "        plt.plot(x.data.numpy().flatten(),\n",
    "                 y_pred.data.numpy().flatten(),\n",
    "                 *args, **kwargs)\n",
    "\n",
    "TRAIN_SIZE = 20000\n",
    "TEST_SIZE = 1000\n",
    "SINE_TRAIN = [SineWaveTask() for _ in range(TRAIN_SIZE)]\n",
    "SINE_TEST = [SineWaveTask() for _ in range(TEST_SIZE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cu4urLF7Q88A",
   "metadata": {
    "id": "cu4urLF7Q88A"
   },
   "source": [
    "<h1> Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G-ExWACxQ3mt",
   "metadata": {
    "id": "G-ExWACxQ3mt"
   },
   "source": [
    "<h1> Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1zyNHFXdOnug",
   "metadata": {
    "id": "1zyNHFXdOnug"
   },
   "outputs": [],
   "source": [
    "# The Minimum Square Error is used to evaluate the difference between prediction and ground truth\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def copy_existing_model(model):\n",
    "    # Function to copy an existing model\n",
    "    # We initialize a new model\n",
    "    new_model = Neural_Network()\n",
    "    # Copy the previous model's parameters into the new model\n",
    "    new_model.load_state_dict(model.state_dict())\n",
    "    return new_model\n",
    "\n",
    "def get_samples_in_good_format(wave, num_samples=10, force_new=False):\n",
    "  #This function is used to sample data from a wave\n",
    "  x, y_true = wave.training_set(size=num_samples, force_new=force_new)\n",
    "  # We add [:,None] to get the right dimensions to pass to the model: we want K x 1 (we have scalars inputs hence the x 1)\n",
    "  # Note that we convert everything torch tensors\n",
    "  x = torch.tensor(x[:,None])\n",
    "  y_true = torch.tensor(y_true[:,None])\n",
    "  return x,y_true\n",
    "\n",
    "def initialization_to_store_meta_losses():\n",
    "  # This function creates lists to store the meta losses\n",
    "  global store_train_loss_meta; store_train_loss_meta = []\n",
    "  global store_test_loss_meta; store_test_loss_meta = []\n",
    "\n",
    "def test_set_validation(model,new_model,wave,lr_inner,k,store_test_loss_meta):\n",
    "    # This functions does not actually affect the main algorithm, it is just used to evaluate the new model\n",
    "    new_model = training(model, wave, lr_inner, k)\n",
    "    # Obtain the loss\n",
    "    loss = evaluation(new_model, wave)\n",
    "    # Store loss\n",
    "    store_test_loss_meta.append(loss)\n",
    "\n",
    "def train_set_evaluation(new_model,wave,store_train_loss_meta):\n",
    "    loss = evaluation(new_model, wave)\n",
    "    store_train_loss_meta.append(loss) \n",
    "\n",
    "def print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step=1000):\n",
    "  if epoch % printing_step == 0:\n",
    "    print(f'Epochh : {epoch}, Average Train Meta Loss : {np.mean(store_train_loss_meta)}, Average Test Meta Loss : {np.mean(store_test_loss_meta)}')\n",
    "\n",
    "#This is based on the paper update rule, we calculate the difference between parameters and then this is used by the optimizer, rather than doing the update by hand\n",
    "def reptile_parameter_update(model,new_model):\n",
    "  # Zip models for the loop\n",
    "  zip_models = zip(model.parameters(), new_model.parameters())\n",
    "  for parameter, new_parameter in zip_models:\n",
    "    if parameter.grad is None:\n",
    "      parameter.grad = torch.tensor(torch.zeros_like(parameter))\n",
    "    # Here we are adding the gradient that will later be used by the optimizer\n",
    "    parameter.grad.data.add_(parameter.data - new_parameter.data)\n",
    "\n",
    "# Define commands in order needed for the metaupdate\n",
    "# Note that if we change the order it doesn't behave the same\n",
    "def metaoptimizer_update(metaoptimizer):\n",
    "  # Take step\n",
    "  metaoptimizer.step()\n",
    "  # Reset gradients\n",
    "  metaoptimizer.zero_grad()\n",
    "\n",
    "def metaupdate(model,new_model,metaoptimizer):\n",
    "  # Combine the two previous functions into a single metaupdate function\n",
    "  # First we calculate the gradients\n",
    "  reptile_parameter_update(model,new_model)\n",
    "  # Use those gradients in the optimizer\n",
    "  metaoptimizer_update(metaoptimizer)\n",
    "\n",
    "def evaluation(new_model, wave, item = True, num_samples=10, force_new=False):\n",
    "    # Get data\n",
    "    x, label = get_samples_in_good_format(wave,num_samples=num_samples, force_new=force_new)\n",
    "    # Make model prediction\n",
    "    prediction = new_model(x)\n",
    "    # Get loss\n",
    "    if item == True: #Depending on whether we need to return the loss value for storing or for backprop\n",
    "      loss = criterion(prediction,label).item()\n",
    "    else:\n",
    "      loss = criterion(prediction,label)\n",
    "    return loss\n",
    "\n",
    "def training(model, wave, lr_k, k):\n",
    "    # Create new model which we will train on\n",
    "    new_model = copy_existing_model(model)\n",
    "    # Define new optimizer\n",
    "    koptimizer = torch.optim.SGD(new_model.parameters(), lr=lr_k)\n",
    "    # Update the model multiple times, note that k>1 (do not confuse k with K)\n",
    "    for i in range(k):\n",
    "        # Reset optimizer\n",
    "        koptimizer.zero_grad()\n",
    "        # Evaluate the model\n",
    "        loss = evaluation(new_model, wave, item = False)\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        koptimizer.step()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3eb594",
   "metadata": {
    "id": "ac3eb594"
   },
   "source": [
    "# Second-Order MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5df8816",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5df8816",
    "outputId": "9f93857a-c1d2-4af3-dd3e-03ce5e80a8eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8151],\n",
      "        [ 0.9552],\n",
      "        [-0.2384],\n",
      "        [-0.1510],\n",
      "        [ 0.0550]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8150],\n",
      "        [ 0.9560],\n",
      "        [-0.2375],\n",
      "        [-0.1501],\n",
      "        [ 0.0555]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8148],\n",
      "        [ 0.9569],\n",
      "        [-0.2371],\n",
      "        [-0.1491],\n",
      "        [ 0.0555]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8149],\n",
      "        [ 0.9578],\n",
      "        [-0.2366],\n",
      "        [-0.1481],\n",
      "        [ 0.0552]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8154],\n",
      "        [ 0.9586],\n",
      "        [-0.2360],\n",
      "        [-0.1472],\n",
      "        [ 0.0548]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8159],\n",
      "        [ 0.9592],\n",
      "        [-0.2353],\n",
      "        [-0.1464],\n",
      "        [ 0.0544]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8162],\n",
      "        [ 0.9599],\n",
      "        [-0.2348],\n",
      "        [-0.1456],\n",
      "        [ 0.0540]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8163],\n",
      "        [ 0.9606],\n",
      "        [-0.2344],\n",
      "        [-0.1448],\n",
      "        [ 0.0538]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8166],\n",
      "        [ 0.9610],\n",
      "        [-0.2343],\n",
      "        [-0.1442],\n",
      "        [ 0.0538]], grad_fn=<SliceBackward>)\n",
      "tensor([[-0.8166],\n",
      "        [ 0.9615],\n",
      "        [-0.2345],\n",
      "        [-0.1435],\n",
      "        [ 0.0538]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/tmpg5kld2jv77847rflwv40m0000gn/T/ipykernel_43488/2063223670.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x[:,None])\n",
      "/var/folders/3x/tmpg5kld2jv77847rflwv40m0000gn/T/ipykernel_43488/2063223670.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true = torch.tensor(y_true[:,None])\n"
     ]
    }
   ],
   "source": [
    "#This uses their neural network configuration\n",
    "# maml training\n",
    "# note: uses comments and structure largely from code ocariz wrote! \n",
    "#alternative code\n",
    "'''\n",
    "Handling computation graphs and second-order backprop help and partial inspiration from: \n",
    "- https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2 \n",
    "- https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853/3 \n",
    "- https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "- https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "- https://discuss.pytorch.org/t/how-to-manually-update-network-parameters-while-keeping-track-of-its-computational-graph/131642/2\n",
    "- https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "- https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "\n",
    "Note, different ways to refer to the task-specific vs. meta/aggregate updates to the parameters\n",
    "Sometimes called \"inner\" and \"outer\" loop, respectively\n",
    "Here, refered to as \"task_specific\" and \"agg\"/meta\" (the latter, for consistency w/ ocariz code)\n",
    "'''\n",
    "\n",
    "\n",
    "T = 5 # num tasks\n",
    "N = 1 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "num_samples = 10 # number of samples to draw from the task\n",
    "lr_task_specific = 0.01 # task specific learning rate\n",
    "lr_meta = 0.001 # meta-update learning rate\n",
    "waves = random.sample(SINE_TRAIN, T)\n",
    "num_epochs = 10\n",
    "printing_step = 2 # show log of loss every x epochs\n",
    "# num_epochs = int(1e5) \n",
    "# printing_step = 1000 # show log of loss every x epochs\n",
    "\n",
    "# Initializations\n",
    "initialization_to_store_meta_losses()\n",
    "\n",
    "\n",
    "#Instantiate the other model, from the prior class\n",
    "model = SineModel()\n",
    "\n",
    "#Use the different syntax of model.params()\n",
    "meta_optimizer = torch.optim.Adam(model.params(), lr = lr_meta)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        \n",
    "    # store loss over all tasks to then do a large meta-level update of initial params\n",
    "    # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "    meta_loss = None \n",
    "    \n",
    "\n",
    "    # loop over tasks and fine-tune weights per task\n",
    "    for i, T_i in enumerate(waves): \n",
    "        # copy model to use the same starting weights\n",
    "        \n",
    "        #Use the different copying function capacity\n",
    "        new_model = SineModel()\n",
    "        new_model.copy(model, same_var=True)\n",
    "        \n",
    "        \n",
    "        # note, b/c of manual gradient updates, need to zero out the gradients at the start\n",
    "        # https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "        new_model.zero_grad()\n",
    "\n",
    "        # use model to predict on task-specific training set\n",
    "        task_specific_loss = evaluation(new_model, T_i, item = False, num_samples=num_samples, force_new=True)\n",
    "        # save the computation graph for later backprop\n",
    "        # help from: https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2\n",
    "        gradient_info = torch.autograd.grad(task_specific_loss, new_model.parameters(), \n",
    "                                        create_graph=True)\n",
    "        # now, need to extract gradients for each param and get a new graph \n",
    "        # help from: https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "        # and: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "        model_param_data = new_model.state_dict()\n",
    "        # note: order of computation is preserved and state_dict = ordered, so okay to loop\n",
    "        # https://github.com/HIPS/autograd/blob/master/docs/tutorial.md\n",
    "        for computation_idx, (param_name, param_obj) in enumerate(new_model.named_parameters()):\n",
    "            task_specific_grad = gradient_info[computation_idx]\n",
    "            model_param_data[param_name] = param_obj - lr_task_specific * task_specific_grad # manual update\n",
    "\n",
    "        # load the updated task-specific params (w/ gradient info!) into the model \n",
    "        new_model.load_state_dict(model_param_data)\n",
    "            \n",
    "        # use new model to predict \n",
    "        # note: we want a new sample from T_i\n",
    "        # e.g., sample new draws from task, feed forward (e.g., get preds), compute loss, sum loss to meta_loss for later gradient use\n",
    "        held_out_task_specific_loss = evaluation(new_model, T_i, item = False, num_samples=num_samples, force_new=True)\n",
    "        # save meta-loss per task help from: \n",
    "        # https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "        # https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "        if meta_loss is None: \n",
    "            meta_loss = held_out_task_specific_loss # b/c we can't add a tensor to None obj\n",
    "        else: meta_loss += held_out_task_specific_loss\n",
    "        \n",
    "\n",
    "    # backpropogate thru all tasks\n",
    "    # use adam optimizer here!! \n",
    "    meta_loss /= T # b/c we want the mean -- divide by the number of tasks (T)\n",
    "    meta_loss.backward() \n",
    "    metaoptimizer_update(meta_optimizer)\n",
    "    \n",
    "    print(model.params()[0][5:10])\n",
    "    \n",
    "    #print(\"Check updated? \", model._modules['hidden1']._parameters[\"weight\"][:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-4Ps8P2IRCmF",
   "metadata": {
    "id": "-4Ps8P2IRCmF"
   },
   "source": [
    "<h1> Reptile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQjoz6FYctJM",
   "metadata": {
    "id": "bQjoz6FYctJM"
   },
   "source": [
    "<h1> Few Shot learning with new meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m-SPUG5Bfpe9",
   "metadata": {
    "id": "m-SPUG5Bfpe9"
   },
   "source": [
    "The model performs good few shot learning"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experimenting_with_second_order.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
