{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308b967c-1101-4ba5-aa0e-501aec71f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns \n",
    "from math import pi as PI\n",
    "import random\n",
    "# !pip3 install higher\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "from higher import innerloop_ctx\n",
    "import warnings\n",
    "\n",
    "ax_size = 14\n",
    "title_size = 16\n",
    "\n",
    "#The code includes extensive warnings when run so have used this to ignore them\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Set random seeds for reproducibility of results \n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# set GPU or CPU depending on available hardware\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Available device: {device}\")\n",
    "\n",
    "if device == \"cuda:0\": \n",
    "  # set default so all tensors are on GPU, if available\n",
    "  # help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "  torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989f328f-489f-4c58-b6ea-a6ceaf3c7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Class that Ocariz wrote\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=40, output_size=1):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # network layers\n",
    "        self.hidden1 = nn.Linear(input_size,hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "        #Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        y = x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee89288c-fbc9-4f21-a03f-e05b6e9adbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a master data class to ensure analogous structure across tasks\n",
    "Help on writing abstract classes from: \n",
    "https://www.geeksforgeeks.org/abstract-classes-in-python/\n",
    "\n",
    "Meta-train has training and test sets\n",
    "Also have meta-val and meta-test sets (at the task level)\n",
    "Help for understanding training and test set structure from [see especially for classification]: \n",
    "https://meta-learning.fastforwardlabs.com/\n",
    "\n",
    "\n",
    "What do we want out of our data generation code? \n",
    "- specify a particular task instance\n",
    "(e.g., amplitude and phase; N number of discrete classes)\n",
    "- extract batches of tasks for train, val, and test\n",
    "- extract sample from a given task of size K for training (and other for testing)\n",
    "'''\n",
    "\n",
    "class Domain(): \n",
    "    \n",
    "    def get_meta_train_batch(self, task_batch_size):\n",
    "        # yields the set of meta-training tasks, each of which has train and test sets\n",
    "        pass \n",
    "    \n",
    "    def get_meta_val_batch(self, task_batch_size):\n",
    "        # yields meta-val tasks (each just has a single data set)\n",
    "        pass\n",
    "\n",
    "    def get_meta_test_batch(self, task_batch_size):\n",
    "        # yields meta-test tasks (each just has a single data set)\n",
    "        pass\n",
    "    \n",
    "'''\n",
    "Regression task, as per MAML Section 5.1 (https://arxiv.org/pdf/1703.03400.pdf)\n",
    "Specifically, sine wave generation \n",
    "\n",
    "Code inspired by and modified from: \n",
    "https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "Checked original MAML code to ensure resampling for all stages of sinusoid:\n",
    "https://github.com/cbfinn/maml/blob/master/data_generator.py\n",
    "'''\n",
    "\n",
    "class RegressionDomain(Domain): \n",
    "    \n",
    "    '''\n",
    "    Each task is a sine wave\n",
    "    Parameterized by amplitude and phase \n",
    "    Always drawn from w/in a specified range of x vals\n",
    "    [Values from Section 5.1 -- but we could vary??]\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, amp_min=0.1, amp_max=0.5, \n",
    "                 phase_min=0, phase_max=PI,\n",
    "                train_size=1000, val_size=100, test_size=1000): \n",
    "        \n",
    "        self.amp_min = amp_min\n",
    "        self.amp_max = amp_max\n",
    "        self.phase_min = phase_min\n",
    "        self.phase_max = phase_max\n",
    "        \n",
    "        # create initial train, val, and test \n",
    "        # parameters specify the number of unique functions we want\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        # looping to instantiate tasks idea from: https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "        # help on generating random numbers in range from: https://pynative.com/python-get-random-float-numbers/\n",
    "        self.tasks = {}\n",
    "        # note: code would be more structured b/w task type for classification\n",
    "        for task_type, num_tasks in zip([\"train\", \"val\", \"test\"], [train_size, val_size, test_size]):\n",
    "            tasks = [SineFunction(amplitude = random.uniform(self.amp_min, self.amp_max), \n",
    "                                     phase=random.uniform(self.phase_min, self.phase_max)) for _ in range(num_tasks)]\n",
    "            self.tasks[task_type] = tasks\n",
    "    \n",
    "    def get_batch_of_tasks(self, task_type, task_batch_size): \n",
    "        # helper function since same sampling per type for regression-specific domain\n",
    "        if task_batch_size is None: \n",
    "            # return all \n",
    "            return self.tasks[task_type]\n",
    "        else: \n",
    "            # sub-sample\n",
    "            # note: we could investigate impact of weighted sub-sampling in batch (?)\n",
    "            # see documentation: https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "            task_batch = np.random.choice(self.tasks[task_type], size=task_batch_size, replace=False)\n",
    "            return task_batch\n",
    "    \n",
    "    def get_meta_train_batch(self, task_batch_size=10): \n",
    "        return self.get_batch_of_tasks(\"train\", task_batch_size)\n",
    "        \n",
    "    def get_meta_val_batch(self, task_batch_size=None): \n",
    "        return self.get_batch_of_tasks(\"val\", task_batch_size) \n",
    "        \n",
    "    def get_meta_test_batch(self, task_batch_size=None): \n",
    "        return self.get_batch_of_tasks(\"test\", task_batch_size)\n",
    "\n",
    "        \n",
    "class SineFunction(): \n",
    "    \n",
    "    def __init__(self, amplitude, phase): \n",
    "        self.amplitude = amplitude\n",
    "        self.phase = phase\n",
    "        \n",
    "    def draw_sample(self, x): \n",
    "        '''\n",
    "        Sample from the specified sine wave \n",
    "        '''\n",
    "        # help to sample from a sine function:\n",
    "        # https://stackoverflow.com/questions/48043004/how-do-i-generate-a-sine-wave-using-python\n",
    "        freq = 1 # TODO: check???\n",
    "        return self.amplitude * np.sin(freq * x + self.phase)\n",
    "    \n",
    "    def get_samples(self, num_samples=10, \n",
    "                    min_query_x=-5.0, max_query_x=5.0): \n",
    "        '''\n",
    "        Return samples drawn from this specific function (e.g., K for training set in meta-train)\n",
    "        Note, input range uses values from paper (Section 5.1)\n",
    "        But modification allowed thru function so we can test generalization beyond??\n",
    "        '''\n",
    "        x_vals = [random.uniform(min_query_x, max_query_x) for _ in range(num_samples)]\n",
    "        y_vals = [self.draw_sample(x) for x in x_vals]\n",
    "        # conversion to tensor idea and code help from: https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "        return {\"input\": torch.Tensor(x_vals), \"output\": torch.Tensor(y_vals)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d744439-ef29-4cdc-b3f4-8e0e7bb4938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "def get_samples_in_good_format(wave, num_samples=10):\n",
    "    #This function is used to sample data from a wave\n",
    "    sample_data = wave.get_samples(num_samples=num_samples)\n",
    "    x = sample_data[\"input\"]\n",
    "    y_true = sample_data[\"output\"]\n",
    "    # We add [:,None] to get the right dimensions to pass to the model: we want K x 1 (we have scalars inputs hence the x 1)\n",
    "    # Note that we convert everything torch tensors\n",
    "    x = torch.tensor(x[:,None])\n",
    "    y_true = torch.tensor(y_true[:,None])\n",
    "    return x,y_true\n",
    "\n",
    "\n",
    "def copy_existing_model(model):\n",
    "    # Function to copy an existing model\n",
    "    # We initialize a new model\n",
    "    new_model = Neural_Network()\n",
    "    # Copy the previous model's parameters into the new model\n",
    "    new_model.load_state_dict(model.state_dict())\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def task_specific_train_and_eval(model, T_i, inner_loop_optimizer, N=1):\n",
    "    #Description of the loop formulation from https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "    with innerloop_ctx(model, inner_loop_optimizer, copy_initial_weights = False) as (fmodel,diffopt):\n",
    "        #get our input data and our label\n",
    "        x, label = get_samples_in_good_format(T_i,num_samples=num_samples)\n",
    "        per_step_loss = []\n",
    "        for _ in range(N):\n",
    "            #Get the task specific loss for our model\n",
    "            task_specifc_loss = criterion(fmodel(x), label)\n",
    "\n",
    "            #Step through the inner gradient\n",
    "            diffopt.step(task_specifc_loss)\n",
    "            \n",
    "            per_step_loss.append(task_specifc_loss.item())\n",
    "            \n",
    "        held_out_task_specific_loss = evaluation(fmodel, T_i, num_samples=num_samples)\n",
    "        \n",
    "        return held_out_task_specific_loss, per_step_loss, fmodel\n",
    "    \n",
    "    \n",
    "def training_reptile(model, wave, lr_k, k):\n",
    "    # Create new model which we will train on\n",
    "    new_model = copy_existing_model(model)\n",
    "    # Define new optimizer\n",
    "    koptimizer = torch.optim.SGD(new_model.parameters(), lr=lr_k)\n",
    "    # Update the model multiple times, note that k>1 (do not confuse k with K)\n",
    "    for i in range(k):\n",
    "        # Reset optimizer\n",
    "        koptimizer.zero_grad()\n",
    "        # Evaluate the model\n",
    "        loss = evaluation(new_model, wave)\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        koptimizer.step()\n",
    "    return new_model, loss\n",
    "\n",
    "def metaupdate(model,new_model,metaoptimizer):\n",
    "  # Combine the two previous functions into a single metaupdate function\n",
    "  # First we calculate the gradients\n",
    "  reptile_parameter_update(model,new_model)\n",
    "  # Use those gradients in the optimizer\n",
    "  metaoptimizer_update(metaoptimizer)\n",
    "\n",
    "def test_set_validation(model,new_model,wave,lr_inner,k):\n",
    "    # This functions does not actually affect the main algorithm, it is just used to evaluate the new model\n",
    "    new_model, oldLoss = training_reptile(model, wave, lr_inner, k)\n",
    "    # Obtain the loss\n",
    "    loss = evaluation(new_model, wave)\n",
    "    # Store loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def evaluation(new_model, wave, num_samples=10):\n",
    "    # Get data\n",
    "    x, label = get_samples_in_good_format(wave,num_samples=num_samples)\n",
    "    # Make model prediction\n",
    "    prediction = new_model(x)\n",
    "    # Get loss\n",
    "    return criterion(prediction,label)\n",
    "\n",
    "def reptile_parameter_update(model,new_model):\n",
    "  # Zip models for the loop\n",
    "  zip_models = zip(model.parameters(), new_model.parameters())\n",
    "  for parameter, new_parameter in zip_models:\n",
    "    if parameter.grad is None:\n",
    "      parameter.grad = torch.tensor(torch.zeros_like(parameter))\n",
    "    # Here we are adding the gradient that will later be used by the optimizer\n",
    "    parameter.grad.data.add_(parameter.data - new_parameter.data)\n",
    "\n",
    "# Define commands in order needed for the metaupdate\n",
    "# Note that if we change the order it doesn't behave the same\n",
    "def metaoptimizer_update(metaoptimizer):\n",
    "  # Take step\n",
    "  metaoptimizer.step()\n",
    "  # Reset gradients\n",
    "  metaoptimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d0b0448-3fb2-4b40-b649-9939da4f78ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reptile Iter =  0  Current Loss 3.531139373779297  Val Loss:  7.74538516998291\n",
      "Reptile Iter =  500  Current Loss 3.196281589727529  Val Loss:  2.4160770304768744\n",
      "Reptile Iter =  1000  Current Loss 2.483566179916218  Val Loss:  2.028038592966316\n",
      "Reptile Iter =  1500  Current Loss 2.108958396277602  Val Loss:  1.7601067066120653\n",
      "Reptile Iter =  2000  Current Loss 1.904083261798801  Val Loss:  1.5591091765407314\n",
      "MAML Iter =  0  Current Loss 1.7437919445113554  Val Loss:  1.3901226970501934\n",
      "MAML Iter =  500  Current Loss 1.6122854833924998  Val Loss:  1.2906133746472273\n",
      "MAML Iter =  1000  Current Loss 1.4747722606601337  Val Loss:  1.1950395693161897\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3x/tmpg5kld2jv77847rflwv40m0000gn/T/ipykernel_12140/1809594776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mmeta_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mmeta_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mmeta_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mmeta_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mmetaLosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Sample of how to use the classes\n",
    "'''\n",
    "\n",
    "#Good code\n",
    "\n",
    "# using parameters from original MAML Section 5.1 (https://arxiv.org/pdf/1703.03400.pdf)\n",
    "amp_min=0.1\n",
    "amp_max=5.0\n",
    "phase_min=0\n",
    "phase_max=PI\n",
    "K = 10\n",
    "\n",
    "# todo: check parameters we want\n",
    "# specify the number of tasks to sample per meta-set\n",
    "meta_train_size=1000\n",
    "meta_val_size=100\n",
    "meta_test_size=1000\n",
    "meta_train_eval_size = 20\n",
    "\n",
    "task_batch_size = 10  \n",
    "\n",
    "dataset = RegressionDomain(amp_min=amp_min, amp_max=amp_max, \n",
    "                           phase_min=phase_min, phase_max=phase_max, \n",
    "                           train_size=meta_train_size, val_size=meta_val_size, test_size=meta_test_size)\n",
    "\n",
    "meta_val_set = dataset.get_meta_val_batch()\n",
    "meta_test_set = dataset.get_meta_test_batch()\n",
    "\n",
    "meta_train_sample = dataset.get_meta_train_batch(task_batch_size=task_batch_size)\n",
    "\n",
    "meta_train_sample[0].get_samples()\n",
    "\n",
    "\n",
    "'''\n",
    "Handling computation graphs and second-order backprop help and partial inspiration from: \n",
    "- https://discuss.pytorch.org/t/how-to-save-computation-graph-of-a-gradient/128286/2 \n",
    "- https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853/3 \n",
    "- https://lucainiaoge.github.io/download/PyTorch-create_graph-is-true_Tutorial_and_Example.pdf\n",
    "- https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "- https://discuss.pytorch.org/t/how-to-manually-update-network-parameters-while-keeping-track-of-its-computational-graph/131642/2\n",
    "- https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/3\n",
    "- https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\n",
    "- https://higher.readthedocs.io/en/latest/toplevel.html\n",
    "\n",
    "Neural network configuration and helper class functions copied directly from \n",
    "-https://github.com/AdrienLE/ANIML/blob/master/ANIML.ipynb\n",
    "\n",
    "Note, different ways to refer to the task-specific vs. meta/aggregate updates to the parameters\n",
    "Sometimes called \"inner\" and \"outer\" loop, respectively\n",
    "Here, refered to as \"task_specific\" and \"agg\"/meta\" (the latter, for consistency w/ ocariz code)\n",
    "'''\n",
    "\n",
    "\n",
    "#Instantiate the model network\n",
    "model = Neural_Network()\n",
    "# move to the current device (GPU or CPU)\n",
    "# help from: https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu\n",
    "model.to(device)\n",
    "\n",
    "T = 25 # num tasks\n",
    "N = 1 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "num_samples = 10 # number of samples to draw from the task\n",
    "lr_task_specific = 0.01 # task specific learning rate\n",
    "#lr_meta = 0.001 # meta-update learning rate\n",
    "lr_meta = 0.01 # meta-update learning rate\n",
    "num_epochs = 10000#70001 #Number of iterations for outer loop\n",
    "printing_step = 500 # show log of loss every x epochs\n",
    "min_lr_meta = .0001 #minimum learning rate for the meta optimizer \n",
    "\n",
    "#Used to store the validation losses\n",
    "metaLosses = []\n",
    "metaValLosses = []\n",
    "\n",
    "#Meta-optimizer for the outer loop\n",
    "meta_optimizer = torch.optim.Adam(model.parameters(), lr = lr_meta)\n",
    "    \n",
    "cosScheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=meta_optimizer, T_max=num_epochs,\n",
    "                   eta_min=0, verbose = False)\n",
    "\n",
    "#Inner optimizer, we were doing this by hand previously\n",
    "inner_loop_optimizer = torch.optim.SGD(model.parameters(), lr = lr_task_specific)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Sample of how to use the classes\n",
    "'''\n",
    "\n",
    "k = 5 # number of inner loop steps (notation from: https://www.bayeswatch.com/2018/11/30/HTYM/)\n",
    "lr_meta_reptile = 0.001 # meta-update learning rate\n",
    "lr_k_reptile = .01\n",
    "num_epochs_reptile = 2500 #70001 #Number of iterations for outer loop\n",
    "printing_step_reptile = 500 # show log of loss every x epochs\n",
    "meta_optimizer_reptile = torch.optim.Adam(model.parameters(), lr=lr_meta_reptile)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs_reptile):\n",
    "   \n",
    "    # store loss over all tasks to then do a large meta-level update of initial params\n",
    "    # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "    \n",
    "    #Sample a new wave each time\n",
    "    waves = dataset.get_meta_train_batch(task_batch_size=1)\n",
    "    \n",
    "    new_model, meta_loss = training_reptile(model, waves[0], lr_k_reptile, k)\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    metaupdate(model,new_model,meta_optimizer_reptile)\n",
    "    \n",
    "    val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "    \n",
    "    val_loss = test_set_validation(model,new_model,val_wave,lr_k_reptile,k)\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step_reptile == 0:\n",
    "        print(\"Reptile Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), \"maml_ca_model.pt\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cosScheduler.step(epoch=epoch)\n",
    "    # store loss over all tasks to then do a large meta-level update of initial params\n",
    "    # idea/help from video: https://www.youtube.com/watch?v=IkDw22a8BDE\n",
    "    meta_loss = None\n",
    "    \n",
    "    #Sample a new wave each time\n",
    "    waves = dataset.get_meta_train_batch(task_batch_size=T)\n",
    "    \n",
    "    #Loop through all of the tasks\n",
    "    for i, T_i in enumerate(waves): \n",
    "        held_out_task_specific_loss, _, _ = task_specific_train_and_eval(model, T_i, inner_loop_optimizer, N)\n",
    "        if meta_loss is None: \n",
    "            meta_loss = held_out_task_specific_loss\n",
    "        else:\n",
    "            meta_loss += held_out_task_specific_loss\n",
    "            \n",
    "    meta_optimizer.zero_grad()\n",
    "    meta_loss /= T\n",
    "    meta_loss.backward()\n",
    "    meta_optimizer.step()\n",
    "    metaLosses.append(meta_loss.item())\n",
    "    \n",
    "    # validation \n",
    "    val_wave = dataset.get_meta_val_batch(task_batch_size=1)[0]\n",
    "    val_loss, _, _ = task_specific_train_and_eval(model, val_wave, inner_loop_optimizer, N)\n",
    "    metaValLosses.append(val_loss.item())\n",
    "    \n",
    "    if epoch % printing_step == 0:\n",
    "        print(\"MAML Iter = \", epoch, \" Current Loss\", np.mean(metaLosses), \" Val Loss: \", np.mean(metaValLosses))\n",
    "        # saving model help from: \n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        torch.save(model.state_dict(), \"maml_ca_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804e161-9383-419e-8948-11fd829167fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
