{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "jNrO9U6opKV0",
   "metadata": {
    "id": "jNrO9U6opKV0"
   },
   "source": [
    "Notes: \n",
    "- some helper functions have been modified for this task\n",
    "- graph specific extra helper functions have been added\n",
    "- A normalization trick has been used to bound the regression target of all tasks between 0 and 1 and ease meta learning (note that in the original sinewave regression, a bound also existed on the y target based on the wave amplitude)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UrjQGgr5nUHC",
   "metadata": {
    "id": "UrjQGgr5nUHC"
   },
   "source": [
    "<h1> Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "S2WfmJiVTDpE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2WfmJiVTDpE",
    "outputId": "9570beeb-e22f-4ff1-ee3e-1a935d956f3f"
   },
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu113.html\n",
    "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu113.html\n",
    "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eGl9mcc0nOMP",
   "metadata": {
    "id": "eGl9mcc0nOMP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Required imports for neural network\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "# For GNNs\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import GraphNorm\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T3KVOwFXFOY0",
   "metadata": {
    "id": "T3KVOwFXFOY0"
   },
   "source": [
    "<h1> Data Loading and Generation\n",
    "\n",
    "Reptile for regression task using GNNs\n",
    "\n",
    "Some common GNN datasets are here:\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.GNNBenchmarkDataset\n",
    "\n",
    "We will use a regression dataset with 19 regression targets from the paper:\n",
    "“MoleculeNet: A Benchmark for Molecular Machine Learning”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jvvZSOiQe-Ne",
   "metadata": {
    "id": "jvvZSOiQe-Ne"
   },
   "source": [
    "For this implementation we focus on regressing only the Dipole moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "PIExsutGTQcB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIExsutGTQcB",
    "outputId": "fb8c10e2-7b81-43b5-fb2d-e2445ff00f9f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import QM9\n",
    "\n",
    "dataset = QM9(root='data/QM9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "LxK2rDRNTSxd",
   "metadata": {
    "id": "LxK2rDRNTSxd"
   },
   "outputs": [],
   "source": [
    "# This function is based on https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html\n",
    "\n",
    "#Function to display properties of the dataset (it is not necessary for anything)\n",
    "def display_graph_dataset_properties(dataset):\n",
    "  print()\n",
    "  print(f'Dataset: {dataset}:')\n",
    "  print('====================')\n",
    "  print(f'Number of graphs: {len(dataset)}')\n",
    "  print(f'Number of features: {dataset.num_features}')\n",
    "  print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "  data = dataset[0]  # Get the first graph object.Ç\n",
    "\n",
    "  print()\n",
    "  print('Look at a sample graph of the dataset')\n",
    "  print(data)\n",
    "  print('=============================================================')\n",
    "\n",
    "  # Gather some statistics about the first graph.\n",
    "  print(f'Number of nodes: {data.num_nodes}')\n",
    "  print(f'Number of edges: {data.num_edges}')\n",
    "  print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "  print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "  print(f'Has self-loops: {data.has_self_loops()}')\n",
    "  print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "GC6T1VZPF9Ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GC6T1VZPF9Ba",
    "outputId": "db2bd0d1-729e-48f4-f587-83f54413e340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: QM9(130831):\n",
      "====================\n",
      "Number of graphs: 130831\n",
      "Number of features: 11\n",
      "Number of classes: 19\n",
      "\n",
      "Look at a sample graph of the dataset\n",
      "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n",
      "=============================================================\n",
      "Number of nodes: 5\n",
      "Number of edges: 8\n",
      "Average node degree: 1.60\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "display_graph_dataset_properties(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lD_MRHC8T8Za",
   "metadata": {
    "id": "lD_MRHC8T8Za"
   },
   "outputs": [],
   "source": [
    "# Transform the dataset into a list\n",
    "dataset_list = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "  dataset_list.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3X51uGHDvSV",
   "metadata": {
    "id": "a3X51uGHDvSV"
   },
   "outputs": [],
   "source": [
    "#Shuffle the dataset list\n",
    "random.shuffle(dataset_list)\n",
    "#Split into train and test\n",
    "GRAPH_TRAIN = dataset_list[:int(np.floor(len(dataset_list)*0.9))]\n",
    "GRAPH_TEST = dataset_list[int(np.floor(len(dataset_list)*0.9)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8e0UtCj9yUh",
   "metadata": {
    "id": "i8e0UtCj9yUh"
   },
   "source": [
    "<h1> Equivariant Message Passing Model (based on Haitz Sáez de Ocáriz Borde's coursework for L45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "NLQskUOr_Q8E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLQskUOr_Q8E",
    "outputId": "3770e76b-0afe-42a1-c7d9-3af91c974846"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
    "!pip install -q torch-geometric==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "iVImWCea--7k",
   "metadata": {
    "id": "iVImWCea--7k"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "#To calculate euclidean distance\n",
    "import torch.nn as nn\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "from torch.nn import Linear, ReLU, BatchNorm1d, Module, Sequential\n",
    "from torch_scatter import scatter\n",
    "from torch_scatter import scatter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "o3Rbh_Rg-9Bw",
   "metadata": {
    "id": "o3Rbh_Rg-9Bw"
   },
   "outputs": [],
   "source": [
    "class MPNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
    "        \"\"\"Message Passing Neural Network Layer\n",
    "\n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            edge_dim: (int) - edge feature dimension `d_e`\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        # MLP `\\psi` for computing messages `m_ij`\n",
    "        # Implemented as a stack of Linear->BN->ReLU->Linear->BN->ReLU\n",
    "        # dims: (2d + d_e) -> d\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2*emb_dim + edge_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
    "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
    "          )\n",
    "        \n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        # Implemented as a stack of Linear->BN->ReLU->Linear->BN->ReLU\n",
    "        # dims: 2d -> d\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
    "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
    "          )\n",
    "\n",
    "    def forward(self, h, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        The forward pass updates node features `h` via one round of message passing.\n",
    "\n",
    "        As our MPNNLayer class inherits from the PyG MessagePassing parent class,\n",
    "        we simply need to call the `propagate()` function which starts the \n",
    "        message passing procedure: `message()` -> `aggregate()` -> `update()`.\n",
    "        \n",
    "        The MessagePassing class handles most of the logic for the implementation.\n",
    "        To build custom GNNs, we only need to define our own `message()`, \n",
    "        `aggregate()`, and `update()` functions (defined subsequently).\n",
    "\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "            edge_attr: (e, d_e) - edge features\n",
    "\n",
    "        Returns:\n",
    "            out: (n, d) - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, edge_attr=edge_attr)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, edge_attr):\n",
    "        \"\"\"Step (1) Message\n",
    "\n",
    "        The `message()` function constructs messages from source nodes j \n",
    "        to destination nodes i for each edge (i, j) in `edge_index`.\n",
    "\n",
    "        The arguments can be a bit tricky to understand: `message()` can take \n",
    "        any arguments that were initially passed to `propagate`. Additionally, \n",
    "        we can differentiate destination nodes and source nodes by appending \n",
    "        `_i` or `_j` to the variable name, e.g. for the node features `h`, we\n",
    "        can use `h_i` and `h_j`. \n",
    "        \n",
    "        This part is critical to understand as the `message()` function\n",
    "        constructs messages for each edge in the graph. The indexing of the\n",
    "        original node features `h` (or other node variables) is handled under\n",
    "        the hood by PyG.\n",
    "\n",
    "        Args:\n",
    "            h_i: (e, d) - destination node features\n",
    "            h_j: (e, d) - source node features\n",
    "            edge_attr: (e, d_e) - edge features\n",
    "        \n",
    "        Returns:\n",
    "            msg: (e, d) - messages `m_ij` passed through MLP `\\psi`\n",
    "        \"\"\"\n",
    "        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
    "        return self.mlp_msg(msg)\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        \"\"\"Step (2) Aggregate\n",
    "\n",
    "        The `aggregate` function aggregates the messages from neighboring nodes,\n",
    "        according to the chosen aggregation function ('sum' by default).\n",
    "\n",
    "        Args:\n",
    "            inputs: (e, d) - messages `m_ij` from destination to source nodes\n",
    "            index: (e, 1) - list of source nodes for each edge/message in `input`\n",
    "\n",
    "        Returns:\n",
    "            aggr_out: (n, d) - aggregated messages `m_i`\n",
    "        \"\"\"\n",
    "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "    \n",
    "    def update(self, aggr_out, h):\n",
    "        \"\"\"\n",
    "        Step (3) Update\n",
    "\n",
    "        The `update()` function computes the final node features by combining the \n",
    "        aggregated messages with the initial node features.\n",
    "\n",
    "        `update()` takes the first argument `aggr_out`, the result of `aggregate()`, \n",
    "        as well as any optional arguments that were initially passed to \n",
    "        `propagate()`. E.g. in this case, we additionally pass `h`.\n",
    "\n",
    "        Args:\n",
    "            aggr_out: (n, d) - aggregated messages `m_i`\n",
    "            h: (n, d) - initial node features\n",
    "\n",
    "        Returns:\n",
    "            upd_out: (n, d) - updated node features passed through MLP `\\phi`\n",
    "        \"\"\"\n",
    "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
    "        return self.mlp_upd(upd_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
    "\n",
    "class MPNNModel(Module):\n",
    "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
    "        \"\"\"Message Passing Neural Network model for graph property prediction\n",
    "\n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers `L`\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            in_dim: (int) - initial node feature dimension `d_n`\n",
    "            edge_dim: (int) - edge feature dimension `d_e`\n",
    "            out_dim: (int) - output dimension (fixed to 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear projection for initial node features\n",
    "        # dim: d_n -> d\n",
    "        self.lin_in = Linear(in_dim, emb_dim)\n",
    "        \n",
    "        # Stack of MPNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
    "        \n",
    "        # Global pooling/readout function `R` (mean pooling)\n",
    "        # PyG handles the underlying logic via `global_mean_pool()`\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        # Linear prediction head\n",
    "        # dim: d -> out_dim\n",
    "        self.lin_pred = Linear(emb_dim, out_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: (PyG.Data) - batch of PyG graphs\n",
    "\n",
    "        Returns: \n",
    "            out: (batch_size, out_dim) - prediction for each graph\n",
    "        \"\"\"\n",
    "        h = self.lin_in(data.x) # (n, d_n) -> (n, d)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            h = h + conv(h, data.edge_index, data.edge_attr) # (n, d) -> (n, d)\n",
    "            # Note that we add a residual connection after each MPNN layer\n",
    "\n",
    "        h_graph = self.pool(h, data.batch) # (n, d) -> (batch_size, d)\n",
    "\n",
    "        out = self.lin_pred(h_graph) # (batch_size, d) -> (batch_size, 1)\n",
    "\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "I-UXy6Pv9w4C",
   "metadata": {
    "id": "I-UXy6Pv9w4C"
   },
   "outputs": [],
   "source": [
    "class EquivariantMPNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
    "        \"\"\"Message Passing Neural Network Layer\n",
    "\n",
    "        This layer is invariant to 3D rotations and translations.\n",
    "\n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            edge_dim: (int) - edge feature dimension `d_e`\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        # MLP `\\psi` for computing messages `m_ij`\n",
    "        # dims: (2d+ d_e+1) -> d \n",
    "        # 2*d --> embedding for each node\n",
    "        # d_e --> edge dimension\n",
    "        # +1 --> distance between nodes in 3d\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2*emb_dim + edge_dim+1, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
    "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
    "          )\n",
    "        \n",
    "        # ==========================================\n",
    "        \n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        # dims: 2d -> d\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
    "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
    "          )\n",
    "\n",
    "        self.msg_to_weight = Sequential(\n",
    "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
    "            Linear(emb_dim, 1), ReLU()\n",
    "          )\n",
    "    def forward(self, h, pos, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        The forward pass updates node features `h` via one round of message passing.\n",
    "\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "            edge_attr: (e, d_e) - edge features\n",
    "\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features and pos\n",
    "        \"\"\"\n",
    "\n",
    "        out, new_pos = self.propagate(edge_index, h=h, edge_attr=edge_attr, pos = pos)\n",
    "        return (out, new_pos)\n",
    "        # ==========================================\n",
    "\n",
    "    def message(self, h_i, h_j, edge_attr, pos_i, pos_j):\n",
    "      \"\"\"The `message()` function constructs messages from source nodes j \n",
    "       to destination nodes i for each edge (i, j) in `edge_index`.\n",
    "    \n",
    "       Args:\n",
    "            h_i: (e, d) - destination node features\n",
    "            h_j: (e, d) - source node features\n",
    "            pos_i: (e, 3) - destination node positions\n",
    "            pos_j: (e, 3) - source node positions\n",
    "            edge_attr: (e, d_e) - edge features\n",
    "\n",
    "        Returns:\n",
    "          msg: [(e, d),(e,3)] - messages m_ij passed through MLP \\psi and relative difference\n",
    "      \"\"\"\n",
    "      dist = pdist(pos_i, pos_j).pow(2).reshape(pos_i.shape[0],1)\n",
    "      relative_difference = pos_i-pos_j\n",
    "      msg = torch.cat([h_i, h_j, edge_attr,dist], dim=-1)\n",
    "      return (self.mlp_msg(msg),relative_difference)\n",
    "    # ==========================================\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        \"\"\"The `aggregate` function aggregates the messages from neighboring nodes,\n",
    "        according to the chosen aggregation function ('sum' by default).\n",
    "\n",
    "        Args:\n",
    "            inputs: [(e, d),(e,3)] - messages `m_ij` from destination to source nodes and relative difference\n",
    "            index: (e, 1) - list of source nodes for each edge/message in `input`\n",
    "\n",
    "        Returns:\n",
    "            aggr_out: [(n, d),(e,3)] - aggregated messages `m_i` and message to weight\n",
    "        \"\"\"\n",
    "        inputs_h,relative_difference=inputs\n",
    "        return (scatter(inputs_h, index, dim=self.node_dim, reduce=self.aggr),scatter_mean(self.msg_to_weight(inputs_h)*relative_difference, index, dim=self.node_dim))\n",
    "    \n",
    "    def update(self, aggr_out, h,pos):\n",
    "        \"\"\"The `update()` function computes the final node features by combining the \n",
    "        aggregated messages with the initial node features.\n",
    "\n",
    "        Args:\n",
    "            aggr_out: [(n, d),(e,3)] - aggregated messages `m_i` and message to weight\n",
    "            h: (n, d) - initial node features\n",
    "\n",
    "        Returns:\n",
    "            upd_out: [(n, d),(n,3)] - updated node features passed through MLP `\\phi` and pos features\n",
    "        \"\"\"\n",
    "        aggr_out1,aggr_out2 = aggr_out\n",
    "        upd_out = torch.cat([h, aggr_out1], dim=-1)\n",
    "        pos_out = pos + aggr_out2\n",
    "        return (self.mlp_upd(upd_out),pos_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
    "\n",
    "\n",
    "class FinalMPNNModel(MPNNModel):\n",
    "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1,normalization=False):\n",
    "        \"\"\"Message Passing Neural Network model for graph property prediction\n",
    "\n",
    "        This model uses both node features and coordinates as inputs, and\n",
    "        is invariant to 3D rotations and translations (the constituent MPNN layers\n",
    "        are equivariant to 3D rotations and translations).\n",
    "\n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers `L`\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            in_dim: (int) - initial node feature dimension `d_n`\n",
    "            edge_dim: (int) - edge feature dimension `d_e`\n",
    "            out_dim: (int) - output dimension (fixed to 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear projection for initial node features\n",
    "        # dim: d_n -> d\n",
    "        self.lin_in = Linear(in_dim, emb_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Stack of MPNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EquivariantMPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
    "        \n",
    "        # Global pooling/readout function `R` (mean pooling)\n",
    "        # PyG handles the underlying logic via `global_mean_pool()`\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        # Linear prediction head\n",
    "        # dim: d -> out_dim\n",
    "        self.lin_pred = Linear(emb_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.normalization = normalization\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: (PyG.Data) - batch of PyG graphs\n",
    "\n",
    "        Returns: \n",
    "            out: (batch_size, out_dim) - prediction for each graph\n",
    "        \"\"\"\n",
    "        h = self.lin_in(data.x) # (n, d_n) -> (n, d)\n",
    "        pos = data.pos\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, data.edge_index, data.edge_attr)\n",
    "            \n",
    "            # Update node features\n",
    "            h = h + h_update # (n, d) -> (n, d)\n",
    "            # Note that we add a residual connection after each MPNN layer\n",
    "            \n",
    "            # Update node coordinates\n",
    "            pos = pos_update # (n, 3) -> (n, 3)\n",
    "\n",
    "        h_graph = self.pool(h, data.batch) # (n, d) -> (batch_size, d)\n",
    "        if self.normalization:\n",
    "          out = self.sigmoid(self.lin_pred(h_graph)) # (batch_size, d) -> (batch_size, 1)\n",
    "        else:\n",
    "          out = self.lin_pred(h_graph) # (batch_size, d) -> (batch_size, 1)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cu4urLF7Q88A",
   "metadata": {
    "id": "cu4urLF7Q88A"
   },
   "source": [
    "<h1> Simplistic GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "R1B0YTz6ytyN",
   "metadata": {
    "id": "R1B0YTz6ytyN"
   },
   "outputs": [],
   "source": [
    "\n",
    "# class GNN(torch.nn.Module):\n",
    "#     def __init__(self, input_dim=11, hidden_dim=200, output_dim=1):\n",
    "#         super(GNN, self).__init__()\n",
    "\n",
    "#         #Hidden Layers\n",
    "#         self.hidden1 = GraphConv(input_dim, hidden_dim)\n",
    "#         self.hidden2 = GraphConv(hidden_dim, hidden_dim)\n",
    "#         self.hidden3 = GraphConv(hidden_dim, output_dim)\n",
    "#         self.norm = GraphNorm(hidden_dim)\n",
    "\n",
    "#         #Activation Function\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "\n",
    "#     def forward(self, input_x, edge_index, batch):\n",
    "      \n",
    "#         #Standard forward\n",
    "#         x = self.hidden1(input_x,edge_index)\n",
    "#         x = self.norm(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.hidden2(x,edge_index)\n",
    "#         x = self.norm(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.hidden3(x,edge_index)\n",
    "\n",
    "#         #Global mean pool across batches\n",
    "#         x = global_mean_pool(x, batch)\n",
    "\n",
    "        \n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G-ExWACxQ3mt",
   "metadata": {
    "id": "G-ExWACxQ3mt"
   },
   "source": [
    "<h1> Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1zyNHFXdOnug",
   "metadata": {
    "id": "1zyNHFXdOnug"
   },
   "outputs": [],
   "source": [
    "# The Minimum Square Error is used to evaluate the difference between prediction and ground truth\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def copy_existing_model(model):\n",
    "    # Function to copy an existing model\n",
    "    # We initialize a new model\n",
    "    new_model = FinalMPNNModel()\n",
    "    # Copy the previous model's parameters into the new model\n",
    "    new_model.load_state_dict(model.state_dict())\n",
    "    return new_model\n",
    "\n",
    "def initialization_to_store_meta_losses():\n",
    "  # This function creates lists to store the meta losses\n",
    "  global store_train_loss_meta; store_train_loss_meta = []\n",
    "  global store_test_loss_meta; store_test_loss_meta = []\n",
    "\n",
    "def test_set_validation(model,new_model,graph,lr_inner,k,store_test_loss_meta,task):\n",
    "    # This functions does not actually affect the main algorithm, it is just used to evaluate the new model\n",
    "    new_model = training(model, graph, lr_inner, k,task)\n",
    "    # Obtain the loss\n",
    "    loss = evaluation(new_model, graph,task)\n",
    "    # Store loss\n",
    "    store_test_loss_meta.append(loss)\n",
    "\n",
    "def train_set_evaluation(new_model,graph,store_train_loss_meta,task):\n",
    "    loss = evaluation(new_model, graph,task)\n",
    "    store_train_loss_meta.append(loss) \n",
    "\n",
    "def print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step=1000):\n",
    "  if epoch % printing_step == 0:\n",
    "    print(f'Epochh : {epoch}, Average Train Meta Loss : {np.mean(store_train_loss_meta)}, Average Test Meta Loss : {np.mean(store_test_loss_meta)}')\n",
    "\n",
    "#This is based on the paper update rule, we calculate the difference between parameters and then this is used by the optimizer, rather than doing the update by hand\n",
    "def reptile_parameter_update(model,new_model):\n",
    "  # Zip models for the loop\n",
    "  zip_models = zip(model.parameters(), new_model.parameters())\n",
    "  for parameter, new_parameter in zip_models:\n",
    "    if parameter.grad is None:\n",
    "      parameter.grad = torch.tensor(torch.zeros_like(parameter))\n",
    "    # Here we are adding the gradient that will later be used by the optimizer\n",
    "    parameter.grad.data.add_(parameter.data - new_parameter.data)\n",
    "\n",
    "# Define commands in order needed for the metaupdate\n",
    "# Note that if we change the order it doesn't behave the same\n",
    "def metaoptimizer_update(metaoptimizer):\n",
    "  # Take step\n",
    "  metaoptimizer.step()\n",
    "  # Reset gradients\n",
    "  metaoptimizer.zero_grad()\n",
    "\n",
    "def metaupdate(model,new_model,metaoptimizer):\n",
    "  # Combine the two previous functions into a single metaupdate function\n",
    "  # First we calculate the gradients\n",
    "  reptile_parameter_update(model,new_model)\n",
    "  # Use those gradients in the optimizer\n",
    "  metaoptimizer_update(metaoptimizer)\n",
    "\n",
    "def evaluation(new_model, graph, task, item = True, normalization = False):\n",
    "    # Make model prediction\n",
    "    prediction = new_model(graph)\n",
    "\n",
    "    label = graph.y[:,task:task+1]\n",
    "    if normalization == True:\n",
    "      sigmoid = nn.Sigmoid()\n",
    "      label = sigmoid(label)\n",
    "\n",
    "    # Get loss\n",
    "    if item == True: #Depending on whether we need to return the loss value for storing or for backprop\n",
    "      loss = criterion(prediction,label).item()\n",
    "    else:\n",
    "      loss = criterion(prediction,label)\n",
    "    return loss\n",
    "\n",
    "def training(model, graph, lr_k, k,task):\n",
    "    # Create new model which we will train on\n",
    "    new_model = copy_existing_model(model)\n",
    "    # Define new optimizer\n",
    "    koptimizer = torch.optim.SGD(new_model.parameters(), lr=lr_k)\n",
    "    # Update the model multiple times, note that k>1 (do not confuse k with K)\n",
    "    for i in range(k):\n",
    "        # Reset optimizer\n",
    "        koptimizer.zero_grad()\n",
    "        # Evaluate the model\n",
    "        loss = evaluation(new_model, graph, task, item = False)\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        koptimizer.step()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7jkgwwMJUSbL",
   "metadata": {
    "id": "7jkgwwMJUSbL"
   },
   "source": [
    "<h1> Additional GNN Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hsATAza-UtiF",
   "metadata": {
    "id": "hsATAza-UtiF"
   },
   "source": [
    "Additional helper functions to handle minibatching based on coursework by Haitz Sáez de Ocáriz for L45 Practical 1. The code was partially given in the practical and we had to fill it in, so this is based on my solution. Also, some further modification applied for our implementation: for message passing we include coordinate encoding features and edge attributes, apart from node features, labels, and the adjancency metrics that describes node connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "iHFyLSSKUoYG",
   "metadata": {
    "id": "iHFyLSSKUoYG"
   },
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "    def __init__(self, edge_index, x, y,edge_attr,pos):\n",
    "        \"\"\" Graph structure \n",
    "            for a mini-batch it will store a big (sparse) graph \n",
    "            representing the entire batch\n",
    "        Args:\n",
    "            x: node features  [num_nodes x num_feats]\n",
    "            y: graph labels   [num_graphs]\n",
    "            edge_index: list of edges [2 x num_edges]\n",
    "        \"\"\"\n",
    "        self.edge_index = edge_index\n",
    "        self.x = x.to(torch.float32)\n",
    "        self.y = y\n",
    "        self.num_nodes = self.x.shape[0]\n",
    "        self.edge_attr = edge_attr\n",
    "        self.pos = pos\n",
    "\n",
    "    #ignore this for now, it will be useful for batching\n",
    "    def set_batch(self, batch):\n",
    "        \"\"\" list of ints that maps each node to the graph it belongs to\n",
    "            e.g. for batch = [0,0,0,1,1,1,1]: the first 3 nodes belong to graph_0 while\n",
    "            the last 4 belong to graph_1\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "\n",
    "    # this function return a sparse tensor\n",
    "    def get_adjacency_matrix(self):\n",
    "        \"\"\" from the list of edges create \n",
    "        a num_nodes x num_nodes sparse adjacency matrix\n",
    "        \"\"\"\n",
    "        return torch.sparse.LongTensor(self.edge_index, \n",
    "                              # we work with a binary adj containing 1 if an edge exist\n",
    "                              torch.ones((self.edge_index.shape[1])), \n",
    "                              torch.Size((self.num_nodes, self.num_nodes))\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "WHUDCDbqUc2A",
   "metadata": {
    "id": "WHUDCDbqUc2A"
   },
   "outputs": [],
   "source": [
    "def create_mini_batch(graph_list):\n",
    "    \"\"\" Built a sparse graph from a batch of graphs\n",
    "    Args:\n",
    "        graph_list: list of Graph objects in a batch\n",
    "    Returns:\n",
    "        a big (sparse) Graph representing the entire batch\n",
    "    \"\"\"\n",
    "    #insert first graph into the structure\n",
    "    batch_edge_index = graph_list[0].edge_index\n",
    "    batch_x = graph_list[0].x\n",
    "    batch_y = graph_list[0].y\n",
    "    batch_edge_attr = graph_list[0].edge_attr\n",
    "    batch_pos = graph_list[0].pos\n",
    "    batch_batch = torch.zeros((graph_list[0].num_nodes), dtype=torch.int64)\n",
    "    # ============ YOUR CODE HERE =============\n",
    "    # you may need additional variables\n",
    "    num_nodes_added= graph_list[0].num_nodes\n",
    "    # ==========================================\n",
    "\n",
    "    #append the rest of the graphs to the structure\n",
    "    for idx, graph in enumerate(graph_list[1:]):\n",
    "        # ============ YOUR CODE HERE =============\n",
    "        # concat the features\n",
    "        batch_x = torch.cat((batch_x,graph.x))\n",
    "        # concat the labels\n",
    "        batch_y = torch.cat((batch_y,graph.y))\n",
    "        # concat the coords \n",
    "        batch_pos = torch.cat((batch_pos,graph.pos))\n",
    "\n",
    "        # concat the adjacency matrix as a block diagonal matrix\n",
    "        batch_edge_index = torch.cat((batch_edge_index, torch.add(graph.edge_index, num_nodes_added)), dim=1)\n",
    "        batch_edge_attr = torch.cat((batch_edge_attr, graph.edge_attr))\n",
    "        num_nodes_added += graph.num_nodes\n",
    "        # ==========================================\n",
    "\n",
    "        # ============ YOUR CODE HERE =============\n",
    "        # create the array of indexes mapping nodes in the batch-graph\n",
    "        # to the graph they belong to\n",
    "        # specify the mapping between the new nodes and the graph they belong to (idx+1)\n",
    "        batch_batch = torch.cat((batch_batch, torch.full((graph.num_nodes,), idx + 1)))\n",
    "        # ==========================================\n",
    "    #create the big sparse graph \n",
    "    batch_graph = Graph(batch_edge_index, batch_x, batch_y, batch_edge_attr,batch_pos)\n",
    "    #attach the index array to the Graph structure\n",
    "    batch_graph.set_batch(batch_batch)\n",
    "    return batch_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "VfcxN2ITVHUD",
   "metadata": {
    "id": "VfcxN2ITVHUD"
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "def get_color_coded_str(i, color):\n",
    "    return \"\\033[3{}m{}\\033[0m\".format(int(color), int(i))\n",
    "def print_color_numpy(map, list_graphs):\n",
    "    \"\"\" print matrix map in color according to list_graphs\n",
    "    \"\"\"\n",
    "    list_blocks = []\n",
    "    for i,graph in enumerate(list_graphs):\n",
    "        block_i = (i+1)*np.ones((graph.num_nodes,graph.num_nodes))\n",
    "        list_blocks += [block_i]\n",
    "    block_color = block_diag(*list_blocks)\n",
    "    \n",
    "    map_modified = np.vectorize(get_color_coded_str)(map, block_color)\n",
    "    print(\"\\n\".join([\" \".join([\"{}\"]*map.shape[0])]*map.shape[1]).format(*[x for y in map_modified.tolist() for x in y]))\n",
    "\n",
    "def draw_one_graph(ax, edges, label=None, node_emb=None, layout=None, special_color=False):\n",
    "    \"\"\"draw a graph with networkx based on adjacency matrix (edges)\n",
    "    graph labels could be displayed as a title for each graph\n",
    "    node_emb could be displayed in colors\n",
    "    \"\"\"\n",
    "    graph = nx.Graph()\n",
    "    edges = zip(edges[0], edges[1])\n",
    "    graph.add_edges_from(edges)\n",
    "    node_pos = layout(graph)\n",
    "    #add colors according to node embeding\n",
    "    if (node_emb is not None) or special_color:\n",
    "        color_map = []\n",
    "        node_list = [node[0] for node in graph.nodes(data = True)]\n",
    "        for i,node in enumerate(node_list):\n",
    "            #just ignore this branch\n",
    "            if special_color:\n",
    "                if len(node_list) == 3:\n",
    "                    crt_color = (1,0,0)\n",
    "                elif len(node_list) == 5:\n",
    "                    crt_color = (0,1,0)\n",
    "                elif len(node_list) == 4:\n",
    "                    crt_color = (1,1,0)\n",
    "                else:\n",
    "                  special_list = [(1,0,0)] * 3 + [(0,1,0)] * 5 + [(1,1,0)] * 4\n",
    "                  crt_color = special_list[i]\n",
    "            else:\n",
    "                crt_node_emb = node_emb[node]\n",
    "                #map float number (node embeding) to a color\n",
    "                crt_color = cm.gist_rainbow(crt_node_emb, bytes=True)\n",
    "                crt_color = (crt_color[0]/255.0, crt_color[1]/255.0, crt_color[2]/255.0, crt_color[3]/255.0)\n",
    "            color_map.append(crt_color)\n",
    "      \n",
    "        nx.draw_networkx_nodes(graph,node_pos, node_color=color_map,\n",
    "                        nodelist = node_list, ax=ax)\n",
    "        nx.draw_networkx_edges(graph, node_pos, ax=ax)\n",
    "        nx.draw_networkx_labels(graph,node_pos, ax=ax)\n",
    "    else:\n",
    "        nx.draw_networkx(graph, node_pos, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "FvoEy9jpVM7j",
   "metadata": {
    "id": "FvoEy9jpVM7j"
   },
   "outputs": [],
   "source": [
    "def gallery(graphs, labels=None, node_emb=None, special_color=False, max_graphs=4, max_fig_size=(40, 10), layout=nx.layout.kamada_kawai_layout):\n",
    "    ''' Draw multiple graphs as a gallery \n",
    "    Args:\n",
    "      graphs: torch_geometrics.dataset object/ List of Graph objects\n",
    "      labels: num_graphs\n",
    "      node_emb: num_graphs* [num_nodes x num_ch]\n",
    "      max_graphs: maximum graphs display\n",
    "    '''\n",
    "    num_graphs = min(len(graphs), max_graphs)\n",
    "    ff, axes = plt.subplots(1, num_graphs,\n",
    "                            figsize=max_fig_size,\n",
    "                            subplot_kw={'xticks': [], 'yticks': []})\n",
    "    if num_graphs == 1:\n",
    "        axes = [axes]\n",
    "    if node_emb is None:\n",
    "        node_emb = num_graphs*[None]\n",
    "    if labels is None:\n",
    "        labels = num_graphs * [\" \"]\n",
    "\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        draw_one_graph(axes[i], graphs[i].edge_index.numpy(), labels[i], node_emb[i], layout, special_color)\n",
    "        if labels[i] != \" \":\n",
    "            axes[i].set_title(f\"Target: {labels[i]}\", fontsize=28)\n",
    "        axes[i].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "AIJLHotuVAie",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIJLHotuVAie",
    "outputId": "13f6d07f-c767-4b23-8325-88f64725bd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number_of_nodes: 54\n",
      "Batch features shape: torch.Size([54, 11])\n",
      "Batch labels shape: torch.Size([3, 19])\n",
      "Batch adjacency: \n",
      "\u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m1\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m1\u001b[0m \u001b[31m1\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m1\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[31m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m1\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[32m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n",
      "\u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[30m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m1\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m \u001b[33m0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 3 random custom-designed graphs for visualisations\n",
    "graph1 = Graph(x=torch.rand((3,32)), \n",
    "               y=torch.rand((1)), \n",
    "               edge_index=torch.tensor([[0,0,0,1,1,1,2,2,2],[0,1,2,0,1,2,0,1,2]]),\n",
    "               edge_attr = torch.tensor([[0,0,0,1,1,1,2,2,2],[0,1,2,0,1,2,0,1,2]]),\n",
    "               pos = torch.rand((3,3))\n",
    "               )\n",
    "graph1 = random.sample(GRAPH_TRAIN, 1)[0]\n",
    "graph2 = random.sample(GRAPH_TRAIN, 1)[0]\n",
    "graph3 = random.sample(GRAPH_TRAIN, 1)[0]\n",
    "# graph2 = Graph(x=torch.rand((5,32)), \n",
    "#                y=torch.rand((1)), \n",
    "#                edge_index=torch.tensor([[0,0,0,0,0,1,1,1,2,1,2,3,4], [0,1,2,3,4,2,3,4,4,0,0,0,0]]))\n",
    "# graph3 = Graph(x=torch.rand((4,32)),\n",
    "#                y=torch.rand((1)), \n",
    "#               edge_index=torch.tensor([[0,1,2,3],[1,2,3,0]]))\n",
    "list_graphs = [graph1, graph2, graph3]\n",
    "\n",
    "# create a mini-batch from these 3 graphs\n",
    "batch_sample = create_mini_batch(list_graphs)\n",
    "\n",
    "# show statistics about the new graph built from this batch of graphs\n",
    "print(f\"Batch number_of_nodes: {batch_sample.num_nodes}\")\n",
    "print(f\"Batch features shape: {batch_sample.x.shape}\")\n",
    "print(f\"Batch labels shape: {batch_sample.y.shape}\")\n",
    "\n",
    "print(f\"Batch adjacency: \")\n",
    "print_color_numpy(batch_sample.get_adjacency_matrix().to_dense().numpy(), list_graphs)\n",
    "\n",
    "\n",
    "# gallery([graph1, graph1, graph1, batch_sample], max_fig_size=(20,6), special_color=True)\n",
    "# print(f\"And we also have access to which graph each node belongs to {batch_sample.batch}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-4Ps8P2IRCmF",
   "metadata": {
    "id": "-4Ps8P2IRCmF"
   },
   "source": [
    "<h1> Reptile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ogpg_DHizlC",
   "metadata": {
    "id": "8ogpg_DHizlC"
   },
   "outputs": [],
   "source": [
    "#Define important variables\n",
    "epochs = int(5000) # number of epochs \n",
    "lr_meta=0.001 # Learning rate for meta model (outer loop)\n",
    "printing_step=10 # how many epochs should we wait to print the loss\n",
    "lr_k=0.0005 # Internal learning rate\n",
    "k=5 # Number of internal updates for each task\n",
    "K = 1 #Number of samples per task\n",
    "number_of_tasks = 5 #number of tasks for metalearning (max is 19), using 5 converges relatively fast, otherwise it is a bit of a pain\n",
    "# Initializations\n",
    "initialization_to_store_meta_losses()\n",
    "model = FinalMPNNModel()\n",
    "metaoptimizer = torch.optim.Adam(model.parameters(), lr=lr_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "DCP6ZPotUEuH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCP6ZPotUEuH",
    "outputId": "81f368a3-1cef-4fa4-8f3b-f2f84b0ec9f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 11])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(GRAPH_TRAIN, 1)[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "-4-zQWWKFt3s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "-4-zQWWKFt3s",
    "outputId": "1a8bca23-0af4-4488-93f8-77740f6c49bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kcollins/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "<ipython-input-13-c9d07cadfc48>:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  parameter.grad = torch.tensor(torch.zeros_like(parameter))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 0, Average Train Meta Loss : 0.0937056839466095, Average Test Meta Loss : 0.5471664071083069\n",
      "Epochh : 10, Average Train Meta Loss : 4.832762798971751, Average Test Meta Loss : 3.696528786810549\n",
      "Epochh : 20, Average Train Meta Loss : 5.307768859975373, Average Test Meta Loss : 4.2919080701768895\n",
      "Epochh : 30, Average Train Meta Loss : 4.388282871525359, Average Test Meta Loss : 3.7559509874314947\n",
      "Epochh : 40, Average Train Meta Loss : 3.5648808013690387, Average Test Meta Loss : 3.0381812579532337\n",
      "Epochh : 50, Average Train Meta Loss : 3.3838038047645513, Average Test Meta Loss : 2.821522143392771\n",
      "Epochh : 60, Average Train Meta Loss : 3.037698514309147, Average Test Meta Loss : 2.727577949841279\n",
      "Epochh : 70, Average Train Meta Loss : 2.7435224440840766, Average Test Meta Loss : 2.4135683974441364\n",
      "Epochh : 80, Average Train Meta Loss : 2.4572235425405298, Average Test Meta Loss : 2.212434231600852\n",
      "Epochh : 90, Average Train Meta Loss : 2.311231020001207, Average Test Meta Loss : 2.0737233253405694\n",
      "Epochh : 100, Average Train Meta Loss : 2.1202957418648984, Average Test Meta Loss : 1.930704935542355\n",
      "Epochh : 110, Average Train Meta Loss : 1.9797285312495922, Average Test Meta Loss : 1.7810520576132756\n",
      "Epochh : 120, Average Train Meta Loss : 1.8523635690913234, Average Test Meta Loss : 1.6841432267902927\n",
      "Epochh : 130, Average Train Meta Loss : 1.7373954167073897, Average Test Meta Loss : 1.5969971779937409\n",
      "Epochh : 140, Average Train Meta Loss : 1.637939606335304, Average Test Meta Loss : 1.5088620928022851\n",
      "Epochh : 150, Average Train Meta Loss : 1.5434948689359609, Average Test Meta Loss : 1.42191507199275\n",
      "Epochh : 160, Average Train Meta Loss : 1.4665045008421782, Average Test Meta Loss : 1.3507403997216316\n",
      "Epochh : 170, Average Train Meta Loss : 1.391872596876864, Average Test Meta Loss : 1.283736041191032\n",
      "Epochh : 180, Average Train Meta Loss : 1.3237030152594047, Average Test Meta Loss : 1.2210066117025347\n",
      "Epochh : 190, Average Train Meta Loss : 1.2637511867086169, Average Test Meta Loss : 1.1658339282964039\n",
      "Epochh : 200, Average Train Meta Loss : 1.2032378931295686, Average Test Meta Loss : 1.1098196142620125\n",
      "Epochh : 210, Average Train Meta Loss : 1.1502969653140263, Average Test Meta Loss : 1.0597889488650118\n",
      "Epochh : 220, Average Train Meta Loss : 1.0993597145348115, Average Test Meta Loss : 1.012954360949681\n",
      "Epochh : 230, Average Train Meta Loss : 1.0523801017657817, Average Test Meta Loss : 0.9699339251359526\n",
      "Epochh : 240, Average Train Meta Loss : 1.0097196291620951, Average Test Meta Loss : 0.9306696593283227\n",
      "Epochh : 250, Average Train Meta Loss : 0.9709009357458013, Average Test Meta Loss : 0.894676001715489\n",
      "Epochh : 260, Average Train Meta Loss : 0.9345721523381723, Average Test Meta Loss : 0.8618719714030585\n",
      "Epochh : 270, Average Train Meta Loss : 0.900640907345932, Average Test Meta Loss : 0.8316430369906351\n",
      "Epochh : 280, Average Train Meta Loss : 0.8696215835041462, Average Test Meta Loss : 0.8024859034329233\n",
      "Epochh : 290, Average Train Meta Loss : 0.8403618076389245, Average Test Meta Loss : 0.7754771336486755\n",
      "Epochh : 300, Average Train Meta Loss : 0.8127438770417117, Average Test Meta Loss : 0.750117859127245\n",
      "Epochh : 310, Average Train Meta Loss : 0.7867981426427092, Average Test Meta Loss : 0.7261793044520117\n",
      "Epochh : 320, Average Train Meta Loss : 0.7623735115796925, Average Test Meta Loss : 0.7036043704335521\n",
      "Epochh : 330, Average Train Meta Loss : 0.7395170374876405, Average Test Meta Loss : 0.6824196826863641\n",
      "Epochh : 340, Average Train Meta Loss : 0.7179056788887129, Average Test Meta Loss : 0.6625272694496421\n",
      "Epochh : 350, Average Train Meta Loss : 0.6975018009025188, Average Test Meta Loss : 0.6437022248650933\n",
      "Epochh : 360, Average Train Meta Loss : 0.6782258174265977, Average Test Meta Loss : 0.6259010649234616\n",
      "Epochh : 370, Average Train Meta Loss : 0.6599755351263948, Average Test Meta Loss : 0.6090546323150127\n",
      "Epochh : 380, Average Train Meta Loss : 0.6426840883619074, Average Test Meta Loss : 0.593089109677979\n",
      "Epochh : 390, Average Train Meta Loss : 0.6262576854396521, Average Test Meta Loss : 0.5779344719747912\n",
      "Epochh : 400, Average Train Meta Loss : 0.610648459438752, Average Test Meta Loss : 0.5635357804548778\n",
      "Epochh : 410, Average Train Meta Loss : 0.5958101463890169, Average Test Meta Loss : 0.5498389890123199\n",
      "Epochh : 420, Average Train Meta Loss : 0.5816820073591825, Average Test Meta Loss : 0.536791644410542\n",
      "Epochh : 430, Average Train Meta Loss : 0.5682037425431274, Average Test Meta Loss : 0.5243494236303446\n",
      "Epochh : 440, Average Train Meta Loss : 0.5553256065974012, Average Test Meta Loss : 0.5124654333573926\n",
      "Epochh : 450, Average Train Meta Loss : 0.5430173512912434, Average Test Meta Loss : 0.5011081698544311\n",
      "Epochh : 460, Average Train Meta Loss : 0.5312408319787288, Average Test Meta Loss : 0.4902431921887178\n",
      "Epochh : 470, Average Train Meta Loss : 0.5199628269512953, Average Test Meta Loss : 0.4798467058868944\n",
      "Epochh : 480, Average Train Meta Loss : 0.5091547928292478, Average Test Meta Loss : 0.4698721739535362\n",
      "Epochh : 490, Average Train Meta Loss : 0.4987886317790673, Average Test Meta Loss : 0.46030597780157995\n",
      "Epochh : 500, Average Train Meta Loss : 0.4888333058345556, Average Test Meta Loss : 0.4511188632238714\n",
      "Epochh : 510, Average Train Meta Loss : 0.47926825388428035, Average Test Meta Loss : 0.44229179332266605\n",
      "Epochh : 520, Average Train Meta Loss : 0.4700788130425902, Average Test Meta Loss : 0.4338036990027686\n",
      "Epochh : 530, Average Train Meta Loss : 0.4612266459444179, Average Test Meta Loss : 0.42563439835180406\n",
      "Epochh : 540, Average Train Meta Loss : 0.4527029370881211, Average Test Meta Loss : 0.41776698295349013\n",
      "Epochh : 550, Average Train Meta Loss : 0.44448706065687277, Average Test Meta Loss : 0.4101899025824266\n",
      "Epochh : 560, Average Train Meta Loss : 0.43656574165039236, Average Test Meta Loss : 0.4028782701941374\n",
      "Epochh : 570, Average Train Meta Loss : 0.4289202226029421, Average Test Meta Loss : 0.3958235129698273\n",
      "Epochh : 580, Average Train Meta Loss : 0.4215385146368352, Average Test Meta Loss : 0.389010782457592\n",
      "Epochh : 590, Average Train Meta Loss : 0.41440593325597325, Average Test Meta Loss : 0.382428584820453\n",
      "Epochh : 600, Average Train Meta Loss : 0.40751066527901947, Average Test Meta Loss : 0.37606538784689897\n",
      "Epochh : 610, Average Train Meta Loss : 0.40084115911397994, Average Test Meta Loss : 0.36991050969010447\n",
      "Epochh : 620, Average Train Meta Loss : 0.3943865472225651, Average Test Meta Loss : 0.36395388530327016\n",
      "Epochh : 630, Average Train Meta Loss : 0.3881366390303342, Average Test Meta Loss : 0.3581860029733806\n",
      "Epochh : 640, Average Train Meta Loss : 0.38208151070005375, Average Test Meta Loss : 0.35259809063355324\n",
      "Epochh : 650, Average Train Meta Loss : 0.37621241871377153, Average Test Meta Loss : 0.3471818938293292\n",
      "Epochh : 660, Average Train Meta Loss : 0.37052103022687777, Average Test Meta Loss : 0.3419299881656603\n",
      "Epochh : 670, Average Train Meta Loss : 0.3649992295261686, Average Test Meta Loss : 0.3368342487090687\n",
      "Epochh : 680, Average Train Meta Loss : 0.35964017246624463, Average Test Meta Loss : 0.33188813116081667\n",
      "Epochh : 690, Average Train Meta Loss : 0.35443567185305636, Average Test Meta Loss : 0.3270853867573742\n",
      "Epochh : 700, Average Train Meta Loss : 0.34937954680720745, Average Test Meta Loss : 0.3224194477256783\n",
      "Epochh : 710, Average Train Meta Loss : 0.34446564349674164, Average Test Meta Loss : 0.3178847322114224\n",
      "Epochh : 720, Average Train Meta Loss : 0.3396881448147969, Average Test Meta Loss : 0.3134759630925508\n",
      "Epochh : 730, Average Train Meta Loss : 0.3350413181610885, Average Test Meta Loss : 0.30918766156991384\n",
      "Epochh : 740, Average Train Meta Loss : 0.3305198432738283, Average Test Meta Loss : 0.30501509016265443\n",
      "Epochh : 750, Average Train Meta Loss : 0.32611883385447415, Average Test Meta Loss : 0.300953640185763\n",
      "Epochh : 760, Average Train Meta Loss : 0.32183343747113263, Average Test Meta Loss : 0.29699893260500004\n",
      "Epochh : 770, Average Train Meta Loss : 0.3176592046922903, Average Test Meta Loss : 0.2931468064391884\n",
      "Epochh : 780, Average Train Meta Loss : 0.31359186666733185, Average Test Meta Loss : 0.289393329692591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 790, Average Train Meta Loss : 0.3096273677979154, Average Test Meta Loss : 0.2857347542182986\n",
      "Epochh : 800, Average Train Meta Loss : 0.3057618578798835, Average Test Meta Loss : 0.2821675288944746\n",
      "Epochh : 810, Average Train Meta Loss : 0.30199167516078307, Average Test Meta Loss : 0.2786882932833434\n",
      "Epochh : 820, Average Train Meta Loss : 0.29831333593772885, Average Test Meta Loss : 0.27529379525553577\n",
      "Epochh : 830, Average Train Meta Loss : 0.29472352543341557, Average Test Meta Loss : 0.2719809954684441\n",
      "Epochh : 840, Average Train Meta Loss : 0.2912190844030184, Average Test Meta Loss : 0.26874697657602387\n",
      "Epochh : 850, Average Train Meta Loss : 0.287797007996716, Average Test Meta Loss : 0.2655889627721981\n",
      "Epochh : 860, Average Train Meta Loss : 0.28445441947070443, Average Test Meta Loss : 0.26250430602299707\n",
      "Epochh : 870, Average Train Meta Loss : 0.28118858259790686, Average Test Meta Loss : 0.2594904796770934\n",
      "Epochh : 880, Average Train Meta Loss : 0.2779968900205056, Average Test Meta Loss : 0.2565450771909417\n",
      "Epochh : 890, Average Train Meta Loss : 0.2748768351775243, Average Test Meta Loss : 0.25366578354701663\n",
      "Epochh : 900, Average Train Meta Loss : 0.27182603932257016, Average Test Meta Loss : 0.25085040456017993\n",
      "Epochh : 910, Average Train Meta Loss : 0.26884221971645617, Average Test Meta Loss : 0.24809683262369717\n",
      "Epochh : 920, Average Train Meta Loss : 0.265923194799922, Average Test Meta Loss : 0.24540305598238932\n",
      "Epochh : 930, Average Train Meta Loss : 0.2630686977186805, Average Test Meta Loss : 0.2427671478193593\n",
      "Epochh : 940, Average Train Meta Loss : 0.2602730696965736, Average Test Meta Loss : 0.24018763300875048\n",
      "Epochh : 950, Average Train Meta Loss : 0.2575362340536266, Average Test Meta Loss : 0.23766200069780025\n",
      "Epochh : 960, Average Train Meta Loss : 0.2548563604661224, Average Test Meta Loss : 0.23518893112633973\n",
      "Epochh : 970, Average Train Meta Loss : 0.25223168219733894, Average Test Meta Loss : 0.23276680045367318\n",
      "Epochh : 980, Average Train Meta Loss : 0.24966052358773783, Average Test Meta Loss : 0.23039405024899792\n",
      "Epochh : 990, Average Train Meta Loss : 0.24714124489333844, Average Test Meta Loss : 0.228069191342696\n",
      "Epochh : 1000, Average Train Meta Loss : 0.2446723034095465, Average Test Meta Loss : 0.22579077787365012\n",
      "Epochh : 1010, Average Train Meta Loss : 0.24225220196688346, Average Test Meta Loss : 0.22355745088569665\n",
      "Epochh : 1020, Average Train Meta Loss : 0.2398795065823378, Average Test Meta Loss : 0.22136785961946515\n",
      "Epochh : 1030, Average Train Meta Loss : 0.2375528382401752, Average Test Meta Loss : 0.21922074186975335\n",
      "Epochh : 1040, Average Train Meta Loss : 0.23527087065292718, Average Test Meta Loss : 0.21711487502592616\n",
      "Epochh : 1050, Average Train Meta Loss : 0.23303233294412729, Average Test Meta Loss : 0.2150490818650062\n",
      "Epochh : 1060, Average Train Meta Loss : 0.23083598674477726, Average Test Meta Loss : 0.21302222935523699\n",
      "Epochh : 1070, Average Train Meta Loss : 0.22868065540327803, Average Test Meta Loss : 0.21103322636227298\n",
      "Epochh : 1080, Average Train Meta Loss : 0.2265652009702694, Average Test Meta Loss : 0.2090810227067855\n",
      "Epochh : 1090, Average Train Meta Loss : 0.2244885274598519, Average Test Meta Loss : 0.20716460645231344\n",
      "Epochh : 1100, Average Train Meta Loss : 0.22244957645428107, Average Test Meta Loss : 0.20528300371789304\n",
      "Epochh : 1110, Average Train Meta Loss : 0.2204473300818403, Average Test Meta Loss : 0.2034352723902402\n",
      "Epochh : 1120, Average Train Meta Loss : 0.21848080621137303, Average Test Meta Loss : 0.20162050734123957\n",
      "Epochh : 1130, Average Train Meta Loss : 0.216549057396098, Average Test Meta Loss : 0.19983783350344925\n",
      "Epochh : 1140, Average Train Meta Loss : 0.21465117146133644, Average Test Meta Loss : 0.1980864064794357\n",
      "Epochh : 1150, Average Train Meta Loss : 0.21278626143667198, Average Test Meta Loss : 0.1963654125712178\n",
      "Epochh : 1160, Average Train Meta Loss : 0.21095347718410626, Average Test Meta Loss : 0.19467406567791543\n",
      "Epochh : 1170, Average Train Meta Loss : 0.2091519958287076, Average Test Meta Loss : 0.19301160568714848\n",
      "Epochh : 1180, Average Train Meta Loss : 0.20738102323319343, Average Test Meta Loss : 0.1913772994273987\n",
      "Epochh : 1190, Average Train Meta Loss : 0.2056397889688248, Average Test Meta Loss : 0.1897704371462832\n",
      "Epochh : 1200, Average Train Meta Loss : 0.20392755183859892, Average Test Meta Loss : 0.1881903385975217\n",
      "Epochh : 1210, Average Train Meta Loss : 0.20224359743680925, Average Test Meta Loss : 0.18663633174554214\n",
      "Epochh : 1220, Average Train Meta Loss : 0.20058722075403546, Average Test Meta Loss : 0.18510778035063205\n",
      "Epochh : 1230, Average Train Meta Loss : 0.19895775892909612, Average Test Meta Loss : 0.18360406443754923\n",
      "Epochh : 1240, Average Train Meta Loss : 0.19735455639813793, Average Test Meta Loss : 0.18212457977160962\n",
      "Epochh : 1250, Average Train Meta Loss : 0.195776982737451, Average Test Meta Loss : 0.18066886604823792\n",
      "Epochh : 1260, Average Train Meta Loss : 0.19422443860260108, Average Test Meta Loss : 0.1792361234478872\n",
      "Epochh : 1270, Average Train Meta Loss : 0.19269631556131178, Average Test Meta Loss : 0.17782592628016533\n",
      "Epochh : 1280, Average Train Meta Loss : 0.1911920673387583, Average Test Meta Loss : 0.17643774579381694\n",
      "Epochh : 1290, Average Train Meta Loss : 0.18971110639370245, Average Test Meta Loss : 0.17507107082978407\n",
      "Epochh : 1300, Average Train Meta Loss : 0.1882529126605878, Average Test Meta Loss : 0.1737254169528107\n",
      "Epochh : 1310, Average Train Meta Loss : 0.18681696422900831, Average Test Meta Loss : 0.17240028212051695\n",
      "Epochh : 1320, Average Train Meta Loss : 0.18540275559819894, Average Test Meta Loss : 0.17109520807008918\n",
      "Epochh : 1330, Average Train Meta Loss : 0.18400979726225875, Average Test Meta Loss : 0.16980974788590716\n",
      "Epochh : 1340, Average Train Meta Loss : 0.18263761530279637, Average Test Meta Loss : 0.16854345662277725\n",
      "Epochh : 1350, Average Train Meta Loss : 0.1812857456456497, Average Test Meta Loss : 0.16729591089213502\n",
      "Epochh : 1360, Average Train Meta Loss : 0.17995374164118266, Average Test Meta Loss : 0.16606669774089888\n",
      "Epochh : 1370, Average Train Meta Loss : 0.17864117173438346, Average Test Meta Loss : 0.16485541629540312\n",
      "Epochh : 1380, Average Train Meta Loss : 0.17734761001308808, Average Test Meta Loss : 0.16366167807887783\n",
      "Epochh : 1390, Average Train Meta Loss : 0.17607264801247208, Average Test Meta Loss : 0.16248510242952052\n",
      "Epochh : 1400, Average Train Meta Loss : 0.17481588405211934, Average Test Meta Loss : 0.16132533098613736\n",
      "Epochh : 1410, Average Train Meta Loss : 0.17357699649275501, Average Test Meta Loss : 0.16018199137209593\n",
      "Epochh : 1420, Average Train Meta Loss : 0.1723554835137175, Average Test Meta Loss : 0.15905474302772704\n",
      "Epochh : 1430, Average Train Meta Loss : 0.1711510494504827, Average Test Meta Loss : 0.1579432493809965\n",
      "Epochh : 1440, Average Train Meta Loss : 0.16996332741605516, Average Test Meta Loss : 0.15684720260915383\n",
      "Epochh : 1450, Average Train Meta Loss : 0.16879197593579529, Average Test Meta Loss : 0.15576626801206542\n",
      "Epochh : 1460, Average Train Meta Loss : 0.16763666715838144, Average Test Meta Loss : 0.15470010678106727\n",
      "Epochh : 1470, Average Train Meta Loss : 0.16649705699091555, Average Test Meta Loss : 0.1536484405245233\n",
      "Epochh : 1480, Average Train Meta Loss : 0.16537283764693034, Average Test Meta Loss : 0.15261097641463633\n",
      "Epochh : 1490, Average Train Meta Loss : 0.1642636972235322, Average Test Meta Loss : 0.1515874286740179\n",
      "Epochh : 1500, Average Train Meta Loss : 0.16316933548875484, Average Test Meta Loss : 0.15057751988504342\n",
      "Epochh : 1510, Average Train Meta Loss : 0.16208945920663356, Average Test Meta Loss : 0.14958097891114183\n",
      "Epochh : 1520, Average Train Meta Loss : 0.16102378686821123, Average Test Meta Loss : 0.1485975426379509\n",
      "Epochh : 1530, Average Train Meta Loss : 0.1599720327130907, Average Test Meta Loss : 0.14762695134253007\n",
      "Epochh : 1540, Average Train Meta Loss : 0.15893392741364562, Average Test Meta Loss : 0.14666896074871533\n",
      "Epochh : 1550, Average Train Meta Loss : 0.15790920843918313, Average Test Meta Loss : 0.14572332459628365\n",
      "Epochh : 1560, Average Train Meta Loss : 0.15689761837880375, Average Test Meta Loss : 0.14478979915773707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 1570, Average Train Meta Loss : 0.15589891160878663, Average Test Meta Loss : 0.14386815817924448\n",
      "Epochh : 1580, Average Train Meta Loss : 0.15491283374095324, Average Test Meta Loss : 0.14295817625749546\n",
      "Epochh : 1590, Average Train Meta Loss : 0.15393915157008053, Average Test Meta Loss : 0.1420596333808684\n",
      "Epochh : 1600, Average Train Meta Loss : 0.15297763454892604, Average Test Meta Loss : 0.14117231851248138\n",
      "Epochh : 1610, Average Train Meta Loss : 0.1520280527343751, Average Test Meta Loss : 0.1402960264886759\n",
      "Epochh : 1620, Average Train Meta Loss : 0.15109018691864953, Average Test Meta Loss : 0.13943053589557278\n",
      "Epochh : 1630, Average Train Meta Loss : 0.15016382158535843, Average Test Meta Loss : 0.13857616932442537\n",
      "Epochh : 1640, Average Train Meta Loss : 0.14924874850095357, Average Test Meta Loss : 0.13773170973508989\n",
      "Epochh : 1650, Average Train Meta Loss : 0.1483447585595782, Average Test Meta Loss : 0.13689749747385332\n",
      "Epochh : 1660, Average Train Meta Loss : 0.14745165345157213, Average Test Meta Loss : 0.13607331025339148\n",
      "Epochh : 1670, Average Train Meta Loss : 0.14656923789807527, Average Test Meta Loss : 0.13525899027537552\n",
      "Epochh : 1680, Average Train Meta Loss : 0.14569732097602967, Average Test Meta Loss : 0.13445435620687696\n",
      "Epochh : 1690, Average Train Meta Loss : 0.14483571736209278, Average Test Meta Loss : 0.13365923883543626\n",
      "Epochh : 1700, Average Train Meta Loss : 0.1439842434337794, Average Test Meta Loss : 0.13287347024250687\n",
      "Epochh : 1710, Average Train Meta Loss : 0.14314272312138768, Average Test Meta Loss : 0.13209688679292636\n",
      "Epochh : 1720, Average Train Meta Loss : 0.14231098307168646, Average Test Meta Loss : 0.1313293287719791\n",
      "Epochh : 1730, Average Train Meta Loss : 0.14148887066937416, Average Test Meta Loss : 0.1305706398694634\n",
      "Epochh : 1740, Average Train Meta Loss : 0.14067621709398387, Average Test Meta Loss : 0.1298206842002796\n",
      "Epochh : 1750, Average Train Meta Loss : 0.13987283610812457, Average Test Meta Loss : 0.12947117559318722\n",
      "Epochh : 1760, Average Train Meta Loss : 0.13908129231565972, Average Test Meta Loss : 0.1287359694301542\n",
      "Epochh : 1770, Average Train Meta Loss : 0.13829596600105107, Average Test Meta Loss : 0.12800909499389834\n",
      "Epochh : 1780, Average Train Meta Loss : 0.13751945938513616, Average Test Meta Loss : 0.12729034714115578\n",
      "Epochh : 1790, Average Train Meta Loss : 0.13675165354006177, Average Test Meta Loss : 0.12657962496788158\n",
      "Epochh : 1800, Average Train Meta Loss : 0.13599234663176074, Average Test Meta Loss : 0.1258767969494459\n",
      "Epochh : 1810, Average Train Meta Loss : 0.1352414228111427, Average Test Meta Loss : 0.12518172909310168\n",
      "Epochh : 1820, Average Train Meta Loss : 0.13449874767778022, Average Test Meta Loss : 0.1244942956116213\n",
      "Epochh : 1830, Average Train Meta Loss : 0.13376422199730087, Average Test Meta Loss : 0.12381437049095502\n",
      "Epochh : 1840, Average Train Meta Loss : 0.13303763742106345, Average Test Meta Loss : 0.12314183182879299\n",
      "Epochh : 1850, Average Train Meta Loss : 0.13231890373066202, Average Test Meta Loss : 0.12247656593574408\n",
      "Epochh : 1860, Average Train Meta Loss : 0.13160789426631211, Average Test Meta Loss : 0.121818443663309\n",
      "Epochh : 1870, Average Train Meta Loss : 0.13090451560681718, Average Test Meta Loss : 0.12116735639120851\n",
      "Epochh : 1880, Average Train Meta Loss : 0.13020858522986353, Average Test Meta Loss : 0.1205231918358524\n",
      "Epochh : 1890, Average Train Meta Loss : 0.12952001534036137, Average Test Meta Loss : 0.11988584024324714\n",
      "Epochh : 1900, Average Train Meta Loss : 0.12883868983432398, Average Test Meta Loss : 0.11925519439261985\n",
      "Epochh : 1910, Average Train Meta Loss : 0.12816456161083153, Average Test Meta Loss : 0.11863115018493135\n",
      "Epochh : 1920, Average Train Meta Loss : 0.12749738611067143, Average Test Meta Loss : 0.1180136024301657\n",
      "Epochh : 1930, Average Train Meta Loss : 0.12683712000074734, Average Test Meta Loss : 0.11740244988610891\n",
      "Epochh : 1940, Average Train Meta Loss : 0.12618365729550676, Average Test Meta Loss : 0.11679761854361972\n",
      "Epochh : 1950, Average Train Meta Loss : 0.12553689335682244, Average Test Meta Loss : 0.11619896663260842\n",
      "Epochh : 1960, Average Train Meta Loss : 0.12489673133145554, Average Test Meta Loss : 0.11560654053573453\n",
      "Epochh : 1970, Average Train Meta Loss : 0.12426306155446556, Average Test Meta Loss : 0.11502000739645768\n",
      "Epochh : 1980, Average Train Meta Loss : 0.12363578714500947, Average Test Meta Loss : 0.11443939150854072\n",
      "Epochh : 1990, Average Train Meta Loss : 0.12301481509429095, Average Test Meta Loss : 0.11386461039998264\n",
      "Epochh : 2000, Average Train Meta Loss : 0.12240004873525631, Average Test Meta Loss : 0.11329557186830518\n",
      "Epochh : 2010, Average Train Meta Loss : 0.12179139630561002, Average Test Meta Loss : 0.11273219388534864\n",
      "Epochh : 2020, Average Train Meta Loss : 0.12118878688010604, Average Test Meta Loss : 0.11217438991743545\n",
      "Epochh : 2030, Average Train Meta Loss : 0.12059209217414306, Average Test Meta Loss : 0.1116220800684944\n",
      "Epochh : 2040, Average Train Meta Loss : 0.12000124421712892, Average Test Meta Loss : 0.1110751811058581\n",
      "Epochh : 2050, Average Train Meta Loss : 0.11941615806859622, Average Test Meta Loss : 0.11053361516606089\n",
      "Epochh : 2060, Average Train Meta Loss : 0.11883682415708899, Average Test Meta Loss : 0.10999730882680434\n",
      "Epochh : 2070, Average Train Meta Loss : 0.11826301042392831, Average Test Meta Loss : 0.10946617755186665\n",
      "Epochh : 2080, Average Train Meta Loss : 0.11769471328060283, Average Test Meta Loss : 0.10894016521182663\n",
      "Epochh : 2090, Average Train Meta Loss : 0.11713185529372672, Average Test Meta Loss : 0.10841916978054444\n",
      "Epochh : 2100, Average Train Meta Loss : 0.11657435051685987, Average Test Meta Loss : 0.10790323224673026\n",
      "Epochh : 2110, Average Train Meta Loss : 0.11602213561774123, Average Test Meta Loss : 0.10739208575932319\n",
      "Epochh : 2120, Average Train Meta Loss : 0.1154751212866269, Average Test Meta Loss : 0.10688576266218852\n",
      "Epochh : 2130, Average Train Meta Loss : 0.11493323979633245, Average Test Meta Loss : 0.10638418736114862\n",
      "Epochh : 2140, Average Train Meta Loss : 0.1143964195700916, Average Test Meta Loss : 0.10588729799499981\n",
      "Epochh : 2150, Average Train Meta Loss : 0.11386459061354225, Average Test Meta Loss : 0.10539502789743797\n",
      "Epochh : 2160, Average Train Meta Loss : 0.11333768511203157, Average Test Meta Loss : 0.1049073171318611\n",
      "Epochh : 2170, Average Train Meta Loss : 0.11281563429665953, Average Test Meta Loss : 0.10442409595679489\n",
      "Epochh : 2180, Average Train Meta Loss : 0.1122983688529559, Average Test Meta Loss : 0.10394530615375297\n",
      "Epochh : 2190, Average Train Meta Loss : 0.11178586254177501, Average Test Meta Loss : 0.10347088677306501\n",
      "Epochh : 2200, Average Train Meta Loss : 0.11127797584452133, Average Test Meta Loss : 0.10300077829526806\n",
      "Epochh : 2210, Average Train Meta Loss : 0.11077468342465538, Average Test Meta Loss : 0.10253492287869923\n",
      "Epochh : 2220, Average Train Meta Loss : 0.11027592303425715, Average Test Meta Loss : 0.10207326198498093\n",
      "Epochh : 2230, Average Train Meta Loss : 0.10978163411937646, Average Test Meta Loss : 0.10161573963516757\n",
      "Epochh : 2240, Average Train Meta Loss : 0.10929175623431511, Average Test Meta Loss : 0.10116230036892714\n",
      "Epochh : 2250, Average Train Meta Loss : 0.10880623159053877, Average Test Meta Loss : 0.10071289021469693\n",
      "Epochh : 2260, Average Train Meta Loss : 0.1083250012360697, Average Test Meta Loss : 0.10026746515601777\n",
      "Epochh : 2270, Average Train Meta Loss : 0.10784800886001988, Average Test Meta Loss : 0.09982595697508717\n",
      "Epochh : 2280, Average Train Meta Loss : 0.10737519865048661, Average Test Meta Loss : 0.09938831578048689\n",
      "Epochh : 2290, Average Train Meta Loss : 0.10690651610264862, Average Test Meta Loss : 0.09895450328365704\n",
      "Epochh : 2300, Average Train Meta Loss : 0.10644190716817163, Average Test Meta Loss : 0.0985244532911623\n",
      "Epochh : 2310, Average Train Meta Loss : 0.10598131960233283, Average Test Meta Loss : 0.09809813359554401\n",
      "Epochh : 2320, Average Train Meta Loss : 0.10552471790345455, Average Test Meta Loss : 0.09767547928345614\n",
      "Epochh : 2330, Average Train Meta Loss : 0.10507202917707799, Average Test Meta Loss : 0.09725645763079514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 2340, Average Train Meta Loss : 0.10462319566696655, Average Test Meta Loss : 0.09684101054644416\n",
      "Epochh : 2350, Average Train Meta Loss : 0.10417821822746669, Average Test Meta Loss : 0.09642910020307728\n",
      "Epochh : 2360, Average Train Meta Loss : 0.10373697466248188, Average Test Meta Loss : 0.09602067570289777\n",
      "Epochh : 2370, Average Train Meta Loss : 0.10329945355980265, Average Test Meta Loss : 0.09561569605894196\n",
      "Epochh : 2380, Average Train Meta Loss : 0.10286560526354967, Average Test Meta Loss : 0.09521412204304344\n",
      "Epochh : 2390, Average Train Meta Loss : 0.10243538623556452, Average Test Meta Loss : 0.09481590321838353\n",
      "Epochh : 2400, Average Train Meta Loss : 0.10200874989434128, Average Test Meta Loss : 0.09442122925890191\n",
      "Epochh : 2410, Average Train Meta Loss : 0.10158565296663735, Average Test Meta Loss : 0.09402960295648603\n",
      "Epochh : 2420, Average Train Meta Loss : 0.10116605093046745, Average Test Meta Loss : 0.09364121137125328\n",
      "Epochh : 2430, Average Train Meta Loss : 0.1007499010089198, Average Test Meta Loss : 0.09325601516225351\n",
      "Epochh : 2440, Average Train Meta Loss : 0.10033716073460083, Average Test Meta Loss : 0.0928739749558277\n",
      "Epochh : 2450, Average Train Meta Loss : 0.09992778866981358, Average Test Meta Loss : 0.09249505260743415\n",
      "Epochh : 2460, Average Train Meta Loss : 0.09952174333549722, Average Test Meta Loss : 0.09211920940763714\n",
      "Epochh : 2470, Average Train Meta Loss : 0.09911906507161095, Average Test Meta Loss : 0.09174640822575164\n",
      "Epochh : 2480, Average Train Meta Loss : 0.09871955289021152, Average Test Meta Loss : 0.09137661732910626\n",
      "Epochh : 2490, Average Train Meta Loss : 0.09832324798097863, Average Test Meta Loss : 0.09100979029678821\n",
      "Epochh : 2500, Average Train Meta Loss : 0.09793011752062683, Average Test Meta Loss : 0.09064589678575044\n",
      "Epochh : 2510, Average Train Meta Loss : 0.09754011307022215, Average Test Meta Loss : 0.0902849033481717\n",
      "Epochh : 2520, Average Train Meta Loss : 0.09715320299932335, Average Test Meta Loss : 0.08992677212402826\n",
      "Epochh : 2530, Average Train Meta Loss : 0.09676955348340233, Average Test Meta Loss : 0.08957147146794293\n",
      "Epochh : 2540, Average Train Meta Loss : 0.09638872107876122, Average Test Meta Loss : 0.0892189724383189\n",
      "Epochh : 2550, Average Train Meta Loss : 0.09601087438685099, Average Test Meta Loss : 0.08886923137234488\n",
      "Epochh : 2560, Average Train Meta Loss : 0.09563597866375863, Average Test Meta Loss : 0.08852222155804314\n",
      "Epochh : 2570, Average Train Meta Loss : 0.09526399897974104, Average Test Meta Loss : 0.08817791218569841\n",
      "Epochh : 2580, Average Train Meta Loss : 0.09489490189776768, Average Test Meta Loss : 0.08783626989260232\n",
      "Epochh : 2590, Average Train Meta Loss : 0.0945286539460252, Average Test Meta Loss : 0.08749726488771363\n",
      "Epochh : 2600, Average Train Meta Loss : 0.09416522198492298, Average Test Meta Loss : 0.08716090496934374\n",
      "Epochh : 2610, Average Train Meta Loss : 0.09380457474425506, Average Test Meta Loss : 0.08682709693631209\n",
      "Epochh : 2620, Average Train Meta Loss : 0.09344669576078189, Average Test Meta Loss : 0.08649582447681306\n",
      "Epochh : 2630, Average Train Meta Loss : 0.09309152049164009, Average Test Meta Loss : 0.08616707098101513\n",
      "Epochh : 2640, Average Train Meta Loss : 0.09273903547354032, Average Test Meta Loss : 0.08584086885492155\n",
      "Epochh : 2650, Average Train Meta Loss : 0.09238921210350548, Average Test Meta Loss : 0.08551706326616831\n",
      "Epochh : 2660, Average Train Meta Loss : 0.09204201477133936, Average Test Meta Loss : 0.08519595283608142\n",
      "Epochh : 2670, Average Train Meta Loss : 0.09169742618882042, Average Test Meta Loss : 0.08487705018943094\n",
      "Epochh : 2680, Average Train Meta Loss : 0.0913554266260285, Average Test Meta Loss : 0.0845604632036762\n",
      "Epochh : 2690, Average Train Meta Loss : 0.09101594898467512, Average Test Meta Loss : 0.08424641939875052\n",
      "Epochh : 2700, Average Train Meta Loss : 0.09067897768851518, Average Test Meta Loss : 0.08393451114919458\n",
      "Epochh : 2710, Average Train Meta Loss : 0.09034449234162359, Average Test Meta Loss : 0.08362490423599246\n",
      "Epochh : 2720, Average Train Meta Loss : 0.09001246578772311, Average Test Meta Loss : 0.08331757316191006\n",
      "Epochh : 2730, Average Train Meta Loss : 0.08968287053288537, Average Test Meta Loss : 0.08301249252236904\n",
      "Epochh : 2740, Average Train Meta Loss : 0.08935568131314006, Average Test Meta Loss : 0.0827096380588365\n",
      "Epochh : 2750, Average Train Meta Loss : 0.08903087004794061, Average Test Meta Loss : 0.08240898577904052\n",
      "Epochh : 2760, Average Train Meta Loss : 0.08870841245105113, Average Test Meta Loss : 0.08211053442643111\n",
      "Epochh : 2770, Average Train Meta Loss : 0.08838828818050448, Average Test Meta Loss : 0.08181421410782855\n",
      "Epochh : 2780, Average Train Meta Loss : 0.08807046193232074, Average Test Meta Loss : 0.0815200242413137\n",
      "Epochh : 2790, Average Train Meta Loss : 0.08775491246261463, Average Test Meta Loss : 0.08122795189888375\n",
      "Epochh : 2800, Average Train Meta Loss : 0.08744161396437106, Average Test Meta Loss : 0.08093798025295061\n",
      "Epochh : 2810, Average Train Meta Loss : 0.0871305945981408, Average Test Meta Loss : 0.08065004727827117\n",
      "Epochh : 2820, Average Train Meta Loss : 0.08682173038651264, Average Test Meta Loss : 0.08036415560258584\n",
      "Epochh : 2830, Average Train Meta Loss : 0.08651504825282097, Average Test Meta Loss : 0.08008028368428088\n",
      "Epochh : 2840, Average Train Meta Loss : 0.0862105256698027, Average Test Meta Loss : 0.07979841959666907\n",
      "Epochh : 2850, Average Train Meta Loss : 0.08590814067098693, Average Test Meta Loss : 0.07951852956574045\n",
      "Epochh : 2860, Average Train Meta Loss : 0.08560786754767441, Average Test Meta Loss : 0.07924058993077898\n",
      "Epochh : 2870, Average Train Meta Loss : 0.08530968899239443, Average Test Meta Loss : 0.07896458941896481\n",
      "Epochh : 2880, Average Train Meta Loss : 0.08501357761276339, Average Test Meta Loss : 0.078690505332855\n",
      "Epochh : 2890, Average Train Meta Loss : 0.08471951487176933, Average Test Meta Loss : 0.0784183140361647\n",
      "Epochh : 2900, Average Train Meta Loss : 0.08442747932276692, Average Test Meta Loss : 0.0781479995782014\n",
      "Epochh : 2910, Average Train Meta Loss : 0.08413745129683797, Average Test Meta Loss : 0.07787954626307124\n",
      "Epochh : 2920, Average Train Meta Loss : 0.0838494152760757, Average Test Meta Loss : 0.07761294200400015\n",
      "Epochh : 2930, Average Train Meta Loss : 0.08356333743927849, Average Test Meta Loss : 0.07734814180039866\n",
      "Epochh : 2940, Average Train Meta Loss : 0.08327920679492241, Average Test Meta Loss : 0.07708514418554469\n",
      "Epochh : 2950, Average Train Meta Loss : 0.08299700437121209, Average Test Meta Loss : 0.07682392832508278\n",
      "Epochh : 2960, Average Train Meta Loss : 0.08271670412803485, Average Test Meta Loss : 0.0765644768596019\n",
      "Epochh : 2970, Average Train Meta Loss : 0.08243829046551025, Average Test Meta Loss : 0.07630677138536267\n",
      "Epochh : 2980, Average Train Meta Loss : 0.08216174501097623, Average Test Meta Loss : 0.07605079481810219\n",
      "Epochh : 2990, Average Train Meta Loss : 0.08188705164920052, Average Test Meta Loss : 0.07579653153563352\n",
      "Epochh : 3000, Average Train Meta Loss : 0.08161418578705569, Average Test Meta Loss : 0.07554396062537322\n",
      "Epochh : 3010, Average Train Meta Loss : 0.08134313236516583, Average Test Meta Loss : 0.07529306742166539\n",
      "Epochh : 3020, Average Train Meta Loss : 0.08107387340943399, Average Test Meta Loss : 0.07504383516148003\n",
      "Epochh : 3030, Average Train Meta Loss : 0.08080639114905828, Average Test Meta Loss : 0.07479624748312032\n",
      "Epochh : 3040, Average Train Meta Loss : 0.08054066808212161, Average Test Meta Loss : 0.07455028811443892\n",
      "Epochh : 3050, Average Train Meta Loss : 0.08027668712263196, Average Test Meta Loss : 0.07430594115742926\n",
      "Epochh : 3060, Average Train Meta Loss : 0.0800144307175141, Average Test Meta Loss : 0.0740631907107707\n",
      "Epochh : 3070, Average Train Meta Loss : 0.07975388227049922, Average Test Meta Loss : 0.07382202120773294\n",
      "Epochh : 3080, Average Train Meta Loss : 0.07949502514142058, Average Test Meta Loss : 0.07358241711541126\n",
      "Epochh : 3090, Average Train Meta Loss : 0.07923784293375322, Average Test Meta Loss : 0.07334436337175013\n",
      "Epochh : 3100, Average Train Meta Loss : 0.07898231941667878, Average Test Meta Loss : 0.07310784494842258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 3110, Average Train Meta Loss : 0.07872843941261837, Average Test Meta Loss : 0.07287284717351199\n",
      "Epochh : 3120, Average Train Meta Loss : 0.07847618553065241, Average Test Meta Loss : 0.07263935520962529\n",
      "Epochh : 3130, Average Train Meta Loss : 0.07822554330209175, Average Test Meta Loss : 0.07240735800255853\n",
      "Epochh : 3140, Average Train Meta Loss : 0.07797649670809581, Average Test Meta Loss : 0.07217683487678535\n",
      "Epochh : 3150, Average Train Meta Loss : 0.07772903120195694, Average Test Meta Loss : 0.07194777478521723\n",
      "Epochh : 3160, Average Train Meta Loss : 0.07748313178920734, Average Test Meta Loss : 0.07172016398244802\n",
      "Epochh : 3170, Average Train Meta Loss : 0.07723878497818777, Average Test Meta Loss : 0.0714939887608131\n",
      "Epochh : 3180, Average Train Meta Loss : 0.07699597207401958, Average Test Meta Loss : 0.07126923557959794\n",
      "Epochh : 3190, Average Train Meta Loss : 0.07675468110085419, Average Test Meta Loss : 0.07104589106770189\n",
      "Epochh : 3200, Average Train Meta Loss : 0.07651491176008116, Average Test Meta Loss : 0.07082394201959918\n",
      "Epochh : 3210, Average Train Meta Loss : 0.07627662179837545, Average Test Meta Loss : 0.07060338700283696\n",
      "Epochh : 3220, Average Train Meta Loss : 0.076039811795882, Average Test Meta Loss : 0.07038419190068042\n",
      "Epochh : 3230, Average Train Meta Loss : 0.07580446728399745, Average Test Meta Loss : 0.07016635163211866\n",
      "Epochh : 3240, Average Train Meta Loss : 0.07557057506819327, Average Test Meta Loss : 0.069949855642177\n",
      "Epochh : 3250, Average Train Meta Loss : 0.07533812180559189, Average Test Meta Loss : 0.06973469375860289\n",
      "Epochh : 3260, Average Train Meta Loss : 0.0751070941463215, Average Test Meta Loss : 0.06952084925157716\n",
      "Epochh : 3270, Average Train Meta Loss : 0.07487747995841591, Average Test Meta Loss : 0.06930831228036988\n",
      "Epochh : 3280, Average Train Meta Loss : 0.07464926457655977, Average Test Meta Loss : 0.06909707086205412\n",
      "Epochh : 3290, Average Train Meta Loss : 0.07442243611502339, Average Test Meta Loss : 0.06888711319031643\n",
      "Epochh : 3300, Average Train Meta Loss : 0.07419698216288984, Average Test Meta Loss : 0.06867842767042198\n",
      "Epochh : 3310, Average Train Meta Loss : 0.07397288980290567, Average Test Meta Loss : 0.06847100265494489\n",
      "Epochh : 3320, Average Train Meta Loss : 0.07375014699981407, Average Test Meta Loss : 0.06826482679658359\n",
      "Epochh : 3330, Average Train Meta Loss : 0.07352874160119707, Average Test Meta Loss : 0.06805988885974801\n",
      "Epochh : 3340, Average Train Meta Loss : 0.0733086615757821, Average Test Meta Loss : 0.0678561783687422\n",
      "Epochh : 3350, Average Train Meta Loss : 0.07308989508782501, Average Test Meta Loss : 0.06765368325580066\n",
      "Epochh : 3360, Average Train Meta Loss : 0.07287243418632147, Average Test Meta Loss : 0.06745239724483536\n",
      "Epochh : 3370, Average Train Meta Loss : 0.07265626064615957, Average Test Meta Loss : 0.06725230550061642\n",
      "Epochh : 3380, Average Train Meta Loss : 0.07244137480329556, Average Test Meta Loss : 0.06705339335263083\n",
      "Epochh : 3390, Average Train Meta Loss : 0.07222774646085259, Average Test Meta Loss : 0.06685566167617157\n",
      "Epochh : 3400, Average Train Meta Loss : 0.0720153744484963, Average Test Meta Loss : 0.06665908532539278\n",
      "Epochh : 3410, Average Train Meta Loss : 0.07180425008572786, Average Test Meta Loss : 0.06649107400520739\n",
      "Epochh : 3420, Average Train Meta Loss : 0.07159435751063278, Average Test Meta Loss : 0.06629671389251034\n",
      "Epochh : 3430, Average Train Meta Loss : 0.07138568969987748, Average Test Meta Loss : 0.06610348843967237\n",
      "Epochh : 3440, Average Train Meta Loss : 0.07117823346953324, Average Test Meta Loss : 0.06591139272561586\n",
      "Epochh : 3450, Average Train Meta Loss : 0.07097198148464696, Average Test Meta Loss : 0.06572040636214133\n",
      "Epochh : 3460, Average Train Meta Loss : 0.07076692310630799, Average Test Meta Loss : 0.06553051834025168\n",
      "Epochh : 3470, Average Train Meta Loss : 0.07056304260202424, Average Test Meta Loss : 0.06534173305123847\n",
      "Epochh : 3480, Average Train Meta Loss : 0.07036033440556655, Average Test Meta Loss : 0.06515402342204608\n",
      "Epochh : 3490, Average Train Meta Loss : 0.07015878661359862, Average Test Meta Loss : 0.06496738916132204\n",
      "Epochh : 3500, Average Train Meta Loss : 0.06995839019877692, Average Test Meta Loss : 0.0647818210733235\n",
      "Epochh : 3510, Average Train Meta Loss : 0.06975913531550601, Average Test Meta Loss : 0.06459731005686682\n",
      "Epochh : 3520, Average Train Meta Loss : 0.06956101228449288, Average Test Meta Loss : 0.06441384714099777\n",
      "Epochh : 3530, Average Train Meta Loss : 0.0693640114543976, Average Test Meta Loss : 0.06423142334124825\n",
      "Epochh : 3540, Average Train Meta Loss : 0.06916812325615325, Average Test Meta Loss : 0.06405002991888277\n",
      "Epochh : 3550, Average Train Meta Loss : 0.06897333842827445, Average Test Meta Loss : 0.06386965832445174\n",
      "Epochh : 3560, Average Train Meta Loss : 0.06877964761556678, Average Test Meta Loss : 0.06369029962752702\n",
      "Epochh : 3570, Average Train Meta Loss : 0.06858704382021302, Average Test Meta Loss : 0.06351194539602537\n",
      "Epochh : 3580, Average Train Meta Loss : 0.06839551340693067, Average Test Meta Loss : 0.06333458732918168\n",
      "Epochh : 3590, Average Train Meta Loss : 0.06820505008494902, Average Test Meta Loss : 0.06315821754976299\n",
      "Epochh : 3600, Average Train Meta Loss : 0.06801564425118023, Average Test Meta Loss : 0.0629828267871764\n",
      "Epochh : 3610, Average Train Meta Loss : 0.06782728807909907, Average Test Meta Loss : 0.06280840744034161\n",
      "Epochh : 3620, Average Train Meta Loss : 0.06763997165532604, Average Test Meta Loss : 0.06263495147992752\n",
      "Epochh : 3630, Average Train Meta Loss : 0.0674536870648978, Average Test Meta Loss : 0.0624624509267909\n",
      "Epochh : 3640, Average Train Meta Loss : 0.06726842569239976, Average Test Meta Loss : 0.06229089825652038\n",
      "Epochh : 3650, Average Train Meta Loss : 0.06708417949754475, Average Test Meta Loss : 0.062120287406366624\n",
      "Epochh : 3660, Average Train Meta Loss : 0.06690093952621369, Average Test Meta Loss : 0.06195060697347438\n",
      "Epochh : 3670, Average Train Meta Loss : 0.06671869780002175, Average Test Meta Loss : 0.06178185210639443\n",
      "Epochh : 3680, Average Train Meta Loss : 0.06653744743225125, Average Test Meta Loss : 0.061614016013626816\n",
      "Epochh : 3690, Average Train Meta Loss : 0.0663571794127251, Average Test Meta Loss : 0.06144708825350057\n",
      "Epochh : 3700, Average Train Meta Loss : 0.06628203415050535, Average Test Meta Loss : 0.06128106000860411\n",
      "Epochh : 3710, Average Train Meta Loss : 0.06610342520442059, Average Test Meta Loss : 0.06111592645981436\n",
      "Epochh : 3720, Average Train Meta Loss : 0.06592577742671306, Average Test Meta Loss : 0.060951682955815455\n",
      "Epochh : 3730, Average Train Meta Loss : 0.06574908008706024, Average Test Meta Loss : 0.06078831741586432\n",
      "Epochh : 3740, Average Train Meta Loss : 0.06557332761076767, Average Test Meta Loss : 0.06062582744294161\n",
      "Epochh : 3750, Average Train Meta Loss : 0.06539851205396428, Average Test Meta Loss : 0.06046420220389574\n",
      "Epochh : 3760, Average Train Meta Loss : 0.06522464193808498, Average Test Meta Loss : 0.06030343598504994\n",
      "Epochh : 3770, Average Train Meta Loss : 0.06505168321231829, Average Test Meta Loss : 0.060143522435931907\n",
      "Epochh : 3780, Average Train Meta Loss : 0.06487963441015128, Average Test Meta Loss : 0.059984459759537055\n",
      "Epochh : 3790, Average Train Meta Loss : 0.06470849571677999, Average Test Meta Loss : 0.05982623124803603\n",
      "Epochh : 3800, Average Train Meta Loss : 0.06453825620682149, Average Test Meta Loss : 0.059668835320372204\n",
      "Epochh : 3810, Average Train Meta Loss : 0.06436891039068007, Average Test Meta Loss : 0.05951226531635253\n",
      "Epochh : 3820, Average Train Meta Loss : 0.0642004588352319, Average Test Meta Loss : 0.05935651587213835\n",
      "Epochh : 3830, Average Train Meta Loss : 0.0640328811361709, Average Test Meta Loss : 0.05920160190781465\n",
      "Epochh : 3840, Average Train Meta Loss : 0.0638661735122758, Average Test Meta Loss : 0.059047476376182385\n",
      "Epochh : 3850, Average Train Meta Loss : 0.06370034018620772, Average Test Meta Loss : 0.05889415025527299\n",
      "Epochh : 3860, Average Train Meta Loss : 0.06353535646163296, Average Test Meta Loss : 0.058741614272545595\n",
      "Epochh : 3870, Average Train Meta Loss : 0.06337122482815136, Average Test Meta Loss : 0.058589866786270345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 3880, Average Train Meta Loss : 0.06320793909084642, Average Test Meta Loss : 0.05843890094707474\n",
      "Epochh : 3890, Average Train Meta Loss : 0.06304549258224001, Average Test Meta Loss : 0.058288726586244136\n",
      "Epochh : 3900, Average Train Meta Loss : 0.06288387893642539, Average Test Meta Loss : 0.05813932881718832\n",
      "Epochh : 3910, Average Train Meta Loss : 0.0627230917369966, Average Test Meta Loss : 0.05799067295107936\n",
      "Epochh : 3920, Average Train Meta Loss : 0.06256312552024956, Average Test Meta Loss : 0.05784277530989473\n",
      "Epochh : 3930, Average Train Meta Loss : 0.062403972313652495, Average Test Meta Loss : 0.057695630117260546\n",
      "Epochh : 3940, Average Train Meta Loss : 0.06224562746047732, Average Test Meta Loss : 0.05754923174273976\n",
      "Epochh : 3950, Average Train Meta Loss : 0.062088083504336825, Average Test Meta Loss : 0.05740357437056055\n",
      "Epochh : 3960, Average Train Meta Loss : 0.06193133501413306, Average Test Meta Loss : 0.05725865246777036\n",
      "Epochh : 3970, Average Train Meta Loss : 0.061775376002614873, Average Test Meta Loss : 0.05711446066566396\n",
      "Epochh : 3980, Average Train Meta Loss : 0.061620200545680764, Average Test Meta Loss : 0.05697099360926066\n",
      "Epochh : 3990, Average Train Meta Loss : 0.06146580520855654, Average Test Meta Loss : 0.05682824561885143\n",
      "Epochh : 4000, Average Train Meta Loss : 0.061312196228682135, Average Test Meta Loss : 0.0566862140782759\n",
      "Epochh : 4010, Average Train Meta Loss : 0.06115935636343988, Average Test Meta Loss : 0.0565449427518044\n",
      "Epochh : 4020, Average Train Meta Loss : 0.061007257559958, Average Test Meta Loss : 0.05640432015368837\n",
      "Epochh : 4030, Average Train Meta Loss : 0.060855913102076345, Average Test Meta Loss : 0.05626439482745757\n",
      "Epochh : 4040, Average Train Meta Loss : 0.06070531789937677, Average Test Meta Loss : 0.0561251615217515\n",
      "Epochh : 4050, Average Train Meta Loss : 0.060555465226345445, Average Test Meta Loss : 0.05598661508503093\n",
      "Epochh : 4060, Average Train Meta Loss : 0.06040635056399105, Average Test Meta Loss : 0.055848751140892064\n",
      "Epochh : 4070, Average Train Meta Loss : 0.06025796850277257, Average Test Meta Loss : 0.05571156439725653\n",
      "Epochh : 4080, Average Train Meta Loss : 0.06011031359473274, Average Test Meta Loss : 0.055575049907175725\n",
      "Epochh : 4090, Average Train Meta Loss : 0.059963382872875656, Average Test Meta Loss : 0.05543920298476475\n",
      "Epochh : 4100, Average Train Meta Loss : 0.05981716653266542, Average Test Meta Loss : 0.055304018434477416\n",
      "Epochh : 4110, Average Train Meta Loss : 0.05967166138637211, Average Test Meta Loss : 0.05516949222921601\n",
      "Epochh : 4120, Average Train Meta Loss : 0.05952686240489575, Average Test Meta Loss : 0.05503561818841135\n",
      "Epochh : 4130, Average Train Meta Loss : 0.059382764478269276, Average Test Meta Loss : 0.054902392384669346\n",
      "Epochh : 4140, Average Train Meta Loss : 0.05923936249255159, Average Test Meta Loss : 0.05476980994601425\n",
      "Epochh : 4150, Average Train Meta Loss : 0.05909665143162171, Average Test Meta Loss : 0.05463786637729211\n",
      "Epochh : 4160, Average Train Meta Loss : 0.058954626327965415, Average Test Meta Loss : 0.05450655693620594\n",
      "Epochh : 4170, Average Train Meta Loss : 0.05881328243572883, Average Test Meta Loss : 0.054375877117871195\n",
      "Epochh : 4180, Average Train Meta Loss : 0.05867261447066774, Average Test Meta Loss : 0.05424582479188688\n",
      "Epochh : 4190, Average Train Meta Loss : 0.058532617840853146, Average Test Meta Loss : 0.05411639075104948\n",
      "Epochh : 4200, Average Train Meta Loss : 0.058393287644560476, Average Test Meta Loss : 0.05398757287284317\n",
      "Epochh : 4210, Average Train Meta Loss : 0.05825461919344213, Average Test Meta Loss : 0.053859372845859114\n",
      "Epochh : 4220, Average Train Meta Loss : 0.05811660777825105, Average Test Meta Loss : 0.05373177435416972\n",
      "Epochh : 4230, Average Train Meta Loss : 0.057979249178824556, Average Test Meta Loss : 0.05360477892053925\n",
      "Epochh : 4240, Average Train Meta Loss : 0.05784253790983805, Average Test Meta Loss : 0.053478382365730125\n",
      "Epochh : 4250, Average Train Meta Loss : 0.057706470028347265, Average Test Meta Loss : 0.05335258047825536\n",
      "Epochh : 4260, Average Train Meta Loss : 0.05757104155396387, Average Test Meta Loss : 0.053227369071608015\n",
      "Epochh : 4270, Average Train Meta Loss : 0.05743624633110942, Average Test Meta Loss : 0.053102744628111605\n",
      "Epochh : 4280, Average Train Meta Loss : 0.05730208084883591, Average Test Meta Loss : 0.05297870179165139\n",
      "Epochh : 4290, Average Train Meta Loss : 0.057168540797714724, Average Test Meta Loss : 0.052855237096773956\n",
      "Epochh : 4300, Average Train Meta Loss : 0.057035621614276215, Average Test Meta Loss : 0.05273234652092028\n",
      "Epochh : 4310, Average Train Meta Loss : 0.05690331908885858, Average Test Meta Loss : 0.052610026074812015\n",
      "Epochh : 4320, Average Train Meta Loss : 0.05677162944240005, Average Test Meta Loss : 0.052488271842990535\n",
      "Epochh : 4330, Average Train Meta Loss : 0.05664056925577507, Average Test Meta Loss : 0.05236707981919942\n",
      "Epochh : 4340, Average Train Meta Loss : 0.05651009109770945, Average Test Meta Loss : 0.05224644617834318\n",
      "Epochh : 4350, Average Train Meta Loss : 0.05638021270097929, Average Test Meta Loss : 0.05212636701662702\n",
      "Epochh : 4360, Average Train Meta Loss : 0.056250929941405195, Average Test Meta Loss : 0.0520068385597172\n",
      "Epochh : 4370, Average Train Meta Loss : 0.05612223876324899, Average Test Meta Loss : 0.051887857003978174\n",
      "Epochh : 4380, Average Train Meta Loss : 0.05599413504619975, Average Test Meta Loss : 0.05176941861776656\n",
      "Epochh : 4390, Average Train Meta Loss : 0.05586661482599573, Average Test Meta Loss : 0.05165151969168796\n",
      "Epochh : 4400, Average Train Meta Loss : 0.055739680285318505, Average Test Meta Loss : 0.051534156559506925\n",
      "Epochh : 4410, Average Train Meta Loss : 0.055613318855967264, Average Test Meta Loss : 0.05141732556205998\n",
      "Epochh : 4420, Average Train Meta Loss : 0.05548752534235023, Average Test Meta Loss : 0.051301023305840944\n",
      "Epochh : 4430, Average Train Meta Loss : 0.055362299778459474, Average Test Meta Loss : 0.05118524634884496\n",
      "Epochh : 4440, Average Train Meta Loss : 0.05523763964809882, Average Test Meta Loss : 0.051069990595459226\n",
      "Epochh : 4450, Average Train Meta Loss : 0.05511353818148121, Average Test Meta Loss : 0.050955261152190824\n",
      "Epochh : 4460, Average Train Meta Loss : 0.05498999905356155, Average Test Meta Loss : 0.05084104566166511\n",
      "Epochh : 4470, Average Train Meta Loss : 0.05486700651996955, Average Test Meta Loss : 0.05072733411875255\n",
      "Epochh : 4480, Average Train Meta Loss : 0.054744564908891756, Average Test Meta Loss : 0.05061413038528841\n",
      "Epochh : 4490, Average Train Meta Loss : 0.0546226667108816, Average Test Meta Loss : 0.05050143185074013\n",
      "Epochh : 4500, Average Train Meta Loss : 0.05450131042174829, Average Test Meta Loss : 0.050389231848532993\n",
      "Epochh : 4510, Average Train Meta Loss : 0.0543804922967992, Average Test Meta Loss : 0.050277528831458446\n",
      "Epochh : 4520, Average Train Meta Loss : 0.05426020863682762, Average Test Meta Loss : 0.05016632142120242\n",
      "Epochh : 4530, Average Train Meta Loss : 0.05414045562605027, Average Test Meta Loss : 0.050055603437701865\n",
      "Epochh : 4540, Average Train Meta Loss : 0.05402122978241737, Average Test Meta Loss : 0.04994537310407783\n",
      "Epochh : 4550, Average Train Meta Loss : 0.05390252790924932, Average Test Meta Loss : 0.04983562723498161\n",
      "Epochh : 4560, Average Train Meta Loss : 0.05378434658467478, Average Test Meta Loss : 0.04972636270329842\n",
      "Epochh : 4570, Average Train Meta Loss : 0.0536666822976469, Average Test Meta Loss : 0.04961757608745909\n",
      "Epochh : 4580, Average Train Meta Loss : 0.05354953175387163, Average Test Meta Loss : 0.04950926457067983\n",
      "Epochh : 4590, Average Train Meta Loss : 0.05343289167823142, Average Test Meta Loss : 0.04940142762689962\n",
      "Epochh : 4600, Average Train Meta Loss : 0.05331675851017147, Average Test Meta Loss : 0.04929405665134968\n",
      "Epochh : 4610, Average Train Meta Loss : 0.05320112901877637, Average Test Meta Loss : 0.049187151304444454\n",
      "Epochh : 4620, Average Train Meta Loss : 0.0530860000259671, Average Test Meta Loss : 0.04908070865168517\n",
      "Epochh : 4630, Average Train Meta Loss : 0.05297136919095975, Average Test Meta Loss : 0.04897472570760702\n",
      "Epochh : 4640, Average Train Meta Loss : 0.052857231357116376, Average Test Meta Loss : 0.048869199474892266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh : 4650, Average Train Meta Loss : 0.05274358433810525, Average Test Meta Loss : 0.04876412705469837\n",
      "Epochh : 4660, Average Train Meta Loss : 0.05263042496395426, Average Test Meta Loss : 0.04865950545835151\n",
      "Epochh : 4670, Average Train Meta Loss : 0.052517750127859725, Average Test Meta Loss : 0.04855533184125354\n",
      "Epochh : 4680, Average Train Meta Loss : 0.05240555669259237, Average Test Meta Loss : 0.04845160329722422\n",
      "Epochh : 4690, Average Train Meta Loss : 0.052293841585595274, Average Test Meta Loss : 0.048348316997294624\n",
      "Epochh : 4700, Average Train Meta Loss : 0.05218260184003521, Average Test Meta Loss : 0.04824547086344162\n",
      "Epochh : 4710, Average Train Meta Loss : 0.052071834271636926, Average Test Meta Loss : 0.0481430608341915\n",
      "Epochh : 4720, Average Train Meta Loss : 0.051961536252116906, Average Test Meta Loss : 0.048041084529014816\n",
      "Epochh : 4730, Average Train Meta Loss : 0.05185170902251663, Average Test Meta Loss : 0.0479395393379887\n",
      "Epochh : 4740, Average Train Meta Loss : 0.05174234252177303, Average Test Meta Loss : 0.04783842517374463\n",
      "Epochh : 4750, Average Train Meta Loss : 0.05163343420243585, Average Test Meta Loss : 0.0477377338980943\n",
      "Epochh : 4760, Average Train Meta Loss : 0.051524983420794694, Average Test Meta Loss : 0.04763746561035492\n",
      "Epochh : 4770, Average Train Meta Loss : 0.05141698725425699, Average Test Meta Loss : 0.04753761766036413\n",
      "Epochh : 4780, Average Train Meta Loss : 0.05130944283769554, Average Test Meta Loss : 0.04743818741227234\n",
      "Epochh : 4790, Average Train Meta Loss : 0.05120234749504652, Average Test Meta Loss : 0.04733917221332278\n",
      "Epochh : 4800, Average Train Meta Loss : 0.0510956988282227, Average Test Meta Loss : 0.04724057767626351\n",
      "Epochh : 4810, Average Train Meta Loss : 0.05098949487671154, Average Test Meta Loss : 0.04714238520204742\n",
      "Epochh : 4820, Average Train Meta Loss : 0.05088372949992796, Average Test Meta Loss : 0.04704459971119828\n",
      "Epochh : 4830, Average Train Meta Loss : 0.0507784035213143, Average Test Meta Loss : 0.0469472193071844\n",
      "Epochh : 4840, Average Train Meta Loss : 0.05067352935600754, Average Test Meta Loss : 0.04685024315406729\n",
      "Epochh : 4850, Average Train Meta Loss : 0.050569070971428286, Average Test Meta Loss : 0.046753670524046054\n",
      "Epochh : 4860, Average Train Meta Loss : 0.05046504112208247, Average Test Meta Loss : 0.046657489427753385\n",
      "Epochh : 4870, Average Train Meta Loss : 0.05036143834669679, Average Test Meta Loss : 0.04656170316715421\n",
      "Epochh : 4880, Average Train Meta Loss : 0.05025825982109461, Average Test Meta Loss : 0.04646630938896081\n",
      "Epochh : 4890, Average Train Meta Loss : 0.050155503219531834, Average Test Meta Loss : 0.046371305692409166\n",
      "Epochh : 4900, Average Train Meta Loss : 0.050053165960406494, Average Test Meta Loss : 0.04627668971128298\n",
      "Epochh : 4910, Average Train Meta Loss : 0.049951245450131895, Average Test Meta Loss : 0.04618245904988758\n",
      "Epochh : 4920, Average Train Meta Loss : 0.049849739167463235, Average Test Meta Loss : 0.046088611341991555\n",
      "Epochh : 4930, Average Train Meta Loss : 0.04974864458846981, Average Test Meta Loss : 0.045995144283857\n",
      "Epochh : 4940, Average Train Meta Loss : 0.04964795922509169, Average Test Meta Loss : 0.04590205556857308\n",
      "Epochh : 4950, Average Train Meta Loss : 0.04954768060293815, Average Test Meta Loss : 0.04580934287916211\n",
      "Epochh : 4960, Average Train Meta Loss : 0.049447806280722796, Average Test Meta Loss : 0.045717003950049274\n",
      "Epochh : 4970, Average Train Meta Loss : 0.04934833374748029, Average Test Meta Loss : 0.0456250365313314\n",
      "Epochh : 4980, Average Train Meta Loss : 0.04924926062266228, Average Test Meta Loss : 0.04553343838986994\n",
      "Epochh : 4990, Average Train Meta Loss : 0.04915058451396813, Average Test Meta Loss : 0.04544220741036733\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # Sample a task at random 0-18 regression tasks --> T, task should only change per epoch, so it is only updated here\n",
    "    task = random.randint(0, number_of_tasks-1) #Note that for this problem 'task' must be passed to the evaluation function --> graph.y has all the selection targets, we only use the one specified by task\n",
    "\n",
    "    # Empty list\n",
    "    graph = []\n",
    "\n",
    "    for i in range(K): #Store graphs\n",
    "      graph.append(random.sample(GRAPH_TRAIN, 1)[0])\n",
    "\n",
    "    # Create graph mini batch from list\n",
    "    graph = create_mini_batch(graph)\n",
    "\n",
    "    # Update model predefined number of times based on k\n",
    "    new_model = training(model, graph, lr_k, k,task)\n",
    "\n",
    "    # Evalaute the loss for the training data\n",
    "    train_set_evaluation(new_model,graph,store_train_loss_meta,task)     \n",
    "    \n",
    "    #Meta-update --> Get gradient for meta loop and update\n",
    "    metaupdate(model,new_model,metaoptimizer)\n",
    "    \n",
    "    # Evalaute the loss for the test data\n",
    "    # Note that we need to sample the graph from the test data\n",
    "\n",
    "    graph = []\n",
    "    for i in range(K): #Store graphs\n",
    "      graph.append(random.sample(GRAPH_TEST, 1)[0])\n",
    "    graph = create_mini_batch(graph)\n",
    "\n",
    "    test_set_validation(model,new_model,graph,lr_k,k,store_test_loss_meta,task)\n",
    "\n",
    "    # Print losses every 'printing_step' epochs\n",
    "    print_losses(epoch,store_train_loss_meta,store_test_loss_meta,printing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQjoz6FYctJM",
   "metadata": {
    "id": "bQjoz6FYctJM"
   },
   "source": [
    "<h1> Few Shot learning with new meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m-SPUG5Bfpe9",
   "metadata": {
    "id": "m-SPUG5Bfpe9"
   },
   "source": [
    "The model performs good few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "GY84TNs8JXVH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "GY84TNs8JXVH",
    "outputId": "3021d8a8-5374-449e-c556-bce7b0e9d0b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'k shots')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgDElEQVR4nO3deXRV9d3v8fc3E2EOhDAmEIZECxVBI6KojEGttWhrfbAOaG15nCqKq1d7133qtc/qXe29FRS0TlWLQ6s+rU/FoZUwCYgMQRFFJQNjGMMUCBBIwu/+cTY2xYScJCdnn+HzWiuLffbe55zPz4Of/NjnnL3NOYeIiMSHBL8DiIhI+Kj0RUTiiEpfRCSOqPRFROKISl9EJI4k+R3gTLp16+ays7P9jiEiElXWrFmz1zmXUd+2iC797OxsCgsL/Y4hIhJVzGxLQ9t0eEdEJI6o9EVE4ohKX0QkjkT0MX0Rkeaqrq6mrKyMqqoqv6O0mtTUVDIzM0lOTg76Pip9EYlJZWVldOzYkezsbMzM7zgh55xj3759lJWV0b9//6Dvp8M7IhKTqqqqSE9Pj8nCBzAz0tPTm/wvGZW+iMSsWC38U5ozvpgs/T2Hq/jV219QcbTa7ygiIhElJkt//5ETvLh8E88t3eh3FBGJYx06dPA7wjfEZOmf3bMTV53Tixc+3MS+yuN+xxERiRgxWfoA903Ipaq6lmeWaLYvIpFj7dq1jBw5kqFDh3Lttddy4MABAGbNmsXgwYMZOnQokydPBuCDDz5g2LBhDBs2jOHDh3P48OEWP3/MfmRzUPcOXDO8Dy99tJmfXNKf7p1S/Y4kIj555O31fLHjUEgfc3DvTjx89ZAm3++WW25h9uzZjB49ml/+8pc88sgjPPbYY/zmN79h06ZNtGnThoMHDwLwu9/9jieffJJRo0ZRWVlJamrLeyxmZ/oA08bnUF3r+P3iUr+jiIhQUVHBwYMHGT16NABTpkxhyZIlAAwdOpQbb7yRV155haSkwHx81KhRTJ8+nVmzZnHw4MGv17dEzM70Afqlt+f6vEz+tHIrUy8bQO+0tn5HEhEfNGdGHm7vvvsuS5Ys4e233+bXv/41n332GQ899BBXXXUV7733HqNGjeL999/n7LPPbtHzxPRMH+CecTkAzF5Y4nMSEYl3nTt3pkuXLixduhSAl19+mdGjR3Py5Em2bdvG2LFj+e1vf0tFRQWVlZWUlpZyzjnn8OCDD3LBBRfw1VdftThDTM/0AfqkteWGEVm8unIrd44eSN/0dn5HEpE4cfToUTIzM7++PX36dObMmcMdd9zB0aNHGTBgAC+++CK1tbXcdNNNVFRU4Jzj3nvvJS0tjf/4j/9g0aJFJCQkMGTIEK688soWZ4r50ge4e+wgXlu9jVkLi/ndD8/1O46IxImTJ0/Wu37FihXfWLds2bJvrJs9e3bIM8X84R2A7p1SuXlkP978uIzS8kq/44iI+CYuSh/gjjEDSU1O5PH5xX5HERHxTdyUfrcObbj14mzeXreDDbta/gUHEYl8zjm/I7Sq5owvbkofYOplA+iQksTMgiK/o4hIK0tNTWXfvn0xW/ynzqff1C9sBf1GrpklAoXAdufcd82sP/AakA6sAW52zp0wszbAS8D5wD7g35xzm73H+AVwO1AL3Ouce79JaVsorV0Kt1/an8fmF/P59gq+3adzOJ9eRMIoMzOTsrIyysvL/Y7Sak5dOaspmvLpnWnAl0An7/ZvgZnOudfM7GkCZf6U9+cB59wgM5vs7fdvZjYYmAwMAXoD880s1zlX26TELfTjS/rzx+WbmVFQxAu3XhDOpxaRMEpOTm7SFaXiRVCHd8wsE7gK+IN324BxwF+8XeYA13jLk7zbeNvHe/tPAl5zzh13zm0CSoARIRhDk3RKTWbqZQNY+NUePt56INxPLyLiq2CP6T8G/A/g1IdO04GDzrka73YZ0Mdb7gNsA/C2V3j7f72+nvt8zcymmlmhmRW21j/LplyUTXr7FGbM07F9EYkvjZa+mX0X2OOcWxOGPDjnnnXO5Tnn8jIyMlrlOdq3SeLOMQNZVrKXFRv3tcpziIhEomBm+qOA75nZZgJv3I4DHgfSzOzUewKZwHZveTuQBeBt70zgDd2v19dzn7C7aWQ/enRqw4x5RTH77r6IyOkaLX3n3C+cc5nOuWwCb8QudM7dCCwCrvN2mwK85S3P9W7jbV/oAq06F5hsZm28T/7kAKtCNpImSk1O5J6xg1i1eT/LSvb6FUNEJKxa8jn9B4HpZlZC4Jj9897654F0b/104CEA59x64A3gC+AfwN3h/uTO6a6/IIs+aW15VLN9EYkTFslll5eX5woLC1v1OV5btZWH3vyM56fkMf5bPVr1uUREwsHM1jjn8urbFlffyK3PD87PpF96O2YUFHHyZOT+AhQRCYW4L/3kxASmjc9h/Y5DvL9+l99xRERaVdyXPsCkYX0YmNGemfOLqNVsX0RimEofSEww7s/PpWh3Je+s2+F3HBGRVqPS93zn2704u2dHHptfTE1t/Ve7ERGJdip9T0KCMT0/l017j/Dfn/j2nTERkVal0q8jf3APhmZ25vEFxZyo0WxfRGKPSr8Os8Bsv+zAMf5rzbbG7yAiEmVU+qcZnZvB+f26MHtBCVXVvn5hWEQk5FT6pzEzHpiYy65DVfx51Va/44iIhJRKvx4XD+zGRQPSeXJRKcdOaLYvIrFDpd+ABybmsrfyOC99tNnvKCIiIaPSb0BedldG52bw9AelVB6vafwOIiJRQKV/BtPzczlwtJoXl23yO4qISEio9M/g3Kw08gf34NmlG6k4Wu13HBGRFlPpN2J6fi6Hq2r4w7KNfkcREWkxlX4jvtWrE1cN7cULyzax/8gJv+OIiLSISj8I90/I4Vh1Lc8sKfU7iohIi6j0gzCoe0cmDevDnOWb2XO4yu84IiLNptIP0rTxOVTXOp5arNm+iEQvlX6Qsru157rzMnl1xVZ2VhzzO46ISLOo9JvgZ+MH4XA8sbDE7ygiIs2i0m+CzC7tmHxBX15fvY1t+4/6HUdEpMlU+k1099hBJCQYsxYU+x1FRKTJVPpN1LNzKjeP7Mebn2xnY3ml33FERJpEpd8Md44ZSEpiAo9rti8iUUal3wzdOrTh1lHZzP10B0W7D/sdR0QkaCr9Zpp66QDapyQxs6DI7ygiIkFT6TdTl/Yp3H5Jf/7++S7W76jwO46ISFBU+i1w+6X96dw2WbN9EYkaKv0W6JSazNTLBjD/yz18svWA33FERBql0m+hWy/Opmv7FGZoti8iUUCl30Lt2yRx5+iBLC3ey6pN+/2OIyJyRir9ELhpZD+6d2zD7+ZtwDnndxwRkQap9EOgbUoid48dxKpN+/mwZJ/fcUREGqTSD5HJI7Lo3TmVRws02xeRyNVo6ZtZqpmtMrNPzWy9mT3ire9vZivNrMTMXjezFG99G+92ibc9u85j/cJbv8HMLm+1UfmgTVIiPxufwydbD7J4Q7nfcURE6hXMTP84MM45dy4wDLjCzEYCvwVmOucGAQeA2739bwcOeOtnevthZoOBycAQ4Arg92aWGMKx+O668zPp27WdZvsiErEaLX0XcOp0ksnejwPGAX/x1s8BrvGWJ3m38baPNzPz1r/mnDvunNsElAAjQjGISJGcmMC08Tl8vv0Q76/f7XccEZFvCOqYvpklmtlaYA9QAJQCB51zNd4uZUAfb7kPsA3A214BpNddX899YsY1w/swIKM9MwuKOHlSs30RiSxBlb5zrtY5NwzIJDA7P7u1ApnZVDMrNLPC8vLoOzaemGDcPyGXDbsP885nO/2OIyLyL5r06R3n3EFgEXARkGZmSd6mTGC7t7wdyALwtncG9tVdX8996j7Hs865POdcXkZGRlPiRYyrzunF2T078tj8ImpqT/odR0Tka8F8eifDzNK85bZAPvAlgfK/ztttCvCWtzzXu423faELvKs5F5jsfbqnP5ADrArROCJKQoJx34RcNpYf4W9rd/gdR0Tka8HM9HsBi8xsHbAaKHDOvQM8CEw3sxICx+yf9/Z/Hkj31k8HHgJwzq0H3gC+AP4B3O2cqw3lYCLJ5UN68O0+nXh8QRHVmu2LSISwSP5oYV5enissLPQ7RrMt+moPt/1xNf/n2nP40YV9/Y4jInHCzNY45/Lq26Zv5LaiMWdlcF7fNGYvLKaqOmb/USMiUUSl34rMjAcmnsXOiipeW7XV7zgiIir91nbxwHQu7N+VJxeXcuyEZvsi4i+Vfis7NdsvP3ycV1Zs8TuOiMQ5lX4YjOjflUtzuvHUB6VUHq9p/A4iIq1EpR8mD0w8i/1HTjBn+Wa/o4hIHFPph8mwrDQmfKs7z3xQSsWxar/jiEicUumH0f35uRyqquH5ZZv8jiIicUqlH0ZDenfmO+f05IVlmzhw5ITfcUQkDqn0w+y+CbkcOVHDM0s2+h1FROKQSj/Mcnt0ZNK5vZmzfDPlh4/7HUdE4oxK3wfTJuRyovYkTy0u9TuKiMQZlb4P+ndrzw/O68MrK7ewq6LK7zgiEkdU+j752bgcnHM8sajY7ygiEkdU+j7J6tqO6/OyeH31NsoOHPU7jojECZW+j+4ZNwgzY/aCEr+jiEicUOn7qFfnttx4YV/+8nEZm/ce8TuOiMQBlb7P7hwzkJTEBB5foGP7ItL6VPo+694xlVsu7sff1m6nePdhv+OISIxT6UeAOy4bSPuUJB6br9m+iLQulX4E6NI+hR+Pyubdz3ayfkeF33FEJIap9CPE7ZcOoFNqEjMLNNsXkdaj0o8QndsmM/WyAcz/cjefbjvodxwRiVEq/Qhy66j+dGmXzKMFRX5HEZEYpdKPIB3aJHHnmIEsKSpn9eb9fscRkRik0o8wN4/MJqNjGx6dt8HvKCISg1T6EaZtSiJ3jRnIio37WV6y1+84IhJjVPoR6IYRfenVOZVHC4pwzvkdR0RiiEo/AqUmJ3LPuEGs2XKAxUXlfscRkRii0o9QPzw/i6yubZkxT7N9EQkdlX6ESklK4N5xOXy2vYJ5X+z2O46IxAiVfgS7dngfBnRrz8yCIk6e1GxfRFpOpR/BkhITmDYhh692Hebdz3b6HUdEYoBKP8JdPbQ3uT068Nj8Imo12xeRFlLpR7iEBGN6fi6l5Ud4a+12v+OISJRT6UeBy4f0ZEjvTjw2v5jq2pN+xxGRKKbSjwJmxgMTc9m6/yh/XVPmdxwRiWKNlr6ZZZnZIjP7wszWm9k0b31XMysws2Lvzy7eejOzWWZWYmbrzOy8Oo81xdu/2MymtN6wYs/Ys7ozLCuNWQuKOV5T63ccEYlSwcz0a4AHnHODgZHA3WY2GHgIWOCcywEWeLcBrgRyvJ+pwFMQ+CUBPAxcCIwAHj71i0Iad2q2v6OiitdXb/M7johEqUZL3zm30zn3sbd8GPgS6ANMAuZ4u80BrvGWJwEvuYAVQJqZ9QIuBwqcc/udcweAAuCKUA4m1l0yqBsj+nfliYUlVFVrti8iTdekY/pmlg0MB1YCPZxzpz48vgvo4S33AepORcu8dQ2tP/05pppZoZkVlpfrvDN1mRkP5Oey5/BxXlmxxe84IhKFgi59M+sA/BW4zzl3qO42Fzg5TEg+RO6ce9Y5l+ecy8vIyAjFQ8aUCwekc2lON55aXMqR4zV+xxGRKBNU6ZtZMoHCf9U596a3erd32Abvzz3e+u1AVp27Z3rrGlovTTQ9P5d9R07wx+Wb/Y4iIlEmmE/vGPA88KVzbkadTXOBU5/AmQK8VWf9Ld6neEYCFd5hoPeBiWbWxXsDd6K3TppoeN8ujD+7O88u2cihqmq/44hIFAlmpj8KuBkYZ2ZrvZ/vAL8B8s2sGJjg3QZ4D9gIlADPAXcBOOf2A/8JrPZ+fuWtk2a4Pz+XimPVPL90k99RRCSKWCSfqz0vL88VFhb6HSNi3fHyGj4s2cvSB8eS1i7F7zgiEiHMbI1zLq++bfpGbhS7Pz+XyhM1PLtko99RRCRKqPSj2Fk9O3L10N68+OFm9lYe9zuOiEQBlX6Uu29CDsdranl6canfUUQkCqj0o9yAjA58/7xMXl6xhd2HqvyOIyIRTqUfA6aNz6H2pOPJRSV+RxGRCKfSjwFZXdtx/QVZ/HnVVsoOHPU7johEMJV+jLhn7CAM44mFmu2LSMNU+jGid1pbfnRhX/5rTRmb9x7xO46IRCiVfgy5a+xAkhONWQuK/Y4iIhFKpR9DundMZcpF2fxt7XZK9hz2O46IRCCVfoz599EDaZucyMz5mu2LyDep9GNM1/Yp3DaqP++u28mXOw81fgcRiSsq/Rj000sH0DE1iZkFRX5HEZEIo9KPQZ3bJfPTSwcw74vdrCs76HccEYkgKv0YdduobLq0S2aGZvsiUodKP0Z1TE3m30cPZPGGctZs0bVqRCRApR/DbrmoH906pPDoPM32RSRApR/D2qUkcdeYQSwv3cfy0r1+xxGRCKDSj3E/urAvPTulMmNeEZF8aUwRCQ+VfoxLTU7knnGDKNxygCXFmu2LxDuVfhy4Pi+LzC5teXTeBs32ReKcSj8OpCQlcO/4HNaVVTD/yz1+xxERH6n048T3h/ehf7f2PDpvAydParYvEq9U+nEiKTGBaeNz+GrXYf7++S6/44iIT1T6ceTqc3uT070DM+cXUavZvkhcUunHkcQE4/78XEr2VDL30+1+xxERH6j048wVQ3oyuFcnHp9fTHXtSb/jiEiYqfTjTEKCMT0/l837jvLmx2V+xxGRMFPpx6Hx3+rOuVlpzFpQwvGaWr/jiEgYqfTjkJnxQH4u2w8e443V2/yOIyJhpNKPU5fmdOOC7C48saiEqmrN9kXihUo/TpkZD0w8i92HjvPqyq1+xxGRMFHpx7GRA9IZNSidpxaXcPREjd9xRCQMVPpxbnr+WeytPMGc5Vv8jiIiYaDSj3Pn9+vC2LMyeGZJKYerqv2OIyKtTKUvTM8/i4NHq3lh2Wa/o4hIK2u09M3sBTPbY2af11nX1cwKzKzY+7OLt97MbJaZlZjZOjM7r859pnj7F5vZlNYZjjTHOZmduXxID/6wdCMHj57wO46ItKJgZvp/BK44bd1DwALnXA6wwLsNcCWQ4/1MBZ6CwC8J4GHgQmAE8PCpXxQSGe7Pz6XyRA3PLd3odxQRaUWNlr5zbgmw/7TVk4A53vIc4Jo6619yASuANDPrBVwOFDjn9jvnDgAFfPMXifjo7J6d+O7Q3rz44Wb2VR73O46ItJLmHtPv4Zzb6S3vAnp4y32Aul/xLPPWNbT+G8xsqpkVmllheXl5M+NJc9w3IYeq6lqe/qDU7ygi0kpa/EauC1x0NWQnZ3fOPeucy3PO5WVkZITqYSUIAzM6cM3wPrz00Rb2HKryO46ItILmlv5u77AN3p+nLry6Hciqs1+mt66h9RJhpo3Pofak48lFJX5HEZFW0NzSnwuc+gTOFOCtOutv8T7FMxKo8A4DvQ9MNLMu3hu4E711EmH6pbfnh3mZ/HnVNrYfPOZ3HBEJsWA+svln4CPgLDMrM7Pbgd8A+WZWDEzwbgO8B2wESoDngLsAnHP7gf8EVns/v/LWSQS6Z1wOAE8s1GxfJNYkNbaDc+6GBjaNr2dfB9zdwOO8ALzQpHTiiz5pbblhRBavrtzKnaMH0je9nd+RRCRE9I1cqdfdYweRmGA8vqDY7ygiEkIqfalX906p3HJRP/77kzJK9lT6HUdEQkSlLw26Y/RAUpMTNdsXiSEqfWlQeoc23DYqm3fW7eCrXYf8jiMiIaDSlzP66aUD6JCSxMyCIr+jiEgIqPTljNLapfCTSwfw/vrdfFZW4XccEWkhlb406seXZJPWLpkZBRv8jiIiLaTSl0Z1TE1m6mUDWLShnDVbDvgdR0RaQKUvQbn14my6dUjRbF8kyqn0JSjtUpK4Y/RAPizZx4qN+/yOIyLNpNKXoN00sh89OrVhxrwiAmfcEJFoo9KXoKUmJ3LP2EGs2ryfpcV7/Y4jIs2g0pcmuf6CLPqkteXRAs32RaKRSl+apE1SIveOH8Sn2w6y4Ms9jd9BRCKKSl+a7PvnZdIvvR0zCoo4eVKzfZFootKXJktOTOC+CTl8sfMQ/1i/y+84ItIEKn1plu+d24dB3Tsws6CIWs32RaKGSl+aJTHBuH9CLsV7Knln3Q6/44hIkFT60mxXfrsnZ/fsyGPzi6mpPel3HBEJgkpfmi0hwZien8umvUd485PtfscRkSCo9KVF8gf3YGhmZx6fX8yJGs32RSKdSl9axCww299+8BhvFG7zO46INEKlLy02OjeDvH5deGJhCVXVtX7HEZEzUOlLi5kZ0yfmsutQFX9audXvOCJyBip9CYmLB3bj4oHp/H5xKUdP1PgdR0QaoNKXkHlgYi57K4/z0kdb/I4iIg1Q6UvInN+vK6NzM3jmg1IOV1X7HUdE6qHSl5B6YGIuB45W8+KHm/2OIiL1UOlLSA3NTCN/cA+eW7qRiqOa7YtEGpW+hNz0/FwOV9Xwh2Ub/Y4iIqdR6UvIfatXJ64a2osXlm1i/5ETfscRkTpU+tIq7p+Qw7HqWp75oNTvKCJSh0pfWsWg7h25Zlgf5ny0mT2Hq/yOIyIelb60mnvH51Bd6/j9Is32RSKFSl9aTXa39vzw/Ez+tHIrOw4e8zuOiKDSl1Z2z7hBOBxPLCrxO4qIoNKXVpbZpR03jOjLG6u3sW3/Ub/jiMS9sJe+mV1hZhvMrMTMHgr380v43T12EIkJxuMLiv2OIhL3wlr6ZpYIPAlcCQwGbjCzweHMIOHXo1MqN43sx5sfl7GxvNLvOCJxLSnMzzcCKHHObQQws9eAScAXYc4hYXbnmIH8aeVWvv/Ucjq0Cf6vnVnwz2E0YecmP3ZTHjf4vZuWuGl3aK3MEh5jcjP4X98N/Zw43KXfB6h7Tb0y4MK6O5jZVGAqQN++fcOXTFpVtw5t+L/XDWXxhvKg9ne44B44tLvhXHB7Bv94Qe7YpMcMbcbgd5Rw6pXWtlUeN9yl3yjn3LPAswB5eXn66xhDrj63N1ef29vvGCJxLdxv5G4HsurczvTWiYhIGIS79FcDOWbW38xSgMnA3DBnEBGJW2E9vOOcqzGze4D3gUTgBefc+nBmEBGJZ2E/pu+cew94L9zPKyIi+kauiEhcUemLiMQRlb6ISBxR6YuIxBEL9tt9fjCzcmBLCx6iG7A3RHH8FCvjAI0lEsXKOEBjOaWfcy6jvg0RXfotZWaFzrk8v3O0VKyMAzSWSBQr4wCNJRg6vCMiEkdU+iIicSTWS/9ZvwOESKyMAzSWSBQr4wCNpVExfUxfRET+VazP9EVEpA6VvohIHIn60m/sQutm1sbMXve2rzSzbB9iBiWIsdxqZuVmttb7+YkfORtjZi+Y2R4z+7yB7WZms7xxrjOz88KdMVhBjGWMmVXUeU1+Ge6MwTCzLDNbZGZfmNl6M5tWzz5R8boEOZZoeV1SzWyVmX3qjeWRevYJbYc556L2h8DpmUuBAUAK8Ckw+LR97gKe9pYnA6/7nbsFY7kVeMLvrEGM5TLgPODzBrZ/B/g7gcu4jgRW+p25BWMZA7zjd84gxtELOM9b7ggU1fP3KypelyDHEi2viwEdvOVkYCUw8rR9Qtph0T7T//pC6865E8CpC63XNQmY4y3/BRhvkXkV6GDGEhWcc0uA/WfYZRLwkgtYAaSZWa/wpGuaIMYSFZxzO51zH3vLh4EvCVyzuq6oeF2CHEtU8P5bV3o3k72f0z9dE9IOi/bSr+9C66e/+F/v45yrASqA9LCka5pgxgLwA++f3n8xs6x6tkeDYMcaLS7y/nn+dzMb4neYxniHB4YTmFXWFXWvyxnGAlHyuphZopmtBfYABc65Bl+XUHRYtJd+vHkbyHbODQUK+Odvf/HPxwTOc3IuMBv4m79xzszMOgB/Be5zzh3yO09LNDKWqHldnHO1zrlhBK4ZPsLMvt2azxftpR/Mhda/3sfMkoDOwL6wpGuaRsfinNvnnDvu3fwDcH6YsoVaMK9bVHDOHTr1z3MXuCpcspl18zlWvcwsmUBJvuqce7OeXaLmdWlsLNH0upzinDsILAKuOG1TSDss2ks/mAutzwWmeMvXAQud945IhGl0LKcdX/0egWOZ0WgucIv3aZGRQIVzbqffoZrDzHqeOr5qZiMI/D8VcZMKL+PzwJfOuRkN7BYVr0swY4mi1yXDzNK85bZAPvDVabuFtMPCfo3cUHINXGjdzH4FFDrn5hL4y/GymZUQeENusn+JGxbkWO41s+8BNQTGcqtvgc/AzP5M4NMT3cysDHiYwBtUOOeeJnCN5O8AJcBR4DZ/kjYuiLFcB9xpZjXAMWByhE4qRgE3A595x48B/ifQF6LudQlmLNHyuvQC5phZIoFfTG84595pzQ7TaRhEROJItB/eERGRJlDpi4jEEZW+iEgcUemLiMQRlb6ISBxR6UvcMbPshs6aGcR9Kxvf61/2v8bMBjfnuURag0pfpHVdA6j0JWKo9CWumdkAM/vEzC44bX0vM1vinYv9czO7tM62X3sn8lphZj28ddlmttA7Gd4CM+trZhcT+Ob0//MeZ6CZ3eudB36dmb0W3tGKqPQljpnZWQTO33Krc271aZt/BLzvnQjrXGCtt749sMI7kdcS4Kfe+tnAHO9keK8Cs5xzywl8hf7nzrlhzrlS4CFguLffHa02OJEGqPQlXmUAbwE3Ouc+rWf7auA2M/vfwDneedsBTgDveMtrgGxv+SLgT97yy8AlDTzvOuBVM7uJwOk0RMJKpS/xqgLYSgPl7F085TICZzj8o5nd4m2qrnMOl1qafv6qq4AnCVyNa7V31kSRsFHpS7w6AVxL4KySPzp9o5n1A3Y7554jcBrrxq4Xu5x/ngjrRmCpt3yYwCX9MLMEIMs5twh4kMApcju0cBwiTaJZhsQt59wRM/suUGBmld4ZDU8ZA/zczKqBSuCW+h6jjp8BL5rZz4Fy/nmGyteA58zsXgK/FJ43s84Ero06yzuHukjY6CybIiJxRId3RETiiEpfRCSOqPRFROKISl9EJI6o9EVE4ohKX0Qkjqj0RUTiyP8HzTi74wbdCQ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = random.randint(0, number_of_tasks-1)\n",
    "graph = []\n",
    "for i in range(K): #Store graphs\n",
    "    graph.append(random.sample(GRAPH_TEST, 1)[0])\n",
    "graph = create_mini_batch(graph)\n",
    "\n",
    "k_shot_updates = 4\n",
    "initialization_to_store_meta_losses()\n",
    "for shots in range(k_shot_updates):\n",
    "    new_model = training(model, graph, lr_k,shots, task)\n",
    "    train_set_evaluation(new_model,graph,store_train_loss_meta, task) \n",
    "\n",
    "plt.plot(store_train_loss_meta,label = 'Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('k shots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "-GCeaVcsqDgx",
   "metadata": {
    "id": "-GCeaVcsqDgx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kcollins/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_type = \"equiv\"\n",
    "\n",
    "num_evals = 100\n",
    "\n",
    "all_losses = []\n",
    "for test_eval in range(num_evals): \n",
    "\n",
    "    task = random.randint(0, number_of_tasks-1)\n",
    "    graph = []\n",
    "    for i in range(K): #Store graphs\n",
    "        graph.append(random.sample(GRAPH_TEST, 1)[0])\n",
    "    graph = create_mini_batch(graph)\n",
    "\n",
    "    k_shot_updates = 5\n",
    "    initialization_to_store_meta_losses()\n",
    "    for shots in range(k_shot_updates):\n",
    "        new_model = training(model, graph, lr_k,shots, task)\n",
    "        train_set_evaluation(new_model,graph,store_train_loss_meta, task) \n",
    "    all_losses.append(np.array(store_train_loss_meta))\n",
    "\n",
    "# plt.plot(store_train_loss_meta,label = 'Loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('k shots')\n",
    "\n",
    "\n",
    "all_losses = np.array(all_losses)\n",
    "np.save(f\"reptile_graph_{graph_type}_k.npy\", all_losses)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "mean_loss = np.mean(all_losses, axis=0)\n",
    "\n",
    "# confidence interval plotting help from: https://stackoverflow.com/questions/59747313/how-to-plot-confidence-interval-in-python\n",
    "y = mean_loss\n",
    "x = list(range(len(mean_loss)))\n",
    "ci = 1.96 * np.std(all_losses, axis=0)**2/np.sqrt(len(y))\n",
    "\n",
    "ax_size=16\n",
    "title_size=18\n",
    "                                                  \n",
    "ax.plot(x, y, linewidth=3, label=f\"Mean Loss\")\n",
    "# to avoid having MSE < 0\n",
    "truncated_error = np.clip(y-ci, a_min=0, a_max=None)\n",
    "ax.fill_between(x, truncated_error, (y+ci), alpha=.5,label=f\"95% CI\")\n",
    "\n",
    "ax.set_xlabel(\"Gradient Steps\",fontsize=ax_size)\n",
    "ax.set_ylabel(\"Mean Squared Error (MSE)\",fontsize=ax_size)\n",
    "ax.set_title(\"Graph Regression: k-Shot Evaluation\",fontsize=title_size)\n",
    "ax.legend()#loc=\"upper right\")\n",
    "plt.savefig(f\"graph_reg_{graph_type}_kshot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "128fe1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Error: 658.0721620178223, Var: 1057556.7344100638\n",
      "Step: 1, Error: 0.35243968663144415, Var: 0.5205111469438494\n",
      "Step: 4, Error: 1.2600247795013785e-06, Var: 2.0546804653063013e-11\n"
     ]
    }
   ],
   "source": [
    "analysis_steps = [0, 1, k_shot_updates-1]\n",
    "for analysis_step in analysis_steps: \n",
    "    print(f\"Step: {analysis_step}, Error: {mean_loss[analysis_step]}, Var: {ci[analysis_step]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dbbb91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_error[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcae814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Reptile_GraphNN_Regression_EquivariantMessagePassing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
